{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rNAxRKvX1nn",
        "outputId": "95f0b5a1-0340-462e-cce5-38f32b2cf3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vt_WUACMYdK",
        "outputId": "91e765a4-f14c-43be-d3d8-9f95a9821369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard_logger in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.11.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard_logger\n",
        "\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/config.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/edgeConstruction.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/data_params.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/make_data.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/pretraining.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/extract_feature.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/copyGraph.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/DCC.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/extractSDAE.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/extractconvSDAE.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/SDAE.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/__init__.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/DCCComputation.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/DCCLoss.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/custom_data.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/convSDAE.py\" ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Qg9L4N5gZIlA"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "85riHxlGsQfT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "from sklearn import preprocessing\n",
        "from scipy.io import savemat\n",
        "import scipy.io\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from config import cfg, get_data_dir\n",
        "from easydict import EasyDict as edict\n",
        "from edgeConstruction import compressed_data\n",
        "from custom_data import DCCPT_data\n",
        "import data_params as dp\n",
        "import make_data\n",
        "import pretraining\n",
        "import extract_feature\n",
        "import copyGraph\n",
        "import DCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qE1IAfhL-Wr3"
      },
      "outputs": [],
      "source": [
        "#some necessary initial functions\n",
        "\n",
        "def set_pretraining_hypers(args):\n",
        "  args.db = dp.oyster.name\n",
        "  args.niter = 500\n",
        "  args.step = 300\n",
        "  args.lr = 0.001\n",
        "\n",
        "# if we need to resume for faster debugging/results\n",
        "  args.resume = False\n",
        "  args.level = None\n",
        "\n",
        "  args.batchsize = 300\n",
        "  args.ngpu = 0\n",
        "  args.deviceID = 0\n",
        "  args.tensorboard = True\n",
        "  args.h5 = False\n",
        "  args.id = 2\n",
        "  args.dim = 10\n",
        "  args.manualSeed = cfg.RNG_SEED\n",
        "  args.clean_log = True\n",
        "\n",
        "def cluster_count(clusters):\n",
        "  cluster_df = pd.DataFrame(clusters.T, columns= ['labels'])\n",
        "  cluster_df_count = pd.DataFrame(np.array(cluster_df['labels'].value_counts()), columns= ['labels'])\n",
        "  df_filtered = cluster_df_count[cluster_df_count['labels'] >= 100]\n",
        "  return len(df_filtered)\n",
        "\n",
        "def set_training_hypers(args):\n",
        "  args.batchsize = cfg.PAIRS_PER_BATCH\n",
        "  args.nepoch = 150\n",
        "  args.M = 10\n",
        "  args.lr = 0.001\n",
        "\n",
        "#function for finding the most frequent element in a list\n",
        "def most_frequent(List):\n",
        "  return max(set(List), key = List.count)\n",
        "\n",
        "#function for deleting certain key/value pairs in a dict and returning the poped keys\n",
        "def delete_dict_items(dict, val):\n",
        "  poped_keys = []\n",
        "  for key, value in dict.copy().items():\n",
        "    if value != val:\n",
        "      poped_keys.append(key)\n",
        "      dict.pop(key)\n",
        "\n",
        "  return dict, poped_keys\n",
        "\n",
        "#function for finding the minimum value and its corresponding key in a dict\n",
        "def min_value(dict):\n",
        "  min_value = min(dict.values())\n",
        "  for key in dict:\n",
        "    if dict[key] == min_value:\n",
        "      min_value_key = key\n",
        "  return min_value, min_value_key\n",
        "\n",
        "\n",
        "def cluster_metrics(labels, features):\n",
        "\n",
        "    numeval = len(labels)\n",
        "    dbl = metrics.davies_bouldin_score(features, labels[:numeval])\n",
        "    sil = metrics.silhouette_score(features, labels[:numeval], metric='euclidean')\n",
        "    ch = metrics.calinski_harabasz_score(features, labels[:numeval])\n",
        "\n",
        "    return dbl,sil,ch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def change_time(start_number):\n",
        "  mat_train = scipy.io.loadmat('/content/drive/MyDrive/DCC-master/data/oyster/Old_traindata.mat')\n",
        "  mat_test = scipy.io.loadmat('/content/drive/MyDrive/DCC-master/data/oyster/Old_testdata.mat')\n",
        "  D=start_number\n",
        "  v=np.array([0,D*60,(24-D)*60,24*60]).reshape(-1, 1)\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  v_scaled = min_max_scaler.fit_transform(v)\n",
        "  #changing EntTime\n",
        "  for i in range(mat_train['X'].shape[0]):\n",
        "      New_time=mat_train['X'][i][0]-v_scaled[1]\n",
        "      if New_time<0:\n",
        "          New_time=mat_train['X'][i][0]+v_scaled[2]\n",
        "      mat_train['X'][i][0]=New_time\n",
        "  #changing EXTime\n",
        "  for i in range(mat_train['X'].shape[0]):\n",
        "      New_time=mat_train['X'][i][1]-v_scaled[1]\n",
        "      if New_time<0:\n",
        "          New_time=mat_train['X'][i][1]+v_scaled[2]\n",
        "      mat_train['X'][i][1]=New_time\n",
        "    #changing EntTime\n",
        "  for i in range(mat_test['X'].shape[0]):\n",
        "      New_time=mat_test['X'][i][0]-v_scaled[1]\n",
        "      if New_time<0:\n",
        "          New_time=mat_test['X'][i][0]+v_scaled[2]\n",
        "      mat_test['X'][i][0]=New_time\n",
        "  #changing EXTime\n",
        "  for i in range(mat_test['X'].shape[0]):\n",
        "      New_time=mat_test['X'][i][1]-v_scaled[1]\n",
        "      if New_time<0:\n",
        "          New_time=mat_test['X'][i][1]+v_scaled[2]\n",
        "      mat_test['X'][i][1]=New_time\n",
        "  savemat(\"/content/drive/MyDrive/DCC-master/data/oyster/traindata.mat\", mat_train)\n",
        "  savemat(\"/content/drive/MyDrive/DCC-master/data/oyster/testdata.mat\", mat_test)"
      ],
      "metadata": {
        "id": "D9HHwm3oyQeK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HuU6tLOYs4dW"
      },
      "outputs": [],
      "source": [
        "#oyster dataset root directory\n",
        "datadir = get_data_dir(dp.oyster.name)\n",
        "\n",
        "#shuffling the oyster dataset and making train/test sets\n",
        "#X, Y = make_data.make_misc_data(datadir,'OysterMetro_Complete_Small_Sample_Feature_Extracted.csv',dim= 10)\n",
        "\n",
        "#oyster dataset shape (whole data)\n",
        "#N = X.shape[0]\n",
        "N = 12617\n",
        "#defining a super dict for storing clusters, cluster counts, and dbl for each eligible k\n",
        "super_dict = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnFJ2NIukcQY",
        "outputId": "454a18df-d28d-44f4-ddcb-17c1fa26712b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " Epoch: 2\n",
            "total_loss:  0.013394840724775172 epoch:  2\n",
            "reconstruction_loss:  0.013392176685049668 epoch:  2\n",
            "dcc_loss:  2.664013650035757e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.013399236206683157 epoch:  3\n",
            "reconstruction_loss:  0.013397390630176349 epoch:  3\n",
            "dcc_loss:  1.845577011678025e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.013402979850220828 epoch:  4\n",
            "reconstruction_loss:  0.013401344097148668 epoch:  4\n",
            "dcc_loss:  1.635748970778345e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.01119306474878607 epoch:  5\n",
            "reconstruction_loss:  0.011151912773386811 epoch:  5\n",
            "dcc_loss:  4.1151963756960074e-05 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.00833785122171764 epoch:  6\n",
            "reconstruction_loss:  0.008285128415358148 epoch:  6\n",
            "dcc_loss:  5.2722816259624665e-05 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.008269634499325117 epoch:  7\n",
            "reconstruction_loss:  0.00826067763994438 epoch:  7\n",
            "dcc_loss:  8.956863491910692e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.008243792273497063 epoch:  8\n",
            "reconstruction_loss:  0.008240982886269704 epoch:  8\n",
            "dcc_loss:  2.809410980797635e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.00822510694287519 epoch:  9\n",
            "reconstruction_loss:  0.008224262588671497 epoch:  9\n",
            "dcc_loss:  8.443761660997475e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.008198592922897742 epoch:  10\n",
            "reconstruction_loss:  0.008198240093540953 epoch:  10\n",
            "dcc_loss:  3.52842646418521e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.1315396705472434\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.008147941455467654 epoch:  11\n",
            "reconstruction_loss:  0.008147661163215573 epoch:  11\n",
            "dcc_loss:  2.802847437057442e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.055707979293442\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.007852128530067415 epoch:  12\n",
            "reconstruction_loss:  0.00784955841892179 epoch:  12\n",
            "dcc_loss:  2.570082170084167e-06 epoch:  12\n",
            "epoch:  12 DBL:  1.0441639365109077\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 6.929011344909668\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.2395004005016615 0\n",
            "val_loss_0 0.21215330106803998 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.1952040120125124 1\n",
            "val_loss_0 0.17147878589324075 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.15908185616512074 2\n",
            "val_loss_0 0.14092890842686365 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.1320342918645517 3\n",
            "val_loss_0 0.11815248480547816 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.11183150024045793 4\n",
            "val_loss_0 0.10118157418943623 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09672107942307136 5\n",
            "val_loss_0 0.08857968167391518 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.08547586188817254 6\n",
            "val_loss_0 0.07927071796685882 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07719449432363575 7\n",
            "val_loss_0 0.07241764047113727 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.07341611006947796 8\n",
            "val_loss_0 0.07186126906170898 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.07285165212355295 9\n",
            "val_loss_0 0.07132419669873741 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.072317736894911 10\n",
            "val_loss_0 0.07080481887430472 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.07177702497396007 11\n",
            "val_loss_0 0.07030097068111794 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.07125752870269163 12\n",
            "val_loss_0 0.06981306395562817 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.07075911661781453 13\n",
            "val_loss_0 0.0693409409309057 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.07028275345029791 14\n",
            "val_loss_0 0.06888096337981534 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.00039213122480147435 0\n",
            "val_loss_1 0.00038733941407117063 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0003914378005270517 1\n",
            "val_loss_1 0.00038649834611854576 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0003906167927734455 2\n",
            "val_loss_1 0.0003856052093376832 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0003897580535890964 3\n",
            "val_loss_1 0.0003846722867442428 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0003888272970959501 4\n",
            "val_loss_1 0.00038370251662589363 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.00038786641081690735 5\n",
            "val_loss_1 0.0003827016793908005 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.00038688654160842355 6\n",
            "val_loss_1 0.0003816704074361416 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0003858349172015141 7\n",
            "val_loss_1 0.00038060988442530997 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.00038523897110473654 8\n",
            "val_loss_1 0.00038050169626283335 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0003851223258185304 9\n",
            "val_loss_1 0.00038039304705140387 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.00038500849796362373 10\n",
            "val_loss_1 0.0003802838924664874 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.0003849139917677588 11\n",
            "val_loss_1 0.000380174341866534 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0003848141727891863 12\n",
            "val_loss_1 0.0003800643173492146 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0003846948271300378 13\n",
            "val_loss_1 0.00037995380872127006 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0003845891076605444 14\n",
            "val_loss_1 0.000379842750625921 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 9.879326115128581e-06 0\n",
            "val_loss_2 9.824614054598128e-06 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 9.862271485396706e-06 1\n",
            "val_loss_2 9.804517878405363e-06 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 9.844280406740854e-06 2\n",
            "val_loss_2 9.783410305187652e-06 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 9.823984630303461e-06 3\n",
            "val_loss_2 9.761415065141013e-06 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 9.8041887036116e-06 4\n",
            "val_loss_2 9.738598118972316e-06 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 9.782304524008832e-06 5\n",
            "val_loss_2 9.71501736589261e-06 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 9.761075554824128e-06 6\n",
            "val_loss_2 9.690723008279804e-06 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 9.738160796163928e-06 7\n",
            "val_loss_2 9.665757055852725e-06 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 9.72515157138155e-06 8\n",
            "val_loss_2 9.663219753010867e-06 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 9.72243436427162e-06 9\n",
            "val_loss_2 9.66067446218226e-06 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 9.720054939691224e-06 10\n",
            "val_loss_2 9.658121008963012e-06 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 9.717499394115613e-06 11\n",
            "val_loss_2 9.655562100216905e-06 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 9.715623131056167e-06 12\n",
            "val_loss_2 9.652995558057264e-06 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 9.712724369051335e-06 13\n",
            "val_loss_2 9.650424449686508e-06 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 9.710676173556339e-06 14\n",
            "val_loss_2 9.647849337232893e-06 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 2.4961488739686296e-07 0\n",
            "val_loss_3 2.4663629280530444e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 2.495030840148278e-07 1\n",
            "val_loss_3 2.464929590617685e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 2.4934873186664864e-07 2\n",
            "val_loss_3 2.4632659296847744e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 2.491805242322735e-07 3\n",
            "val_loss_3 2.4614533455255864e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 2.4899666484164416e-07 4\n",
            "val_loss_3 2.459535138006366e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 2.4880381145414684e-07 5\n",
            "val_loss_3 2.4575276651496397e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 2.4860362599777184e-07 6\n",
            "val_loss_3 2.455425248469024e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 2.4838674189303213e-07 7\n",
            "val_loss_3 2.453226223875684e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 2.4826289756495255e-07 8\n",
            "val_loss_3 2.4530008602141606e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 2.482395156894576e-07 9\n",
            "val_loss_3 2.4527744141854774e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 2.4821764902927476e-07 10\n",
            "val_loss_3 2.452547174285751e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 2.4819375291317066e-07 11\n",
            "val_loss_3 2.452319350412232e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 2.481753310582246e-07 12\n",
            "val_loss_3 2.4520906774908165e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 2.481497160468081e-07 13\n",
            "val_loss_3 2.4518613683465075e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 2.4812543564767845e-07 14\n",
            "val_loss_3 2.4516313824412094e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07726774707748078 0\n",
            "val_loss_4 0.07337407066402647 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.07249260269918185 1\n",
            "val_loss_4 0.06909389973347992 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.06860157700799377 2\n",
            "val_loss_4 0.06591023185339102 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.06570817063764929 3\n",
            "val_loss_4 0.06356464243138271 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.06356302149653624 4\n",
            "val_loss_4 0.06184140605211447 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.06197503161574454 5\n",
            "val_loss_4 0.06057512442538553 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.06079536347320481 6\n",
            "val_loss_4 0.0596555757990163 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.05992483225889429 7\n",
            "val_loss_4 0.0589797885264912 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.05952439876102567 8\n",
            "val_loss_4 0.05892465820332337 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.05946433003875577 9\n",
            "val_loss_4 0.058871972337719375 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.05940629922818361 10\n",
            "val_loss_4 0.058820528192253765 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.05934978974895436 11\n",
            "val_loss_4 0.058770710971389445 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.05929499635406272 12\n",
            "val_loss_4 0.05872231207067728 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.05924142169492351 13\n",
            "val_loss_4 0.05867569523431042 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.059189775974712956 14\n",
            "val_loss_4 0.05863000547569452 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.05913924939474974 15\n",
            "val_loss_4 0.05858592993082411 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.059090445054176245 16\n",
            "val_loss_4 0.058543154222982244 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.05906290695385691 17\n",
            "val_loss_4 0.058538976118039404 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.05905820559136319 18\n",
            "val_loss_4 0.058534793266450925 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.059053495117354936 19\n",
            "val_loss_4 0.05853065758612946 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.05904881225175986 20\n",
            "val_loss_4 0.05852651867643589 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.05904414547249464 21\n",
            "val_loss_4 0.05852238794348886 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.05903949722851408 22\n",
            "val_loss_4 0.058518226900493665 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.05903484197982142 23\n",
            "val_loss_4 0.058514115567394366 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.059030194167314594 24\n",
            "val_loss_4 0.05851002822138617 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.05902559248890913 25\n",
            "val_loss_4 0.05850592640519331 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.0590229151471689 26\n",
            "val_loss_4 0.05850551798703554 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.05902245260437735 27\n",
            "val_loss_4 0.058505107815452685 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.059021989707622326 28\n",
            "val_loss_4 0.05850469504029929 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.05902152473765264 29\n",
            "val_loss_4 0.058504284248818685 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.900365, 0.893912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.122047, 0.092983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.909399, 0.895239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.249218, 0.22636 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.643937, 0.6464  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.729413, 0.731337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), array([[2.4692301e-04, 5.5920903e-04],\n",
            "       [6.9453323e-05, 4.5285380e-04],\n",
            "       [3.2516086e-04, 5.0022255e-04],\n",
            "       ...,\n",
            "       [1.1822632e-04, 4.4576926e-04],\n",
            "       [3.0331887e-04, 4.5303922e-04],\n",
            "       [1.6064415e-04, 4.9796508e-04]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.900365, 0.893912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.122047, 0.092983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.909399, 0.895239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.249218, 0.22636 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.643937, 0.6464  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.729413, 0.731337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), 'Z': array([[2.4692301e-04, 5.5920903e-04],\n",
            "       [6.9453323e-05, 4.5285380e-04],\n",
            "       [3.2516086e-04, 5.0022255e-04],\n",
            "       ...,\n",
            "       [1.1822632e-04, 4.4576926e-04],\n",
            "       [3.0331887e-04, 4.5303922e-04],\n",
            "       [1.6064415e-04, 4.9796508e-04]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.013569713856535989 epoch:  0\n",
            "reconstruction_loss:  0.013565284172782762 epoch:  0\n",
            "dcc_loss:  4.429666053396726e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.085368282551483\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.013570653229740395 epoch:  1\n",
            "reconstruction_loss:  0.013566603842394179 epoch:  1\n",
            "dcc_loss:  4.049383239026811e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.013579415860225185 epoch:  2\n",
            "reconstruction_loss:  0.013576681352942453 epoch:  2\n",
            "dcc_loss:  2.734494783005623e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.013574190802001403 epoch:  3\n",
            "reconstruction_loss:  0.013572248596064842 epoch:  3\n",
            "dcc_loss:  1.9422500721406136e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.013567398124953606 epoch:  4\n",
            "reconstruction_loss:  0.013565861214721489 epoch:  4\n",
            "dcc_loss:  1.5369012770728136e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.013556424807687442 epoch:  5\n",
            "reconstruction_loss:  0.013555152967286455 epoch:  5\n",
            "dcc_loss:  1.271842458955775e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.013561464128860671 epoch:  6\n",
            "reconstruction_loss:  0.013560360675162057 epoch:  6\n",
            "dcc_loss:  1.1034573837125652e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.013566820399424482 epoch:  7\n",
            "reconstruction_loss:  0.013565835518042548 epoch:  7\n",
            "dcc_loss:  9.848732818635448e-07 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.013563472239712136 epoch:  8\n",
            "reconstruction_loss:  0.013562585290391949 epoch:  8\n",
            "dcc_loss:  8.869740069048533e-07 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.013561394306053161 epoch:  9\n",
            "reconstruction_loss:  0.013560564514616368 epoch:  9\n",
            "dcc_loss:  8.297708275810226e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.01356880193725464 epoch:  10\n",
            "reconstruction_loss:  0.013568015320858664 epoch:  10\n",
            "dcc_loss:  7.86617395882773e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.1242829283205225\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.01356603349669914 epoch:  11\n",
            "reconstruction_loss:  0.013565268297934316 epoch:  11\n",
            "dcc_loss:  7.651812682197622e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.0852693079832059\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.013555356928178474 epoch:  12\n",
            "reconstruction_loss:  0.013554576514984814 epoch:  12\n",
            "dcc_loss:  7.804356576099298e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.0932239105455974\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 8.141754388809204\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.243069279126555 0\n",
            "val_loss_0 0.21640476669449057 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.19796748383895832 1\n",
            "val_loss_0 0.17490016580478135 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16120513949724463 2\n",
            "val_loss_0 0.14369124462222901 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.13366570184630136 3\n",
            "val_loss_0 0.12041624268242372 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.11309711616879399 4\n",
            "val_loss_0 0.10305447066453671 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.0977037329214552 5\n",
            "val_loss_0 0.09016912188469514 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.08626347667431791 6\n",
            "val_loss_0 0.08063341795264635 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07783069696919977 7\n",
            "val_loss_0 0.07361185447963785 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.07398868757959377 8\n",
            "val_loss_0 0.07304166850121765 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.07341525610287197 9\n",
            "val_loss_0 0.07249057079221102 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.07286831865969845 10\n",
            "val_loss_0 0.07195881100020204 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.07231871566040292 11\n",
            "val_loss_0 0.0714421113879197 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.07179338261625356 12\n",
            "val_loss_0 0.07094225370090472 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.07128945928530923 13\n",
            "val_loss_0 0.07045797997769765 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.07079976791272158 14\n",
            "val_loss_0 0.06998650930385016 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0004064580212466889 0\n",
            "val_loss_1 0.00040288259026745007 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.000405738643276057 1\n",
            "val_loss_1 0.00040200743983830316 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.00040488691356169745 2\n",
            "val_loss_1 0.000401077491139754 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0004039922455472623 3\n",
            "val_loss_1 0.00040010500546336684 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0004030254950958109 4\n",
            "val_loss_1 0.0003990941117709523 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0004020276997818986 5\n",
            "val_loss_1 0.00039805102532156837 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0004010090927003225 6\n",
            "val_loss_1 0.00039697586552855425 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.00039991365776079495 7\n",
            "val_loss_1 0.0003958711361976727 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.00039929471638241066 8\n",
            "val_loss_1 0.00039575841632434025 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.00039917552281406107 9\n",
            "val_loss_1 0.0003956451954131152 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.00039905651823173585 10\n",
            "val_loss_1 0.0003955314149334732 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.000398956357605394 11\n",
            "val_loss_1 0.00039541725144557737 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.00039885078061289086 12\n",
            "val_loss_1 0.000395302513722041 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.00039872935354975096 13\n",
            "val_loss_1 0.0003951873676685348 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0003986182004444203 14\n",
            "val_loss_1 0.0003950715654643279 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.0216246156231784e-05 0\n",
            "val_loss_2 1.018898637273748e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.019857517348379e-05 1\n",
            "val_loss_2 1.0168214808085346e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.0180098390775293e-05 2\n",
            "val_loss_2 1.01463831574526e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.0159125627680849e-05 3\n",
            "val_loss_2 1.0123617458643842e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.0138649439231873e-05 4\n",
            "val_loss_2 1.0099997582878414e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.0116066483666904e-05 5\n",
            "val_loss_2 1.0075558527685401e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.0093988366957113e-05 6\n",
            "val_loss_2 1.005038031562637e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.0070318276146046e-05 7\n",
            "val_loss_2 1.0024524313811339e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.0056974934116705e-05 8\n",
            "val_loss_2 1.0021894869499115e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.0053948756436761e-05 9\n",
            "val_loss_2 1.0019258206883458e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.00516080654452e-05 10\n",
            "val_loss_2 1.0016612057272354e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.0049029633457732e-05 11\n",
            "val_loss_2 1.0013960576090884e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.0047094198184584e-05 12\n",
            "val_loss_2 1.001130137645596e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.0044073468551818e-05 13\n",
            "val_loss_2 1.0008635202106517e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.0042024965506499e-05 14\n",
            "val_loss_2 1.0005966330982768e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 2.5809113570492506e-07 0\n",
            "val_loss_3 2.5569630440731827e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 2.579741904180438e-07 1\n",
            "val_loss_3 2.5554731951809257e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 2.578149217506104e-07 2\n",
            "val_loss_3 2.553747183954043e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 2.576395523551516e-07 3\n",
            "val_loss_3 2.5518697489760037e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 2.5745004024893346e-07 4\n",
            "val_loss_3 2.549887250964501e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 2.5725093376433945e-07 5\n",
            "val_loss_3 2.5478164848526513e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 2.5704454861597227e-07 6\n",
            "val_loss_3 2.5456548102591434e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 2.568225678169824e-07 7\n",
            "val_loss_3 2.543397581518192e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 2.5669519879987996e-07 8\n",
            "val_loss_3 2.5431659252181326e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 2.566710843606676e-07 9\n",
            "val_loss_3 2.5429330987183727e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 2.56648471496149e-07 10\n",
            "val_loss_3 2.542699471366008e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 2.56624423846405e-07 11\n",
            "val_loss_3 2.5424649846060116e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 2.5660532442106e-07 12\n",
            "val_loss_3 2.5422297978882264e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 2.5657871028504085e-07 13\n",
            "val_loss_3 2.541993826983498e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 2.5655382935234027e-07 14\n",
            "val_loss_3 2.541756985410555e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07810173179028966 0\n",
            "val_loss_4 0.0747979867689772 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.07322285148349919 1\n",
            "val_loss_4 0.07039376031814022 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.06925328649830725 2\n",
            "val_loss_4 0.06710937748502249 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.066295556714475 3\n",
            "val_loss_4 0.06469221795110619 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.06410798464325862 4\n",
            "val_loss_4 0.06290737156964331 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.06248554227347536 5\n",
            "val_loss_4 0.0615987138034519 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.06128262273541329 6\n",
            "val_loss_4 0.0606437680590455 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.06039475917817638 7\n",
            "val_loss_4 0.05993949508563085 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.05998775409142409 8\n",
            "val_loss_4 0.05988181067155394 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.05992643818713966 9\n",
            "val_loss_4 0.05982699608297431 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.059867299529563725 10\n",
            "val_loss_4 0.059773435087050956 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.05980978308661327 11\n",
            "val_loss_4 0.05972126742054468 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.059753850827330014 12\n",
            "val_loss_4 0.05967062538734707 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.05969915998366771 13\n",
            "val_loss_4 0.05962203198633557 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.0596466782677874 14\n",
            "val_loss_4 0.05957414698085959 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.05959513778730461 15\n",
            "val_loss_4 0.05952805782507414 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.0595453420256607 16\n",
            "val_loss_4 0.0594833881583909 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.059517431376011204 17\n",
            "val_loss_4 0.059479012801983464 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.059512640121188516 18\n",
            "val_loss_4 0.05947464822589312 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.059507842951595495 19\n",
            "val_loss_4 0.059470317578873055 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.059503071138927416 20\n",
            "val_loss_4 0.05946597216647892 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.05949830183980605 21\n",
            "val_loss_4 0.059461654643579824 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.05949357047752336 22\n",
            "val_loss_4 0.059457294265082825 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.05948881364832498 23\n",
            "val_loss_4 0.05945300296681052 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.05948408407800547 24\n",
            "val_loss_4 0.05944873122188444 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.059479403650661435 25\n",
            "val_loss_4 0.05944442457376305 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.059476667800492836 26\n",
            "val_loss_4 0.05944399967813265 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.05947619762257248 27\n",
            "val_loss_4 0.05944356946318917 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.05947572499089904 28\n",
            "val_loss_4 0.05944313897076764 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.05947525061450675 29\n",
            "val_loss_4 0.05944270924583857 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.8586983 , 0.85224533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.08038034, 0.05131633, 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.86773235, 0.8535723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.20755133, 0.18469334, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.6022703 , 0.60473335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.68774635, 0.6896703 , 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[2.5719524e-04, 5.6475867e-04],\n",
            "       [8.3477396e-05, 4.5538388e-04],\n",
            "       [3.3335402e-04, 5.0656416e-04],\n",
            "       ...,\n",
            "       [1.3333763e-04, 4.5116301e-04],\n",
            "       [3.1340591e-04, 4.5777581e-04],\n",
            "       [1.7059209e-04, 5.0573354e-04]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.8586983 , 0.85224533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.08038034, 0.05131633, 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.86773235, 0.8535723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.20755133, 0.18469334, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.6022703 , 0.60473335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.68774635, 0.6896703 , 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[2.5719524e-04, 5.6475867e-04],\n",
            "       [8.3477396e-05, 4.5538388e-04],\n",
            "       [3.3335402e-04, 5.0656416e-04],\n",
            "       ...,\n",
            "       [1.3333763e-04, 4.5116301e-04],\n",
            "       [3.1340591e-04, 4.5777581e-04],\n",
            "       [1.7059209e-04, 5.0573354e-04]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.013663177598082927 epoch:  0\n",
            "reconstruction_loss:  0.013658320096729216 epoch:  0\n",
            "dcc_loss:  4.857508111965507e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.1292286136975538\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.013667463093186463 epoch:  1\n",
            "reconstruction_loss:  0.013663273036248938 epoch:  1\n",
            "dcc_loss:  4.1900647002153016e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.013659858116196584 epoch:  2\n",
            "reconstruction_loss:  0.01365710644465346 epoch:  2\n",
            "dcc_loss:  2.7516911376033917e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.013662507966255321 epoch:  3\n",
            "reconstruction_loss:  0.013660669120090474 epoch:  3\n",
            "dcc_loss:  1.8388576325825998e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.01365595144728162 epoch:  4\n",
            "reconstruction_loss:  0.013654537335909501 epoch:  4\n",
            "dcc_loss:  1.4141290872873164e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.01366173446238516 epoch:  5\n",
            "reconstruction_loss:  0.013660545553053082 epoch:  5\n",
            "dcc_loss:  1.1888747342885765e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.01364746358794788 epoch:  6\n",
            "reconstruction_loss:  0.013646361027645047 epoch:  6\n",
            "dcc_loss:  1.1025678748784636e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.013663326444138119 epoch:  7\n",
            "reconstruction_loss:  0.013662260736073102 epoch:  7\n",
            "dcc_loss:  1.0657196077636708e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.013659856722314347 epoch:  8\n",
            "reconstruction_loss:  0.013658845271244485 epoch:  8\n",
            "dcc_loss:  1.0114026289603498e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.013658674136141049 epoch:  9\n",
            "reconstruction_loss:  0.013657702350077515 epoch:  9\n",
            "dcc_loss:  9.718062334128007e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.013650643286138131 epoch:  10\n",
            "reconstruction_loss:  0.013649772104456991 epoch:  10\n",
            "dcc_loss:  8.711546028273854e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.1667840917903856\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.013646937044756124 epoch:  11\n",
            "reconstruction_loss:  0.013646115626871087 epoch:  11\n",
            "dcc_loss:  8.214341758687959e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.1004571576264224\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.013659185179568573 epoch:  12\n",
            "reconstruction_loss:  0.01365844587546724 epoch:  12\n",
            "dcc_loss:  7.393065080164183e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.04455911368151\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.659404039382935\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.25037315954399525 0\n",
            "val_loss_0 0.2244157700927813 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.20332133621612247 1\n",
            "val_loss_0 0.18089264874507432 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16496407926779466 2\n",
            "val_loss_0 0.1481459146681376 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.1362409707541557 3\n",
            "val_loss_0 0.12366951682063176 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.11477281381303653 4\n",
            "val_loss_0 0.10538878078128192 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09871449768410369 5\n",
            "val_loss_0 0.09180534267340523 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.0867887910790974 6\n",
            "val_loss_0 0.08174228900822142 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07799898417729116 7\n",
            "val_loss_0 0.07436223979891764 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.07401245018364144 8\n",
            "val_loss_0 0.07375976084340394 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.07341966565957121 9\n",
            "val_loss_0 0.0731783546330245 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.07284961951909281 10\n",
            "val_loss_0 0.07261735139482561 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.07228577671592681 11\n",
            "val_loss_0 0.07207115301522891 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.07173782271473941 12\n",
            "val_loss_0 0.0715449390056205 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.07121576067626056 13\n",
            "val_loss_0 0.07103304145783894 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.07070144926794195 14\n",
            "val_loss_0 0.07053614516407487 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.00044052259771027005 0\n",
            "val_loss_1 0.00043966964341252867 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0004397352617043907 1\n",
            "val_loss_1 0.00043870961994099504 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0004388054071733512 2\n",
            "val_loss_1 0.0004376884627896885 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0004378274332375646 3\n",
            "val_loss_1 0.0004366197539921466 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.00043677279394429837 4\n",
            "val_loss_1 0.00043550821727070894 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.00043567836386115903 5\n",
            "val_loss_1 0.00043436060074743374 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.00043456718983944556 6\n",
            "val_loss_1 0.00043317804606734714 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.00043336301999330674 7\n",
            "val_loss_1 0.000431962732881346 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.00043269004892954824 8\n",
            "val_loss_1 0.00043183863640832234 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0004325571143825528 9\n",
            "val_loss_1 0.00043171404175705347 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.00043242457924980214 10\n",
            "val_loss_1 0.0004315887753653935 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.00043231437767507583 11\n",
            "val_loss_1 0.00043146298538152537 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.000432202395576273 12\n",
            "val_loss_1 0.0004313365397081901 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0004320689305915663 13\n",
            "val_loss_1 0.00043120971720712924 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.00043194732212906384 14\n",
            "val_loss_1 0.0004310820530933991 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.1014119442200678e-05 0\n",
            "val_loss_2 1.1048038477571047e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.0994798278231165e-05 1\n",
            "val_loss_2 1.1025529498415749e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.0974850261641486e-05 2\n",
            "val_loss_2 1.100188280480813e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.095221848225988e-05 3\n",
            "val_loss_2 1.097716177454777e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.0930123153635444e-05 4\n",
            "val_loss_2 1.0951480885776016e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.0905684232160944e-05 5\n",
            "val_loss_2 1.0924921985809258e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.0881882787805106e-05 6\n",
            "val_loss_2 1.0897574245486195e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.0856121910780395e-05 7\n",
            "val_loss_2 1.0869470615612499e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.0841659562983145e-05 8\n",
            "val_loss_2 1.0866610054427145e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.0838373299645378e-05 9\n",
            "val_loss_2 1.086374046459708e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.083587851260515e-05 10\n",
            "val_loss_2 1.0860861386330216e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.0833187531436814e-05 11\n",
            "val_loss_2 1.0857973874698056e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.0830799804890018e-05 12\n",
            "val_loss_2 1.0855079149086528e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.0827504697475466e-05 13\n",
            "val_loss_2 1.0852174666946258e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.0825544112912135e-05 14\n",
            "val_loss_2 1.0849267157961495e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 2.7833989827577867e-07 0\n",
            "val_loss_3 2.772684505095039e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 2.7821264856058273e-07 1\n",
            "val_loss_3 2.771057713109889e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 2.7803983571661344e-07 2\n",
            "val_loss_3 2.769182809060301e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 2.7784940644214175e-07 3\n",
            "val_loss_3 2.767147860157836e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 2.7764530487520316e-07 4\n",
            "val_loss_3 2.765007155921316e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 2.7743077913311815e-07 5\n",
            "val_loss_3 2.762776493596274e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 2.7720905333540613e-07 6\n",
            "val_loss_3 2.7604622768505866e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 2.769722553331266e-07 7\n",
            "val_loss_3 2.7580566831830216e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 2.768373133708359e-07 8\n",
            "val_loss_3 2.757810310653319e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 2.7681073575529136e-07 9\n",
            "val_loss_3 2.757562840442065e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 2.7678683222752187e-07 10\n",
            "val_loss_3 2.7573144324495265e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 2.767616617358085e-07 11\n",
            "val_loss_3 2.757064977673269e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 2.7674123137303357e-07 12\n",
            "val_loss_3 2.756814869332821e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 2.767130300254827e-07 13\n",
            "val_loss_3 2.7565636232231494e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 2.7668666630069024e-07 14\n",
            "val_loss_3 2.756311418162299e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07871883680618968 0\n",
            "val_loss_4 0.0760333329887851 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.07358662397065059 1\n",
            "val_loss_4 0.07132293398472851 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.0694118202335001 2\n",
            "val_loss_4 0.06780679371627121 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.06630314633000728 3\n",
            "val_loss_4 0.06521508099419757 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.06400607964936539 4\n",
            "val_loss_4 0.0632914094794571 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.06230403517609036 5\n",
            "val_loss_4 0.061874461479827454 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.06104082098371161 6\n",
            "val_loss_4 0.06084054536680601 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.06011130244579186 7\n",
            "val_loss_4 0.06007040480355453 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.059687623942936543 8\n",
            "val_loss_4 0.06000700590633175 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.059623448478801154 9\n",
            "val_loss_4 0.059946942767467815 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.05956157930080912 10\n",
            "val_loss_4 0.05988820943326088 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.05950144451175409 11\n",
            "val_loss_4 0.05983108881323727 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.05944292690506377 12\n",
            "val_loss_4 0.05977566504228134 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.059385937395253975 13\n",
            "val_loss_4 0.05972204687407391 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.05933092617588781 14\n",
            "val_loss_4 0.05966946995697573 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.05927704726908248 15\n",
            "val_loss_4 0.059618857855983844 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.05922502815504914 16\n",
            "val_loss_4 0.05956967366289601 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.059195953322469116 17\n",
            "val_loss_4 0.05956487047667549 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.05919096327747496 18\n",
            "val_loss_4 0.059560053516917295 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.05918594843621333 19\n",
            "val_loss_4 0.05955530520640727 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.05918096490872399 20\n",
            "val_loss_4 0.05955052733563017 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.059175981332144814 21\n",
            "val_loss_4 0.05954578661739165 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.059171057201498956 22\n",
            "val_loss_4 0.05954093876901406 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.059166062192157774 23\n",
            "val_loss_4 0.05953624577109485 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.059161129642053174 24\n",
            "val_loss_4 0.05953155884226982 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.059156238160201394 25\n",
            "val_loss_4 0.05952681198999545 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.0591533995783153 26\n",
            "val_loss_4 0.05952634259751406 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.059152908774548076 27\n",
            "val_loss_4 0.05952586615000587 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.05915241453596944 28\n",
            "val_loss_4 0.05952539731838435 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.05915192031436924 29\n",
            "val_loss_4 0.059524923633848995 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.8170317 , 0.81057864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.03871367, 0.00964967, 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.82606566, 0.8119057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.16588467, 0.14302666, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.5606037 , 0.56306666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.64607966, 0.64800364, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[0.00027383, 0.00058249],\n",
            "       [0.00010086, 0.0004673 ],\n",
            "       [0.00034666, 0.00052745],\n",
            "       ...,\n",
            "       [0.00015351, 0.0004694 ],\n",
            "       [0.00032757, 0.00047421],\n",
            "       [0.00018553, 0.00052687]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.8170317 , 0.81057864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.03871367, 0.00964967, 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.82606566, 0.8119057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.16588467, 0.14302666, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.5606037 , 0.56306666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.64607966, 0.64800364, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[0.00027383, 0.00058249],\n",
            "       [0.00010086, 0.0004673 ],\n",
            "       [0.00034666, 0.00052745],\n",
            "       ...,\n",
            "       [0.00015351, 0.0004694 ],\n",
            "       [0.00032757, 0.00047421],\n",
            "       [0.00018553, 0.00052687]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.],\n",
            "       [12577., 12604.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.013609228966119352 epoch:  0\n",
            "reconstruction_loss:  0.013604810280607595 epoch:  0\n",
            "dcc_loss:  4.418694782996865e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.1607528775285867\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.013608905296083232 epoch:  1\n",
            "reconstruction_loss:  0.013604910804966779 epoch:  1\n",
            "dcc_loss:  3.994483530224199e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.013607923076747911 epoch:  2\n",
            "reconstruction_loss:  0.013605183534757608 epoch:  2\n",
            "dcc_loss:  2.7395734073504453e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.013598885473832697 epoch:  3\n",
            "reconstruction_loss:  0.01359691190292015 epoch:  3\n",
            "dcc_loss:  1.9735870094607413e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.013611753688101974 epoch:  4\n",
            "reconstruction_loss:  0.013610214372054223 epoch:  4\n",
            "dcc_loss:  1.5393053109640633e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.013606584142457393 epoch:  5\n",
            "reconstruction_loss:  0.013605277865641243 epoch:  5\n",
            "dcc_loss:  1.3063093271461495e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.013599852689662333 epoch:  6\n",
            "reconstruction_loss:  0.013598718158543079 epoch:  6\n",
            "dcc_loss:  1.1345386062443016e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.013603025729905342 epoch:  7\n",
            "reconstruction_loss:  0.013602005222232396 epoch:  7\n",
            "dcc_loss:  1.020527625098226e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.013604624887956019 epoch:  8\n",
            "reconstruction_loss:  0.013603722330830046 epoch:  8\n",
            "dcc_loss:  9.025720233487312e-07 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.0136059423050022 epoch:  9\n",
            "reconstruction_loss:  0.013605075897070286 epoch:  9\n",
            "dcc_loss:  8.66418356208214e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.013608877951256967 epoch:  10\n",
            "reconstruction_loss:  0.013607981352997475 epoch:  10\n",
            "dcc_loss:  8.966080393172104e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.057849481962388\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.0135934306044033 epoch:  11\n",
            "reconstruction_loss:  0.013592489227948394 epoch:  11\n",
            "dcc_loss:  9.413568700130169e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.1080856190796353\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.013595166845492284 epoch:  12\n",
            "reconstruction_loss:  0.013594254263312658 epoch:  12\n",
            "dcc_loss:  9.12584591254495e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.1172606511122094\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.830789089202881\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.261702438794467 0\n",
            "val_loss_0 0.23544688815643594 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.21101677294919835 1\n",
            "val_loss_0 0.188287125029579 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16970787683599098 2\n",
            "val_loss_0 0.15276694342659317 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.13875638657030545 3\n",
            "val_loss_0 0.12621833758337184 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.11564497518714233 4\n",
            "val_loss_0 0.10630643664137497 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09833236909309512 5\n",
            "val_loss_0 0.09152669713444264 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.0855071831774714 6\n",
            "val_loss_0 0.08058735525173544 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07608215659886323 7\n",
            "val_loss_0 0.07256577780290187 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.07180609705234252 8\n",
            "val_loss_0 0.0719124774689153 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.07118129573358625 9\n",
            "val_loss_0 0.07128259727344649 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.07057470253537788 10\n",
            "val_loss_0 0.07067319746062797 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.06997361145695834 11\n",
            "val_loss_0 0.07008227282063519 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.06938540312038663 12\n",
            "val_loss_0 0.06951077311947682 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.0688277602625368 13\n",
            "val_loss_0 0.06895731495790738 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.06827446747411411 14\n",
            "val_loss_0 0.06841989586546002 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005053703124591276 0\n",
            "val_loss_1 0.0005073252899395877 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.000504450445761189 1\n",
            "val_loss_1 0.0005061962655981598 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0005033581917322076 2\n",
            "val_loss_1 0.0005049946196355138 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0005022117480260826 3\n",
            "val_loss_1 0.0005037372200967517 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0005009800782868724 4\n",
            "val_loss_1 0.0005024297359560375 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0004996970242975332 5\n",
            "val_loss_1 0.0005010797232780576 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0004983958551879054 6\n",
            "val_loss_1 0.0004996894491635699 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0004969855256294312 7\n",
            "val_loss_1 0.000498258261197865 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0004961972712914415 8\n",
            "val_loss_1 0.0004981119513069611 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0004960422105407609 9\n",
            "val_loss_1 0.0004979650093878659 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0004958852965280544 10\n",
            "val_loss_1 0.0004978171456014817 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.0004957590070000402 11\n",
            "val_loss_1 0.0004976687925386578 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0004956215284405359 12\n",
            "val_loss_1 0.0004975195360578826 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0004954669471345875 13\n",
            "val_loss_1 0.0004973698353170522 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0004953271683545784 14\n",
            "val_loss_1 0.0004972191929681413 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.256402604713575e-05 0\n",
            "val_loss_2 1.2666773187179597e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.25417227925053e-05 1\n",
            "val_loss_2 1.2640788149542567e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.251868997615228e-05 2\n",
            "val_loss_2 1.261341368218265e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.2492742141962098e-05 3\n",
            "val_loss_2 1.2584740851319741e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.2467193660019964e-05 4\n",
            "val_loss_2 1.2554953444068612e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.2439013616749413e-05 5\n",
            "val_loss_2 1.2524160304315802e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.2411355860701086e-05 6\n",
            "val_loss_2 1.2492448485550221e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.2381587578177859e-05 7\n",
            "val_loss_2 1.2459767818545112e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.236489967918353e-05 8\n",
            "val_loss_2 1.2456437251829928e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.2361024098422472e-05 9\n",
            "val_loss_2 1.2453095596774426e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.2358102041440545e-05 10\n",
            "val_loss_2 1.2449741412024087e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.235514125005878e-05 11\n",
            "val_loss_2 1.2446376660703767e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.23521229610942e-05 12\n",
            "val_loss_2 1.2443006250625601e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.2348459413708425e-05 13\n",
            "val_loss_2 1.2439622982223768e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.234617339107099e-05 14\n",
            "val_loss_2 1.2436233859039878e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.1767834581020724e-07 0\n",
            "val_loss_3 3.180490730495827e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.175289314053409e-07 1\n",
            "val_loss_3 3.1785990180004255e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.17330835751388e-07 2\n",
            "val_loss_3 3.176441837280055e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.171120865370865e-07 3\n",
            "val_loss_3 3.174103098589759e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.1687727357142327e-07 4\n",
            "val_loss_3 3.171652649068702e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.1663137000024076e-07 5\n",
            "val_loss_3 3.1691083718726624e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.163805716407395e-07 6\n",
            "val_loss_3 3.166476810751157e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.161127903087719e-07 7\n",
            "val_loss_3 3.1637625640755276e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.159613520067334e-07 8\n",
            "val_loss_3 3.163486141105787e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.159315113801379e-07 9\n",
            "val_loss_3 3.1632087132416935e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.159049602151151e-07 10\n",
            "val_loss_3 3.162930251205731e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.158766235117426e-07 11\n",
            "val_loss_3 3.1626510423679594e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.15853546691611e-07 12\n",
            "val_loss_3 3.1623709597090096e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.158218098557451e-07 13\n",
            "val_loss_3 3.1620900081835386e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.1579160480724717e-07 14\n",
            "val_loss_3 3.161807978344718e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07763018610263549 0\n",
            "val_loss_4 0.07511225576540559 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.07202364229951022 1\n",
            "val_loss_4 0.06987217578759473 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.06746100546196243 2\n",
            "val_loss_4 0.06597052753656489 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.06407808995854863 3\n",
            "val_loss_4 0.06307582636312146 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.061572469914855966 4\n",
            "val_loss_4 0.06093294961634793 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.059725369342733724 5\n",
            "val_loss_4 0.05933916478428145 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.05835198339255338 6\n",
            "val_loss_4 0.058175068519856016 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.05734443937977419 7\n",
            "val_loss_4 0.057301877612593435 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.05688423538143342 8\n",
            "val_loss_4 0.05723021349466735 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.05681495973208278 9\n",
            "val_loss_4 0.05716194139820082 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.056748143730657986 10\n",
            "val_loss_4 0.05709466686481152 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.056682904605529216 11\n",
            "val_loss_4 0.05703030752162737 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.056619802296285666 12\n",
            "val_loss_4 0.056967396501646554 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.05655825604666842 13\n",
            "val_loss_4 0.056906473251700215 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.056498772860456244 14\n",
            "val_loss_4 0.05684692030049353 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.05644070910175279 15\n",
            "val_loss_4 0.05678914212835761 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.05638449043987776 16\n",
            "val_loss_4 0.056733114594702864 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.05635314481051462 17\n",
            "val_loss_4 0.05672763354733893 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.05634773811802488 18\n",
            "val_loss_4 0.05672216405368834 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.0563423238760367 19\n",
            "val_loss_4 0.05671671019326866 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.05633692758579173 20\n",
            "val_loss_4 0.05671127402059817 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.05633154928752148 21\n",
            "val_loss_4 0.05670584386979154 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.056326212128370565 22\n",
            "val_loss_4 0.05670035393722651 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.056320826183898616 23\n",
            "val_loss_4 0.056694998174150094 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.05631549520010012 24\n",
            "val_loss_4 0.05668961904978412 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.05631018709179125 25\n",
            "val_loss_4 0.05668424211572354 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.056307166226095266 26\n",
            "val_loss_4 0.05668370318252198 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.05630663271787074 27\n",
            "val_loss_4 0.056683157459964086 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.056306097332052656 28\n",
            "val_loss_4 0.05668261644862911 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.05630556045729956 29\n",
            "val_loss_4 0.05668208014851705 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.775365, 0.768912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.997047, 0.967983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.784399, 0.770239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.124218, 0.10136 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.518937, 0.5214  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.604413, 0.606337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), array([[0.00030946, 0.0006308 ],\n",
            "       [0.00026854, 0.00068431],\n",
            "       [0.00037691, 0.00057595],\n",
            "       ...,\n",
            "       [0.00019105, 0.00051889],\n",
            "       [0.00036153, 0.00051998],\n",
            "       [0.00022487, 0.00057717]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.775365, 0.768912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.997047, 0.967983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.784399, 0.770239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.124218, 0.10136 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.518937, 0.5214  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.604413, 0.606337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), 'Z': array([[0.00030946, 0.0006308 ],\n",
            "       [0.00026854, 0.00068431],\n",
            "       [0.00037691, 0.00057595],\n",
            "       ...,\n",
            "       [0.00019105, 0.00051889],\n",
            "       [0.00036153, 0.00051998],\n",
            "       [0.00022487, 0.00057717]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.012923372882869785 epoch:  0\n",
            "reconstruction_loss:  0.012918595341029874 epoch:  0\n",
            "dcc_loss:  4.777554448067551e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.111596748196622\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.012893059617629711 epoch:  1\n",
            "reconstruction_loss:  0.012889039090949406 epoch:  1\n",
            "dcc_loss:  4.020521975849473e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.012896249644752653 epoch:  2\n",
            "reconstruction_loss:  0.012893584447591182 epoch:  2\n",
            "dcc_loss:  2.6652184645049214e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.012899235379298326 epoch:  3\n",
            "reconstruction_loss:  0.01289731232483701 epoch:  3\n",
            "dcc_loss:  1.923051560526304e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.012890848049195906 epoch:  4\n",
            "reconstruction_loss:  0.012889278434173885 epoch:  4\n",
            "dcc_loss:  1.5696242769024426e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.012895239994811613 epoch:  5\n",
            "reconstruction_loss:  0.012893833969791983 epoch:  5\n",
            "dcc_loss:  1.4060314217889518e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.01288192027189371 epoch:  6\n",
            "reconstruction_loss:  0.012880667297872043 epoch:  6\n",
            "dcc_loss:  1.2529718222025152e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.012895307814732342 epoch:  7\n",
            "reconstruction_loss:  0.012894201297503691 epoch:  7\n",
            "dcc_loss:  1.1064844364162508e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.012887502189641726 epoch:  8\n",
            "reconstruction_loss:  0.012886475183290358 epoch:  8\n",
            "dcc_loss:  1.0270073008442425e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.012893731521347792 epoch:  9\n",
            "reconstruction_loss:  0.012892771161025527 epoch:  9\n",
            "dcc_loss:  9.60378592174919e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.012895217261497447 epoch:  10\n",
            "reconstruction_loss:  0.012894303392177404 epoch:  10\n",
            "dcc_loss:  9.138361381242795e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.0592048958148876\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.012880849775621678 epoch:  11\n",
            "reconstruction_loss:  0.012879972413263505 epoch:  11\n",
            "dcc_loss:  8.773823659116297e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.1309689809652657\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.012887754773020708 epoch:  12\n",
            "reconstruction_loss:  0.012886894288049794 epoch:  12\n",
            "dcc_loss:  8.604849746666912e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.0865713649029336\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 8.381032705307007\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.266963275527973 0\n",
            "val_loss_0 0.24034474754673554 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.21392159599485663 1\n",
            "val_loss_0 0.19080168344139486 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.17069664598987677 2\n",
            "val_loss_0 0.1535024451661601 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.138336718844243 3\n",
            "val_loss_0 0.12553766660286017 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.1141366775258438 4\n",
            "val_loss_0 0.1045948956899711 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09603813203193973 5\n",
            "val_loss_0 0.0890004579125418 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.08261670803443227 6\n",
            "val_loss_0 0.07748256088861762 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07279295665039884 7\n",
            "val_loss_0 0.06903279282258921 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.0683432550233289 8\n",
            "val_loss_0 0.06834625319921649 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.06768315778114178 9\n",
            "val_loss_0 0.06768568376707955 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.06706074313367191 10\n",
            "val_loss_0 0.06704592381835739 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.06643709151782402 11\n",
            "val_loss_0 0.06642553970076014 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.0658298480927643 12\n",
            "val_loss_0 0.06582561280525241 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.06524835647194825 13\n",
            "val_loss_0 0.06524435032259454 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.06468344359400015 14\n",
            "val_loss_0 0.06468054909028834 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005472895730509032 0\n",
            "val_loss_1 0.0005514835859479149 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0005462746821417902 1\n",
            "val_loss_1 0.0005502370200909545 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0005450758577693455 2\n",
            "val_loss_1 0.0005489096084975662 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0005438092074808747 3\n",
            "val_loss_1 0.0005475217672972608 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0005424550664989484 4\n",
            "val_loss_1 0.0005460803757868885 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0005410448149755342 5\n",
            "val_loss_1 0.000544593875460982 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0005396118980533338 6\n",
            "val_loss_1 0.0005430649168436635 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0005380658819562089 7\n",
            "val_loss_1 0.0005414898099751166 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0005371938713213631 8\n",
            "val_loss_1 0.000541328416920301 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0005370299188934328 9\n",
            "val_loss_1 0.0005411663125512646 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0005368572744117516 10\n",
            "val_loss_1 0.0005410032489089068 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.0005367124192524167 11\n",
            "val_loss_1 0.0005408394362234323 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0005365704849665401 12\n",
            "val_loss_1 0.0005406746930456034 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0005363950966508117 13\n",
            "val_loss_1 0.0005405093672376852 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0005362432024937726 14\n",
            "val_loss_1 0.0005403429525498472 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.3593858682726555e-05 0\n",
            "val_loss_2 1.3748659817769879e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.3569323828349252e-05 1\n",
            "val_loss_2 1.372015393035283e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.3544432629838713e-05 2\n",
            "val_loss_2 1.3690096939359158e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.3515917932257052e-05 3\n",
            "val_loss_2 1.3658648454622925e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.3488011636258587e-05 4\n",
            "val_loss_2 1.3625986352264025e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.345729170391361e-05 5\n",
            "val_loss_2 1.3592218918364795e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.3426790388845715e-05 6\n",
            "val_loss_2 1.3557405858153492e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.339419740366112e-05 7\n",
            "val_loss_2 1.3521506330849811e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.3375895753605633e-05 8\n",
            "val_loss_2 1.3517851393065443e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.3371640086330976e-05 9\n",
            "val_loss_2 1.3514183565247609e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.336840204874026e-05 10\n",
            "val_loss_2 1.351050229391617e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.3365261157132473e-05 11\n",
            "val_loss_2 1.3506811304972562e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.3362005028049408e-05 12\n",
            "val_loss_2 1.350311082038538e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.3357907594199793e-05 13\n",
            "val_loss_2 1.349939825868868e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.3355511303552437e-05 14\n",
            "val_loss_2 1.3495679523670572e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.437057780149737e-07 0\n",
            "val_loss_3 3.451960609746679e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.43540278950077e-07 1\n",
            "val_loss_3 3.4498876757475473e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.433253228977356e-07 2\n",
            "val_loss_3 3.4475389448670445e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.4308719763851507e-07 3\n",
            "val_loss_3 3.4449915340761946e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.4283130108947703e-07 4\n",
            "val_loss_3 3.4423256930399844e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.4256535959081403e-07 5\n",
            "val_loss_3 3.4395627132672056e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.422935312895849e-07 6\n",
            "val_loss_3 3.4367047896705375e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.4200288754356265e-07 7\n",
            "val_loss_3 3.4337615752714506e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.418403609760862e-07 8\n",
            "val_loss_3 3.433462021264213e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.418066253559783e-07 9\n",
            "val_loss_3 3.433161638027704e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.4177818877534534e-07 10\n",
            "val_loss_3 3.432860254851496e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.4174763467457645e-07 11\n",
            "val_loss_3 3.432558216759829e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.4172283651767576e-07 12\n",
            "val_loss_3 3.43225534673635e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.416890570104837e-07 13\n",
            "val_loss_3 3.4319515929823815e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.4165591868838306e-07 14\n",
            "val_loss_3 3.431647144675704e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07485233325676204 0\n",
            "val_loss_4 0.07210909786579538 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.06893045054673912 1\n",
            "val_loss_4 0.06650257863919067 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.06410582163481414 2\n",
            "val_loss_4 0.062328002620754074 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.060538437196435486 3\n",
            "val_loss_4 0.059214829194186795 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.057893638627803515 4\n",
            "val_loss_4 0.05692107578158001 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.05595066633828801 5\n",
            "val_loss_4 0.05520786667942245 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.05450977218729245 6\n",
            "val_loss_4 0.053949140401914265 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.05345095674982808 7\n",
            "val_loss_4 0.05301066403603403 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.05297263558737341 8\n",
            "val_loss_4 0.05293283718394591 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.0529000440203292 9\n",
            "val_loss_4 0.052858935069830026 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.052830053284287415 10\n",
            "val_loss_4 0.05278594700970854 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.05276164469782241 11\n",
            "val_loss_4 0.05271621263348357 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.05269578382421577 12\n",
            "val_loss_4 0.052647626142588735 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.05263120329483087 13\n",
            "val_loss_4 0.052581900615605234 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.05256903165929041 14\n",
            "val_loss_4 0.05251736499121185 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.05250835266744171 15\n",
            "val_loss_4 0.052454644232610516 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.05244947342855553 16\n",
            "val_loss_4 0.05239387443392477 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.05241682447687221 17\n",
            "val_loss_4 0.0523879093172622 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.05241116514562509 18\n",
            "val_loss_4 0.05238197263324317 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.052405502103113344 19\n",
            "val_loss_4 0.052376044710445706 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.052399870007492015 20\n",
            "val_loss_4 0.052370131033488955 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.05239424625529556 21\n",
            "val_loss_4 0.052364259964171 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.05238866817582992 22\n",
            "val_loss_4 0.052358305592402794 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.05238304016167695 23\n",
            "val_loss_4 0.052352501808557945 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.0523774709818645 24\n",
            "val_loss_4 0.05234666428456405 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.052371916336700484 25\n",
            "val_loss_4 0.05234083638964856 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.05236877398438996 26\n",
            "val_loss_4 0.052340245030808565 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.05236821266105824 27\n",
            "val_loss_4 0.05233965307568597 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.052367651929757196 28\n",
            "val_loss_4 0.05233906320460058 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.05236708499874337 29\n",
            "val_loss_4 0.052338473510628834 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.7336983 , 0.72724533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.9553803 , 0.9263163 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.74273235, 0.7285723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.08255133, 0.05969333, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.47727033, 0.47973335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.56274635, 0.5646703 , 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[0.0003105 , 0.00066549],\n",
            "       [0.0002685 , 0.00071739],\n",
            "       [0.00037631, 0.00061098],\n",
            "       ...,\n",
            "       [0.0001934 , 0.00055866],\n",
            "       [0.00036214, 0.00055875],\n",
            "       [0.00022596, 0.0006143 ]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.7336983 , 0.72724533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.9553803 , 0.9263163 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.74273235, 0.7285723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.08255133, 0.05969333, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.47727033, 0.47973335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.56274635, 0.5646703 , 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[0.0003105 , 0.00066549],\n",
            "       [0.0002685 , 0.00071739],\n",
            "       [0.00037631, 0.00061098],\n",
            "       ...,\n",
            "       [0.0001934 , 0.00055866],\n",
            "       [0.00036214, 0.00055875],\n",
            "       [0.00022596, 0.0006143 ]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.01195251492323002 epoch:  0\n",
            "reconstruction_loss:  0.011948129994186645 epoch:  0\n",
            "dcc_loss:  4.384917538933567e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.1425736143966065\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.011942933021475026 epoch:  1\n",
            "reconstruction_loss:  0.011938411339106664 epoch:  1\n",
            "dcc_loss:  4.521704199784057e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.010448180186014752 epoch:  2\n",
            "reconstruction_loss:  0.010406214012176429 epoch:  2\n",
            "dcc_loss:  4.1966204658684095e-05 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.00806721676314784 epoch:  3\n",
            "reconstruction_loss:  0.008042709501615319 epoch:  3\n",
            "dcc_loss:  2.4507250106300112e-05 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.006528375556112313 epoch:  4\n",
            "reconstruction_loss:  0.006500287562460405 epoch:  4\n",
            "dcc_loss:  2.808797142041514e-05 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.006118750875861827 epoch:  5\n",
            "reconstruction_loss:  0.006105751023186941 epoch:  5\n",
            "dcc_loss:  1.2999837165905043e-05 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.005940941220161266 epoch:  6\n",
            "reconstruction_loss:  0.005933800074884478 epoch:  6\n",
            "dcc_loss:  7.141138757525557e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.005723769375645385 epoch:  7\n",
            "reconstruction_loss:  0.00571877005801724 epoch:  7\n",
            "dcc_loss:  4.9993271371109785e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.005570782344421508 epoch:  8\n",
            "reconstruction_loss:  0.005567152031130372 epoch:  8\n",
            "dcc_loss:  3.630300181621799e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.0054464271443611725 epoch:  9\n",
            "reconstruction_loss:  0.005443548010153868 epoch:  9\n",
            "dcc_loss:  2.8791405762993587e-06 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.005274791847973171 epoch:  10\n",
            "reconstruction_loss:  0.00527291399101511 epoch:  10\n",
            "dcc_loss:  1.8778591802092553e-06 epoch:  10\n",
            "epoch:  10 DBL:  0.9917377978475873\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.00503805418553686 epoch:  11\n",
            "reconstruction_loss:  0.0050346654343699036 epoch:  11\n",
            "dcc_loss:  3.3887548912197513e-06 epoch:  11\n",
            "epoch:  11 DBL:  1.1267285860251366\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.004733375940606661 epoch:  12\n",
            "reconstruction_loss:  0.0047209382476511 epoch:  12\n",
            "dcc_loss:  1.243768925556918e-05 epoch:  12\n",
            "epoch:  12 DBL:  1.1551956639775147\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.734793663024902\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.2662049419864205 0\n",
            "val_loss_0 0.23804761811783876 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.21259048240668801 1\n",
            "val_loss_0 0.18822301189797427 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16891691275586887 2\n",
            "val_loss_0 0.15074245603356612 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.1362347122014933 3\n",
            "val_loss_0 0.12263026231347854 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.11178280503357955 4\n",
            "val_loss_0 0.10162475712231335 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09350990997065428 5\n",
            "val_loss_0 0.08599398245004769 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.07996200038295287 6\n",
            "val_loss_0 0.07447271464649358 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07005310613862424 7\n",
            "val_loss_0 0.06604135208859119 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.06556949252922471 8\n",
            "val_loss_0 0.06535794924230282 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.06490571012342695 9\n",
            "val_loss_0 0.06470019565732846 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.0642777247795274 10\n",
            "val_loss_0 0.06406413473295146 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.06364927025279146 11\n",
            "val_loss_0 0.06344711323004518 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.06304390267398399 12\n",
            "val_loss_0 0.06285096325346294 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.06246121979627585 13\n",
            "val_loss_0 0.062272837087393944 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.061889918602063304 14\n",
            "val_loss_0 0.06171284702779185 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005535283774681638 0\n",
            "val_loss_1 0.000555319999929122 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0005524932732754859 1\n",
            "val_loss_1 0.0005540539729726249 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0005512711399730897 2\n",
            "val_loss_1 0.0005527052858795693 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0005499834676588653 3\n",
            "val_loss_1 0.0005512964024714736 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0005486027402917721 4\n",
            "val_loss_1 0.0005498335988440871 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0005471694514703127 5\n",
            "val_loss_1 0.0005483261429300317 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0005457099483532608 6\n",
            "val_loss_1 0.0005467784594340035 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0005441417374899047 7\n",
            "val_loss_1 0.0005451820466243817 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0005432538357253275 8\n",
            "val_loss_1 0.0005450184172330787 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0005430868961480606 9\n",
            "val_loss_1 0.0005448539909226274 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0005429109840787265 10\n",
            "val_loss_1 0.0005446885571860827 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.0005427623994421555 11\n",
            "val_loss_1 0.0005445223444262475 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0005426208333415051 12\n",
            "val_loss_1 0.0005443551410292165 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0005424421290288195 13\n",
            "val_loss_1 0.0005441873861814771 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0005422838880625321 14\n",
            "val_loss_1 0.0005440185383027168 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.3756798737353115e-05 0\n",
            "val_loss_2 1.3860238807111416e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.37317987410007e-05 1\n",
            "val_loss_2 1.3831303733324528e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.3706449902382024e-05 2\n",
            "val_loss_2 1.3800765231909252e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.3677549268973369e-05 3\n",
            "val_loss_2 1.3768798952092743e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.364901147051464e-05 4\n",
            "val_loss_2 1.37356001210206e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.361770718913039e-05 5\n",
            "val_loss_2 1.3701285806948244e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.3586878342429449e-05 6\n",
            "val_loss_2 1.3665926620392232e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.3553754737418346e-05 7\n",
            "val_loss_2 1.3629481498603658e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.3534946673179595e-05 8\n",
            "val_loss_2 1.3625769337603514e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.3530621898184762e-05 9\n",
            "val_loss_2 1.3622046702280075e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.3527364380214169e-05 10\n",
            "val_loss_2 1.3618311156744209e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.3524025974886434e-05 11\n",
            "val_loss_2 1.3614565364619062e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.3520755586389857e-05 12\n",
            "val_loss_2 1.3610807963824616e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.351665999576962e-05 13\n",
            "val_loss_2 1.3607040887217281e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.3514188389625414e-05 14\n",
            "val_loss_2 1.3603267249564172e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.478036009798414e-07 0\n",
            "val_loss_3 3.480471896207504e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.4763544171841035e-07 1\n",
            "val_loss_3 3.478369061759289e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.4741708638400216e-07 2\n",
            "val_loss_3 3.475989205278362e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.471737372748503e-07 3\n",
            "val_loss_3 3.473408160479801e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.469148485615716e-07 4\n",
            "val_loss_3 3.4707060720799664e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.4664458861887277e-07 5\n",
            "val_loss_3 3.4679056292511104e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.46368924913133e-07 6\n",
            "val_loss_3 3.465007763468592e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.4607344872745624e-07 7\n",
            "val_loss_3 3.4620234429898807e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.459077639879501e-07 8\n",
            "val_loss_3 3.461719937419143e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.4587404050191e-07 9\n",
            "val_loss_3 3.461415485509079e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.458446163703617e-07 10\n",
            "val_loss_3 3.461110028254238e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.45813845137645e-07 11\n",
            "val_loss_3 3.460803715645574e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.457888085796822e-07 12\n",
            "val_loss_3 3.460496896310712e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.4575476927277836e-07 13\n",
            "val_loss_3 3.4601890018154627e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.457208241407708e-07 14\n",
            "val_loss_3 3.459880335745123e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07210048752725508 0\n",
            "val_loss_4 0.06907932503996107 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.0661104937306123 1\n",
            "val_loss_4 0.06347120247438857 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.061228726812155104 2\n",
            "val_loss_4 0.05930467431688838 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.05761721423586922 3\n",
            "val_loss_4 0.05620749519101791 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.05494328393770389 4\n",
            "val_loss_4 0.053926056200747634 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.052975447287966285 5\n",
            "val_loss_4 0.05223841175271667 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.05151994080739228 6\n",
            "val_loss_4 0.050993913793478826 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.05044869499599025 7\n",
            "val_loss_4 0.050073637873548335 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.04996398150002556 8\n",
            "val_loss_4 0.04999784020656829 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.04989075358083401 9\n",
            "val_loss_4 0.049925451103462096 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.04981999559333312 10\n",
            "val_loss_4 0.049854297633688725 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.04975085617009398 11\n",
            "val_loss_4 0.04978603916316516 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.0496842105915652 12\n",
            "val_loss_4 0.04971935574656054 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.04961914671822108 13\n",
            "val_loss_4 0.049654993758479316 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.04955623444159794 14\n",
            "val_loss_4 0.049592170209023165 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.04949496812615749 15\n",
            "val_loss_4 0.049531018081019305 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.04943540663613704 16\n",
            "val_loss_4 0.0494719206779109 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.04940244820890573 17\n",
            "val_loss_4 0.04946611054749383 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.04939672790698794 18\n",
            "val_loss_4 0.04946031525329629 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.049390999370528724 19\n",
            "val_loss_4 0.049454539364850274 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.04938531029675284 20\n",
            "val_loss_4 0.04944877044287423 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.04937961822406844 21\n",
            "val_loss_4 0.04944306998122513 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.04937397390323506 22\n",
            "val_loss_4 0.04943731374058595 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.04936830755177379 23\n",
            "val_loss_4 0.04943162784358212 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.04936268025256061 24\n",
            "val_loss_4 0.04942594252514953 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.04935707521835156 25\n",
            "val_loss_4 0.04942026605649531 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.049353878368214504 26\n",
            "val_loss_4 0.04941969419786745 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.0493533117970697 27\n",
            "val_loss_4 0.04941912070389028 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.049352746557255724 28\n",
            "val_loss_4 0.0494185457280623 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.04935217506384089 29\n",
            "val_loss_4 0.04941797934224602 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.6920317 , 0.68557864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.9137137 , 0.8846497 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.70106566, 0.6869057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.04088467, 0.01802667, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.43560368, 0.43806666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.52107966, 0.52300364, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[0.00031846, 0.00065792],\n",
            "       [0.00026969, 0.00071282],\n",
            "       [0.00037999, 0.00060447],\n",
            "       ...,\n",
            "       [0.00020257, 0.00056042],\n",
            "       [0.00036676, 0.00055607],\n",
            "       [0.00022761, 0.00061246]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.6920317 , 0.68557864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.9137137 , 0.8846497 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.70106566, 0.6869057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.04088467, 0.01802667, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.43560368, 0.43806666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.52107966, 0.52300364, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[0.00031846, 0.00065792],\n",
            "       [0.00026969, 0.00071282],\n",
            "       [0.00037999, 0.00060447],\n",
            "       ...,\n",
            "       [0.00020257, 0.00056042],\n",
            "       [0.00036676, 0.00055607],\n",
            "       [0.00022761, 0.00061246]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.011231457084028444 epoch:  0\n",
            "reconstruction_loss:  0.01122671618855397 epoch:  0\n",
            "dcc_loss:  4.740871441766797e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.1148094974252278\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.011223914008521352 epoch:  1\n",
            "reconstruction_loss:  0.011219788467414815 epoch:  1\n",
            "dcc_loss:  4.125511275725919e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.011223405082650083 epoch:  2\n",
            "reconstruction_loss:  0.01122062200610812 epoch:  2\n",
            "dcc_loss:  2.783083067326276e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.011223034559351348 epoch:  3\n",
            "reconstruction_loss:  0.011221096334960415 epoch:  3\n",
            "dcc_loss:  1.9382020906185324e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.011223811865279884 epoch:  4\n",
            "reconstruction_loss:  0.011222129431877063 epoch:  4\n",
            "dcc_loss:  1.6824509575218996e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.01122825477597324 epoch:  5\n",
            "reconstruction_loss:  0.011226763357639028 epoch:  5\n",
            "dcc_loss:  1.491424173007647e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.011230808305430842 epoch:  6\n",
            "reconstruction_loss:  0.011229524609868886 epoch:  6\n",
            "dcc_loss:  1.2837002268112222e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.011232167646674767 epoch:  7\n",
            "reconstruction_loss:  0.011230967846720952 epoch:  7\n",
            "dcc_loss:  1.1997684020884553e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.011222736026411917 epoch:  8\n",
            "reconstruction_loss:  0.011221625952777366 epoch:  8\n",
            "dcc_loss:  1.1100842127439402e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.011222168955303655 epoch:  9\n",
            "reconstruction_loss:  0.011221124846004248 epoch:  9\n",
            "dcc_loss:  1.0441301134730172e-06 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.011222831661100774 epoch:  10\n",
            "reconstruction_loss:  0.011221825393334132 epoch:  10\n",
            "dcc_loss:  1.0062576657290458e-06 epoch:  10\n",
            "epoch:  10 DBL:  1.1357768252348046\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.011226332053799833 epoch:  11\n",
            "reconstruction_loss:  0.011225356931127325 epoch:  11\n",
            "dcc_loss:  9.751582445530477e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.123221864450469\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.011227981737640525 epoch:  12\n",
            "reconstruction_loss:  0.011227073911512182 epoch:  12\n",
            "dcc_loss:  9.078518280871274e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.0881644651544142\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}], 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)}, {10: 9}, {10: 1.0881644651544142}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 8.025096654891968\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.26260612006497125 0\n",
            "val_loss_0 0.23487808263717477 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.20937975695882505 1\n",
            "val_loss_0 0.18537644491709546 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16604375394693655 2\n",
            "val_loss_0 0.1481202961242331 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.1336118330139832 3\n",
            "val_loss_0 0.12019083960640072 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.10935587855328137 4\n",
            "val_loss_0 0.0993307414339197 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09124273674821494 5\n",
            "val_loss_0 0.08380088971694562 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.07781243432646498 6\n",
            "val_loss_0 0.07235258249728313 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.06799083822273493 7\n",
            "val_loss_0 0.06397626242598906 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.06354608644125932 8\n",
            "val_loss_0 0.06329604117834436 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.06288824388627241 9\n",
            "val_loss_0 0.06264231517533114 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.06226379622390718 10\n",
            "val_loss_0 0.06200950062596099 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.061641780604745566 11\n",
            "val_loss_0 0.06139625502652865 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.06103625536122939 12\n",
            "val_loss_0 0.0608026649029527 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.06046534858973981 13\n",
            "val_loss_0 0.0602274361684942 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.05989519317235687 14\n",
            "val_loss_0 0.05967029972849104 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005407828422999428 0\n",
            "val_loss_1 0.0005431893032941548 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0005397707849672146 1\n",
            "val_loss_1 0.000541950520843846 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0005385759231333348 2\n",
            "val_loss_1 0.0005406312611826331 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0005373178193823461 3\n",
            "val_loss_1 0.0005392521683815955 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0005359665820359213 4\n",
            "val_loss_1 0.0005378208428299536 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0005345657073727851 5\n",
            "val_loss_1 0.0005363467476543234 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0005331372625613784 6\n",
            "val_loss_1 0.000534834840468525 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0005316076268601425 7\n",
            "val_loss_1 0.0005332761544331306 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0005307417683938478 8\n",
            "val_loss_1 0.0005331164506919351 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.000530577048470255 9\n",
            "val_loss_1 0.0005329559057531805 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0005304072355060927 10\n",
            "val_loss_1 0.0005327945086395106 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.0005302625945957028 11\n",
            "val_loss_1 0.0005326321947782429 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0005301205589294169 12\n",
            "val_loss_1 0.0005324689720103462 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0005299455716254286 13\n",
            "val_loss_1 0.0005323051516683964 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0005297931739458822 14\n",
            "val_loss_1 0.0005321403463162989 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.3451577768171095e-05 0\n",
            "val_loss_2 1.356756154408543e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.3427212310763927e-05 1\n",
            "val_loss_2 1.3539202046398794e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.3402190657840503e-05 2\n",
            "val_loss_2 1.3509254487364301e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.33740319538586e-05 3\n",
            "val_loss_2 1.347787407059525e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.3345894181861666e-05 4\n",
            "val_loss_2 1.3445290046410669e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.3315266992474733e-05 5\n",
            "val_loss_2 1.341161029443092e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.3285047910652871e-05 6\n",
            "val_loss_2 1.3376935362105926e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.3252682908837682e-05 7\n",
            "val_loss_2 1.334119336476174e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.3234223464109724e-05 8\n",
            "val_loss_2 1.3337553726955596e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.3229961577614235e-05 9\n",
            "val_loss_2 1.3333903557171979e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.3226699287882776e-05 10\n",
            "val_loss_2 1.3330241752774679e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.3223533333041224e-05 11\n",
            "val_loss_2 1.3326570449851099e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.3220151853276664e-05 12\n",
            "val_loss_2 1.3322886759926777e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.321625378691941e-05 13\n",
            "val_loss_2 1.3319193949111056e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.3213841097973547e-05 14\n",
            "val_loss_2 1.3315494954884446e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.401759470759882e-07 0\n",
            "val_loss_3 3.4078386660498815e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.4001126268040506e-07 1\n",
            "val_loss_3 3.405779118630849e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.3979750913135647e-07 2\n",
            "val_loss_3 3.4034435428128763e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.395588916018208e-07 3\n",
            "val_loss_3 3.400909491126305e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.3930518232198447e-07 4\n",
            "val_loss_3 3.3982576690644896e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.3903925037231744e-07 5\n",
            "val_loss_3 3.395505720608441e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.387687709069018e-07 6\n",
            "val_loss_3 3.3926560519193596e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.384778693605152e-07 7\n",
            "val_loss_3 3.389721791034378e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.3831505976710496e-07 8\n",
            "val_loss_3 3.3894233608332427e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.3828249232064964e-07 9\n",
            "val_loss_3 3.38912379241246e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.382537717791768e-07 10\n",
            "val_loss_3 3.3888234470115064e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.3822348100091347e-07 11\n",
            "val_loss_3 3.3885222016648247e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.3819895240673517e-07 12\n",
            "val_loss_3 3.388220264017551e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.381656606473906e-07 13\n",
            "val_loss_3 3.387917434532168e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.3813214943009685e-07 14\n",
            "val_loss_3 3.387613965445717e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.06975792796320289 0\n",
            "val_loss_4 0.06674163650408411 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.06382697596658828 1\n",
            "val_loss_4 0.061178652389586065 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.058997257404198415 2\n",
            "val_loss_4 0.05703093034175421 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.055419479003202855 3\n",
            "val_loss_4 0.05394948201870956 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.05277137056858242 4\n",
            "val_loss_4 0.051677792467377644 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.05081992998887523 5\n",
            "val_loss_4 0.04999605066212346 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.04937821675716124 6\n",
            "val_loss_4 0.04875266205419073 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.04831584945895397 7\n",
            "val_loss_4 0.0478324089996316 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.04783310551212208 8\n",
            "val_loss_4 0.04775665889356858 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.04776044457857063 9\n",
            "val_loss_4 0.04768411386551456 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.04769021288604268 10\n",
            "val_loss_4 0.04761297915207603 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.0476216041939465 11\n",
            "val_loss_4 0.04754448493148559 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.04755538854853941 12\n",
            "val_loss_4 0.04747772845550309 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.04749083891953879 13\n",
            "val_loss_4 0.047413075463802044 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.04742829164274736 14\n",
            "val_loss_4 0.04735013809521489 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.047367484880048244 15\n",
            "val_loss_4 0.047288882361633464 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.04730834103629833 16\n",
            "val_loss_4 0.047229700097474736 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.04727564159849762 17\n",
            "val_loss_4 0.04722386489957335 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.0472699617683728 18\n",
            "val_loss_4 0.04721804627950898 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.04726426650263516 19\n",
            "val_loss_4 0.04721228350337636 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.047258630851507895 20\n",
            "val_loss_4 0.047206485948027975 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.04725297133275979 21\n",
            "val_loss_4 0.047200774824137544 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.047247360057868254 22\n",
            "val_loss_4 0.04719500628000394 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.0472417302620868 23\n",
            "val_loss_4 0.04718928228585542 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.04723614430093593 24\n",
            "val_loss_4 0.047183570300011926 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.04723057466049113 25\n",
            "val_loss_4 0.04717786694550665 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.04722738256837619 26\n",
            "val_loss_4 0.04717729490386136 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.047226819181795415 27\n",
            "val_loss_4 0.04717671879450606 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.04722625831909604 28\n",
            "val_loss_4 0.04717613991037035 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.04722568990468365 29\n",
            "val_loss_4 0.04717557214897144 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.650365, 0.643912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.872047, 0.842983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.659399, 0.645239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.999218, 0.97636 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.393937, 0.3964  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.479413, 0.481337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), array([[0.00030553, 0.00064224],\n",
            "       [0.00025101, 0.00070038],\n",
            "       [0.00036466, 0.00058834],\n",
            "       ...,\n",
            "       [0.00028564, 0.00072072],\n",
            "       [0.00035191, 0.00054342],\n",
            "       [0.0002082 , 0.0005977 ]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.650365, 0.643912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.872047, 0.842983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.659399, 0.645239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.999218, 0.97636 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.393937, 0.3964  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.479413, 0.481337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), 'Z': array([[0.00030553, 0.00064224],\n",
            "       [0.00025101, 0.00070038],\n",
            "       [0.00036466, 0.00058834],\n",
            "       ...,\n",
            "       [0.00028564, 0.00072072],\n",
            "       [0.00035191, 0.00054342],\n",
            "       [0.0002082 , 0.0005977 ]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.0107355454953307 epoch:  0\n",
            "reconstruction_loss:  0.010731144805109005 epoch:  0\n",
            "dcc_loss:  4.400687341545072e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.127709431072422\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.010737493223050934 epoch:  1\n",
            "reconstruction_loss:  0.010733219249749256 epoch:  1\n",
            "dcc_loss:  4.273977919513257e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.010724103954510927 epoch:  2\n",
            "reconstruction_loss:  0.010721257131526074 epoch:  2\n",
            "dcc_loss:  2.8468224021769157e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.01072365235927861 epoch:  3\n",
            "reconstruction_loss:  0.0107215850532881 epoch:  3\n",
            "dcc_loss:  2.0673226036960743e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.010714386909785205 epoch:  4\n",
            "reconstruction_loss:  0.01071273850066539 epoch:  4\n",
            "dcc_loss:  1.6483941823464225e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.010721976654952503 epoch:  5\n",
            "reconstruction_loss:  0.010720567543473369 epoch:  5\n",
            "dcc_loss:  1.4091041437342464e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.010723414657370867 epoch:  6\n",
            "reconstruction_loss:  0.010722127842642922 epoch:  6\n",
            "dcc_loss:  1.2868228408666483e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.01072629838483766 epoch:  7\n",
            "reconstruction_loss:  0.010725096442578165 epoch:  7\n",
            "dcc_loss:  1.2019771207675024e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.010719453368550078 epoch:  8\n",
            "reconstruction_loss:  0.010718309474656488 epoch:  8\n",
            "dcc_loss:  1.1439085625480196e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.01071878473085674 epoch:  9\n",
            "reconstruction_loss:  0.010717687141529314 epoch:  9\n",
            "dcc_loss:  1.097606940319975e-06 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.010724170614534331 epoch:  10\n",
            "reconstruction_loss:  0.010723136116963192 epoch:  10\n",
            "dcc_loss:  1.0344694680126036e-06 epoch:  10\n",
            "epoch:  10 DBL:  1.1246394368514294\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.010723607724244863 epoch:  11\n",
            "reconstruction_loss:  0.010722680472022867 epoch:  11\n",
            "dcc_loss:  9.272208484711632e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.1495188175968498\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.01072047125830303 epoch:  12\n",
            "reconstruction_loss:  0.010719614666380318 epoch:  12\n",
            "dcc_loss:  8.565828338685489e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.0714495073472743\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}], 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)}, {10: 9}, {10: 1.0881644651544142}], 21: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 7}, {10: 1.0714495073472743}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.803095579147339\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.258234411356979 0\n",
            "val_loss_0 0.2311142521029229 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.20571410146622265 1\n",
            "val_loss_0 0.18219958368553035 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16295727098099153 2\n",
            "val_loss_0 0.1453797245507384 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.1309592228770008 3\n",
            "val_loss_0 0.11779896142743469 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.10703597766644629 4\n",
            "val_loss_0 0.09718658302788122 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.08917598355327856 5\n",
            "val_loss_0 0.08184181969677778 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.07593776208433373 6\n",
            "val_loss_0 0.07051875650079806 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.06625074741094611 7\n",
            "val_loss_0 0.06223203414215717 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.06186271264239663 8\n",
            "val_loss_0 0.061558229237700414 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.061212675187714234 9\n",
            "val_loss_0 0.06091120007275212 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.06059644896753725 10\n",
            "val_loss_0 0.0602841348329739 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.059980572991454775 11\n",
            "val_loss_0 0.05967636409373556 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.059382596908656334 12\n",
            "val_loss_0 0.059087880681934525 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.05881985631011693 13\n",
            "val_loss_0 0.05851716750113976 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.0582559076227808 14\n",
            "val_loss_0 0.05796539823033164 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005232275278954062 0\n",
            "val_loss_1 0.0005261502082005037 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0005222494794079073 1\n",
            "val_loss_1 0.0005249517637514546 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0005210926191545237 2\n",
            "val_loss_1 0.0005236764636965212 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0005198759254569461 3\n",
            "val_loss_1 0.0005223420252115798 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0005185677479615496 4\n",
            "val_loss_1 0.0005209584946990627 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0005172156321540801 5\n",
            "val_loss_1 0.000519533071551832 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0005158322039007699 6\n",
            "val_loss_1 0.0005180722868354864 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0005143573882760838 7\n",
            "val_loss_1 0.0005165669435551081 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0005135198902108243 8\n",
            "val_loss_1 0.000516412793987072 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.000513361937768372 9\n",
            "val_loss_1 0.0005162578244382153 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0005131965076686392 10\n",
            "val_loss_1 0.0005161019446912758 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.000513057072292491 11\n",
            "val_loss_1 0.0005159452423806083 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0005129219353903334 12\n",
            "val_loss_1 0.0005157876907546728 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0005127525861554691 13\n",
            "val_loss_1 0.0005156294307664108 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0005126059756427044 14\n",
            "val_loss_1 0.0005154702493259698 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.3029803368143394e-05 0\n",
            "val_loss_2 1.3153613110806918e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.300634202398317e-05 1\n",
            "val_loss_2 1.312608309823268e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.2981971983778811e-05 2\n",
            "val_loss_2 1.309698803553549e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.2954726797163199e-05 3\n",
            "val_loss_2 1.3066502121939854e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.2927310824329479e-05 4\n",
            "val_loss_2 1.3034835674131835e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.2897564325073935e-05 5\n",
            "val_loss_2 1.3002120978187876e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.28683662222879e-05 6\n",
            "val_loss_2 1.2968445142368357e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.2836919178981019e-05 7\n",
            "val_loss_2 1.2933760508286091e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.2819018303041893e-05 8\n",
            "val_loss_2 1.293022993921779e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.2814917177777389e-05 9\n",
            "val_loss_2 1.2926687638965054e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.2811769029135387e-05 10\n",
            "val_loss_2 1.2923137529453532e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.2808647816873524e-05 11\n",
            "val_loss_2 1.2919575870368245e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.2805375634889078e-05 12\n",
            "val_loss_2 1.2916002784224328e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.2801648530737564e-05 13\n",
            "val_loss_2 1.291242129786627e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.2799299861730209e-05 14\n",
            "val_loss_2 1.2908833625214616e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.2961751362024203e-07 0\n",
            "val_loss_3 3.3045925464404876e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.2945794312456095e-07 1\n",
            "val_loss_3 3.3025978306973167e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.2925029850276285e-07 2\n",
            "val_loss_3 3.300330151235252e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.29019195095361e-07 3\n",
            "val_loss_3 3.297869351887899e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.2877316806588217e-07 4\n",
            "val_loss_3 3.2952924176522583e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.2851418941294457e-07 5\n",
            "val_loss_3 3.292616144812713e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.282518556381747e-07 6\n",
            "val_loss_3 3.289844194860167e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.2796844531435385e-07 7\n",
            "val_loss_3 3.2869894926908503e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.2780994626896503e-07 8\n",
            "val_loss_3 3.286699049395445e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.277785809963326e-07 9\n",
            "val_loss_3 3.286407345815683e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.277510862519582e-07 10\n",
            "val_loss_3 3.2861148913803e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.277213762587819e-07 11\n",
            "val_loss_3 3.2858215437555384e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.2769734770421394e-07 12\n",
            "val_loss_3 3.2855275542775926e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.276647611822945e-07 13\n",
            "val_loss_3 3.285232641431908e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.2763275024715865e-07 14\n",
            "val_loss_3 3.2849369079149934e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.06769527941366424 0\n",
            "val_loss_4 0.06469062971993218 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.06186419573497999 1\n",
            "val_loss_4 0.05920182837382927 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.05711426258745626 2\n",
            "val_loss_4 0.05510495931925562 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.05359171880439572 3\n",
            "val_loss_4 0.05205987230911497 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.0509860021747146 4\n",
            "val_loss_4 0.049805098120270744 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.049063508485476276 5\n",
            "val_loss_4 0.04813628169676959 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.04764286133375901 6\n",
            "val_loss_4 0.04690472596869605 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.046597655095956515 7\n",
            "val_loss_4 0.04598690941929817 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.04611877619668016 8\n",
            "val_loss_4 0.045911684490610415 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.04604718897830893 9\n",
            "val_loss_4 0.045839439221493986 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.04597785385792642 10\n",
            "val_loss_4 0.045768826308463906 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.04591033936955347 11\n",
            "val_loss_4 0.04570042502530592 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.045845011092134076 12\n",
            "val_loss_4 0.045633892485910287 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.04578132975990846 13\n",
            "val_loss_4 0.045569552070270816 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.04571967865700742 14\n",
            "val_loss_4 0.0455068624364404 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.04565966728494128 15\n",
            "val_loss_4 0.04544582053216059 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.045601318601877265 16\n",
            "val_loss_4 0.04538679605854672 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.04556911063996838 17\n",
            "val_loss_4 0.04538098299985076 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.04556350939859428 18\n",
            "val_loss_4 0.04537515031105932 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.04555788559546251 19\n",
            "val_loss_4 0.04536940987486091 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.045552325450392554 20\n",
            "val_loss_4 0.04536363596418391 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.045546753414142806 21\n",
            "val_loss_4 0.045357938466236446 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.04554121281309807 22\n",
            "val_loss_4 0.045352221054811674 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.04553567832798123 23\n",
            "val_loss_4 0.04534649327633498 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.04553016323482188 24\n",
            "val_loss_4 0.04534079946235129 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.04552467102305771 25\n",
            "val_loss_4 0.045335120189397726 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.04552152704155553 26\n",
            "val_loss_4 0.045334547728583474 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.0455209691128037 27\n",
            "val_loss_4 0.045333969676881886 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.04552041240723745 28\n",
            "val_loss_4 0.045333391660603024 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.04551985218639677 29\n",
            "val_loss_4 0.04533282118936537 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.6086983 , 0.60224533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.8303803 , 0.8013163 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.61773235, 0.6035723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.95755136, 0.93469334, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.35227033, 0.35473335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.43774635, 0.43967032, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[0.00029849, 0.00062246],\n",
            "       [0.00023946, 0.00068365],\n",
            "       [0.00035556, 0.00056871],\n",
            "       ...,\n",
            "       [0.00027568, 0.00070015],\n",
            "       [0.00034333, 0.00052481],\n",
            "       [0.00019509, 0.00058009]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.6086983 , 0.60224533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.8303803 , 0.8013163 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.61773235, 0.6035723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.95755136, 0.93469334, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.35227033, 0.35473335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.43774635, 0.43967032, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[0.00029849, 0.00062246],\n",
            "       [0.00023946, 0.00068365],\n",
            "       [0.00035556, 0.00056871],\n",
            "       ...,\n",
            "       [0.00027568, 0.00070015],\n",
            "       [0.00034333, 0.00052481],\n",
            "       [0.00019509, 0.00058009]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.010348421584149857 epoch:  0\n",
            "reconstruction_loss:  0.010343911296404149 epoch:  0\n",
            "dcc_loss:  4.510273719607047e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.0884112845012384\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.01032742750429075 epoch:  1\n",
            "reconstruction_loss:  0.010323004282133598 epoch:  1\n",
            "dcc_loss:  4.423217663206035e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.010322166926568515 epoch:  2\n",
            "reconstruction_loss:  0.010319097127941814 epoch:  2\n",
            "dcc_loss:  3.0697839070726597e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.010322852820999229 epoch:  3\n",
            "reconstruction_loss:  0.010320783566075003 epoch:  3\n",
            "dcc_loss:  2.0692659588742238e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.010324280671975888 epoch:  4\n",
            "reconstruction_loss:  0.010322729687675742 epoch:  4\n",
            "dcc_loss:  1.5509950125665604e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.01032968292964214 epoch:  5\n",
            "reconstruction_loss:  0.010328388151761004 epoch:  5\n",
            "dcc_loss:  1.2947628519184165e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.010324355670439475 epoch:  6\n",
            "reconstruction_loss:  0.010323254778782289 epoch:  6\n",
            "dcc_loss:  1.100897966985196e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.010319011730170507 epoch:  7\n",
            "reconstruction_loss:  0.010318037005679789 epoch:  7\n",
            "dcc_loss:  9.747352656983754e-07 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.01031867117177002 epoch:  8\n",
            "reconstruction_loss:  0.01031780073372752 epoch:  8\n",
            "dcc_loss:  8.704574355997368e-07 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.010325552707066319 epoch:  9\n",
            "reconstruction_loss:  0.010324769087897876 epoch:  9\n",
            "dcc_loss:  7.83638311558134e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.010319028525034505 epoch:  10\n",
            "reconstruction_loss:  0.010318302357136008 epoch:  10\n",
            "dcc_loss:  7.261407383132265e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.0535045810609172\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.010322747244262078 epoch:  11\n",
            "reconstruction_loss:  0.010322049573246901 epoch:  11\n",
            "dcc_loss:  6.976868652049139e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.0885802530598143\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.01032386851370717 epoch:  12\n",
            "reconstruction_loss:  0.010323207347217112 epoch:  12\n",
            "dcc_loss:  6.611412207718694e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.121244221724319\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}], 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)}, {10: 9}, {10: 1.0881644651544142}], 21: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 7}, {10: 1.0714495073472743}], 22: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.121244221724319}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.8145670890808105\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.253048214532511 0\n",
            "val_loss_0 0.22576279387987927 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.20154668042914406 1\n",
            "val_loss_0 0.1779126914811399 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.15961884991933437 2\n",
            "val_loss_0 0.14191889411012645 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.128251594955534 3\n",
            "val_loss_0 0.11496411425706513 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.10480224822972582 4\n",
            "val_loss_0 0.0948385991829133 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.087301519391476 5\n",
            "val_loss_0 0.07987675943342518 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.07432998068845509 6\n",
            "val_loss_0 0.06882703680952452 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.06482904740500992 7\n",
            "val_loss_0 0.0607497088386782 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.06052365246818782 8\n",
            "val_loss_0 0.0600919587015161 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.05988403284448138 9\n",
            "val_loss_0 0.05946112571729911 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.05927987176145043 10\n",
            "val_loss_0 0.0588499130991106 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.058675290205977566 11\n",
            "val_loss_0 0.05825667140530142 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.058087215328401394 12\n",
            "val_loss_0 0.05768238316200048 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.057531063207122056 13\n",
            "val_loss_0 0.05712605129024117 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.05698322391150953 14\n",
            "val_loss_0 0.05658672118881236 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005012086119617572 0\n",
            "val_loss_1 0.0005027281756275931 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0005002729709588754 1\n",
            "val_loss_1 0.0005015841030376253 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0004991671055588839 2\n",
            "val_loss_1 0.0005003674976138779 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0004980049738212233 3\n",
            "val_loss_1 0.0004990942229272535 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0004967541937537478 4\n",
            "val_loss_1 0.0004977746801148279 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0004954605784794441 5\n",
            "val_loss_1 0.0004964155807228472 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0004941382450083671 6\n",
            "val_loss_1 0.0004950231516795755 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.000492730416381001 7\n",
            "val_loss_1 0.0004935898685609412 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.000491932643449703 8\n",
            "val_loss_1 0.0004934431535687015 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0004917803837545512 9\n",
            "val_loss_1 0.0004932956886570015 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0004916228366145852 10\n",
            "val_loss_1 0.000493147353720652 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.000491492917934359 11\n",
            "val_loss_1 0.0004929982561347992 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0004913597580799434 12\n",
            "val_loss_1 0.0004928484584426982 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0004911995213342699 13\n",
            "val_loss_1 0.0004926978602338281 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0004910599729108049 14\n",
            "val_loss_1 0.0004925464009021139 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.2504927647224238e-05 0\n",
            "val_loss_2 1.2594922374972794e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.248245636919224e-05 1\n",
            "val_loss_2 1.2568472762829765e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.245899369543781e-05 2\n",
            "val_loss_2 1.2540500535482187e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.2432720429210992e-05 3\n",
            "val_loss_2 1.2511213173030429e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.2406495953099136e-05 4\n",
            "val_loss_2 1.2480770193061864e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.237780373374002e-05 5\n",
            "val_loss_2 1.2449330917139657e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.234990018136438e-05 6\n",
            "val_loss_2 1.2416996242962877e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.231975111468483e-05 7\n",
            "val_loss_2 1.2383749142369232e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.2302414351730614e-05 8\n",
            "val_loss_2 1.238036578172071e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.2298499528521456e-05 9\n",
            "val_loss_2 1.2376971100673792e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.2295567519722887e-05 10\n",
            "val_loss_2 1.237357065853286e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.2292487229045582e-05 11\n",
            "val_loss_2 1.2370157940375486e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.2289355655590833e-05 12\n",
            "val_loss_2 1.2366735390738935e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.2285853791726566e-05 13\n",
            "val_loss_2 1.2363307061270762e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.2283461292968348e-05 14\n",
            "val_loss_2 1.2359871955994992e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.1649502697974476e-07 0\n",
            "val_loss_3 3.1659420916464523e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.163419037009007e-07 1\n",
            "val_loss_3 3.1640337810529125e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.161427085494718e-07 2\n",
            "val_loss_3 3.1618561506648743e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.159205884494691e-07 3\n",
            "val_loss_3 3.159493775562152e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.1568418519158945e-07 4\n",
            "val_loss_3 3.1570172514874154e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.154349388353932e-07 5\n",
            "val_loss_3 3.154443567956639e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.1518220893730756e-07 6\n",
            "val_loss_3 3.1517762707019406e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.1490941445565285e-07 7\n",
            "val_loss_3 3.14902724594345e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.1475640618434225e-07 8\n",
            "val_loss_3 3.148747263728893e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.1472635575298484e-07 9\n",
            "val_loss_3 3.148466222569188e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.1469987338804183e-07 10\n",
            "val_loss_3 3.148184326956507e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.146711694073609e-07 11\n",
            "val_loss_3 3.147901380055873e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.146478158240402e-07 12\n",
            "val_loss_3 3.147617899854067e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.146160700981181e-07 13\n",
            "val_loss_3 3.1473334341261084e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.1458568457613003e-07 14\n",
            "val_loss_3 3.147048089171893e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.06593173194333747 0\n",
            "val_loss_4 0.06283462733006705 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.06024356544081276 1\n",
            "val_loss_4 0.05751424119844679 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.05560967147816849 2\n",
            "val_loss_4 0.05355028857708733 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.05217218512102089 3\n",
            "val_loss_4 0.050606219768051866 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.049627291348488846 4\n",
            "val_loss_4 0.048424809627978814 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.047746850793483085 5\n",
            "val_loss_4 0.04681099978424668 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.04635525498504883 6\n",
            "val_loss_4 0.04562367579166183 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.045331766593964734 7\n",
            "val_loss_4 0.04473718404533746 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.04485983777830755 8\n",
            "val_loss_4 0.04466491258111317 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.04478963266515193 9\n",
            "val_loss_4 0.04459541964030304 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.04472158668788103 10\n",
            "val_loss_4 0.04452744677733695 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.044655346958193495 11\n",
            "val_loss_4 0.04446149001668448 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.044591191609444615 12\n",
            "val_loss_4 0.04439748135851416 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.044528670400826954 13\n",
            "val_loss_4 0.0443355938605669 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.04446815496188236 14\n",
            "val_loss_4 0.044275084566531585 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.0444091730215446 15\n",
            "val_loss_4 0.04421640828062917 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.044351842399824246 16\n",
            "val_loss_4 0.044159866485703386 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.04432032335936037 17\n",
            "val_loss_4 0.04415426759872875 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.04431481632689802 18\n",
            "val_loss_4 0.04414865107123523 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.044309292605002264 19\n",
            "val_loss_4 0.04414312284079671 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.04430382136566651 20\n",
            "val_loss_4 0.04413759853637728 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.04429835743185646 21\n",
            "val_loss_4 0.044132106295431094 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.04429291723353072 22\n",
            "val_loss_4 0.0441266029848822 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.044287484715363555 23\n",
            "val_loss_4 0.0441210810006515 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.04428206278140571 24\n",
            "val_loss_4 0.04411560827172502 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.04427666970857435 25\n",
            "val_loss_4 0.04411014784038917 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.044273569914558934 26\n",
            "val_loss_4 0.044109598262657645 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.04427302064038027 27\n",
            "val_loss_4 0.04410904382020471 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.04427247417539456 28\n",
            "val_loss_4 0.04410848662658652 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.04427192482001468 29\n",
            "val_loss_4 0.04410793653522543 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.5670317 , 0.56057864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.7887137 , 0.7596497 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.57606566, 0.5619057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.9158847 , 0.89302665, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.31060368, 0.31306666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.39607966, 0.39800367, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[0.00028969, 0.00059192],\n",
            "       [0.00022661, 0.00065451],\n",
            "       [0.00034502, 0.00053823],\n",
            "       ...,\n",
            "       [0.0002623 , 0.00066711],\n",
            "       [0.00033335, 0.00049736],\n",
            "       [0.00018098, 0.00055366]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.5670317 , 0.56057864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.7887137 , 0.7596497 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.57606566, 0.5619057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.9158847 , 0.89302665, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.31060368, 0.31306666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.39607966, 0.39800367, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[0.00028969, 0.00059192],\n",
            "       [0.00022661, 0.00065451],\n",
            "       [0.00034502, 0.00053823],\n",
            "       ...,\n",
            "       [0.0002623 , 0.00066711],\n",
            "       [0.00033335, 0.00049736],\n",
            "       [0.00018098, 0.00055366]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.010054586873423855 epoch:  0\n",
            "reconstruction_loss:  0.010049864230282975 epoch:  0\n",
            "dcc_loss:  4.7226446677155165e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.1021781719877373\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.010040866769965514 epoch:  1\n",
            "reconstruction_loss:  0.010036371179899792 epoch:  1\n",
            "dcc_loss:  4.495611023794652e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.010036063563172229 epoch:  2\n",
            "reconstruction_loss:  0.0100332086625253 epoch:  2\n",
            "dcc_loss:  2.8549112818779244e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.010042261180479625 epoch:  3\n",
            "reconstruction_loss:  0.010040241669740001 epoch:  3\n",
            "dcc_loss:  2.0194714146239204e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.0100338195675412 epoch:  4\n",
            "reconstruction_loss:  0.010032283890975503 epoch:  4\n",
            "dcc_loss:  1.5356956575229988e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.010040296558749847 epoch:  5\n",
            "reconstruction_loss:  0.010039058431478164 epoch:  5\n",
            "dcc_loss:  1.238154261882098e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.010032674563381026 epoch:  6\n",
            "reconstruction_loss:  0.010031605246670523 epoch:  6\n",
            "dcc_loss:  1.0693436516667677e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.010037209741799027 epoch:  7\n",
            "reconstruction_loss:  0.010036243923261905 epoch:  7\n",
            "dcc_loss:  9.65823091200128e-07 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.010030943248412225 epoch:  8\n",
            "reconstruction_loss:  0.010030026157085412 epoch:  8\n",
            "dcc_loss:  9.171030382732581e-07 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.010034670801695312 epoch:  9\n",
            "reconstruction_loss:  0.010033829158673908 epoch:  9\n",
            "dcc_loss:  8.416346231247524e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.010028735813163714 epoch:  10\n",
            "reconstruction_loss:  0.010027898015913798 epoch:  10\n",
            "dcc_loss:  8.377867444780584e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.0614525431984505\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.010033994590217329 epoch:  11\n",
            "reconstruction_loss:  0.010033236773731517 epoch:  11\n",
            "dcc_loss:  7.578175309163958e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.0394309668087685\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.010032796389833783 epoch:  12\n",
            "reconstruction_loss:  0.010032066019667966 epoch:  12\n",
            "dcc_loss:  7.303708185035043e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.051417670560449\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}], 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)}, {10: 9}, {10: 1.0881644651544142}], 21: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 7}, {10: 1.0714495073472743}], 22: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.121244221724319}], 23: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.051417670560449}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.326040029525757\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.24571837007225794 0\n",
            "val_loss_0 0.21883706271459485 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.19597155513015077 1\n",
            "val_loss_0 0.17270272700986847 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.15547540181221486 2\n",
            "val_loss_0 0.13800748660806242 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.12517992422437924 3\n",
            "val_loss_0 0.11205754255530573 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.1025404123735005 4\n",
            "val_loss_0 0.09269118790770106 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.08564539407477274 5\n",
            "val_loss_0 0.07829924968031432 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.073118962996379 6\n",
            "val_loss_0 0.0676623728191229 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.06393150383429202 7\n",
            "val_loss_0 0.059884563176973875 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.059760758907406016 8\n",
            "val_loss_0 0.059250719426125996 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.05913847149928769 9\n",
            "val_loss_0 0.05864254403534669 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.0585556832781357 10\n",
            "val_loss_0 0.05805289476118072 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.057969543340257994 11\n",
            "val_loss_0 0.05748090447884543 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.057397904563065885 12\n",
            "val_loss_0 0.05692718319134365 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.05685698965420976 13\n",
            "val_loss_0 0.05639062685487577 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.056326514724699246 14\n",
            "val_loss_0 0.05587007619570439 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.00046751975937195866 0\n",
            "val_loss_1 0.0004679921493004334 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0004666494704076097 1\n",
            "val_loss_1 0.0004669311294489335 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.00046562130899669324 2\n",
            "val_loss_1 0.0004658039884159132 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0004645447790611223 3\n",
            "val_loss_1 0.0004646249909987806 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0004633830421816723 4\n",
            "val_loss_1 0.0004634043232614555 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0004621838769919251 5\n",
            "val_loss_1 0.0004621474516504959 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0004609580019975077 6\n",
            "val_loss_1 0.0004608595448401469 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0004596537063613864 7\n",
            "val_loss_1 0.00045953589732679 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0004589151076305666 8\n",
            "val_loss_1 0.0004594004496680381 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.00045877542157738555 9\n",
            "val_loss_1 0.00045926433534246297 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0004586302822368657 10\n",
            "val_loss_1 0.00045912749134557597 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.00045850857223535783 11\n",
            "val_loss_1 0.000458989933866671 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0004583838724604683 12\n",
            "val_loss_1 0.00045885168504495346 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.000458237726432496 13\n",
            "val_loss_1 0.0004587127755063242 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.00045810705375746275 14\n",
            "val_loss_1 0.0004585730722771807 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.1704528671580217e-05 0\n",
            "val_loss_2 1.176773409415243e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.168347820098091e-05 1\n",
            "val_loss_2 1.1742969873703893e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.166152553831043e-05 2\n",
            "val_loss_2 1.1716773803027607e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.163687069515228e-05 3\n",
            "val_loss_2 1.1689386826151817e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.1612366410808573e-05 4\n",
            "val_loss_2 1.1660880682173663e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.158560765553499e-05 5\n",
            "val_loss_2 1.1631431435430407e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.1559420360211741e-05 6\n",
            "val_loss_2 1.1601184815512091e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.1531307476091719e-05 7\n",
            "val_loss_2 1.157011791208863e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.15150769498449e-05 8\n",
            "val_loss_2 1.1566959986493723e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.151141714082207e-05 9\n",
            "val_loss_2 1.1563791443881426e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.1508703212686994e-05 10\n",
            "val_loss_2 1.1560617983367507e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.1505740575192283e-05 11\n",
            "val_loss_2 1.1557433159214558e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.1502878296153116e-05 12\n",
            "val_loss_2 1.1554240494093024e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.1499642046733695e-05 13\n",
            "val_loss_2 1.1551041283780615e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.1497376989873016e-05 14\n",
            "val_loss_2 1.1547836633796252e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 2.963264269181054e-07 0\n",
            "val_loss_3 2.9591350880764367e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 2.961840924869325e-07 1\n",
            "val_loss_3 2.957361835103909e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 2.9599807963860103e-07 2\n",
            "val_loss_3 2.955328144233686e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 2.9579044176389655e-07 3\n",
            "val_loss_3 2.9531215340617643e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 2.9556957843116576e-07 4\n",
            "val_loss_3 2.950804209395394e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 2.953361851570628e-07 5\n",
            "val_loss_3 2.948392702570913e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 2.9509916882604304e-07 6\n",
            "val_loss_3 2.9458913241391813e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 2.948427109756996e-07 7\n",
            "val_loss_3 2.943307592565711e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 2.9469891878933365e-07 8\n",
            "val_loss_3 2.943044053954105e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 2.9467050279071537e-07 9\n",
            "val_loss_3 2.942779428471107e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 2.946455636122367e-07 10\n",
            "val_loss_3 2.942513846739469e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 2.9461847292753167e-07 11\n",
            "val_loss_3 2.9422472389435835e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 2.9459662713659364e-07 12\n",
            "val_loss_3 2.941979949657264e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 2.9456684384621387e-07 13\n",
            "val_loss_3 2.9417115059360586e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 2.9453790622348087e-07 14\n",
            "val_loss_3 2.941442091102246e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.06457165510044052 0\n",
            "val_loss_4 0.061491537360967435 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.05912025441375653 1\n",
            "val_loss_4 0.05641130486683309 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.05467180161634758 2\n",
            "val_loss_4 0.052636212549761245 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.05137272134429161 3\n",
            "val_loss_4 0.04983099573623926 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.048927756871064404 4\n",
            "val_loss_4 0.047753739045274614 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.047120133057394646 5\n",
            "val_loss_4 0.04621669201389923 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.04578053092671305 6\n",
            "val_loss_4 0.045087112423782306 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.04479504255327859 7\n",
            "val_loss_4 0.04424501212222263 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.04433887759492098 8\n",
            "val_loss_4 0.04417656434082947 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.04427122851073547 9\n",
            "val_loss_4 0.04411062415211023 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.04420561235286218 10\n",
            "val_loss_4 0.04404616256121977 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.0441417774170959 11\n",
            "val_loss_4 0.043983505693413374 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.04407986623076243 12\n",
            "val_loss_4 0.043922778968765695 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.04401954495905649 13\n",
            "val_loss_4 0.04386411838693966 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.04396116724175976 14\n",
            "val_loss_4 0.043806818905751416 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.04390433181199413 15\n",
            "val_loss_4 0.043751049166908355 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.04384895711716452 16\n",
            "val_loss_4 0.043697582171099314 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.04381863416986329 17\n",
            "val_loss_4 0.043692263141392525 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.043813316804615886 18\n",
            "val_loss_4 0.043686937216061233 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.04380798743930525 19\n",
            "val_loss_4 0.04368170693800113 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.0438027119343916 20\n",
            "val_loss_4 0.0436764780650426 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.043797441027312214 21\n",
            "val_loss_4 0.0436712696369023 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.043792189759854036 22\n",
            "val_loss_4 0.043666041661319564 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.043786944098970666 23\n",
            "val_loss_4 0.043660809015840435 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.04378171113574279 24\n",
            "val_loss_4 0.043655624285505616 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.043776506089123625 25\n",
            "val_loss_4 0.04365044937907422 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.04377351443667212 26\n",
            "val_loss_4 0.043649927094555126 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.04377298851042652 27\n",
            "val_loss_4 0.04364940697672628 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.043772463979734534 28\n",
            "val_loss_4 0.043648884060501866 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.04377193981260253 29\n",
            "val_loss_4 0.04364836655805115 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.525365, 0.518912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.747047, 0.717983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.534399, 0.520239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.874218, 0.85136 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.268937, 0.2714  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.354413, 0.356337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), array([[0.00026456, 0.00056541],\n",
            "       [0.0001984 , 0.00062972],\n",
            "       [0.00032084, 0.0005174 ],\n",
            "       ...,\n",
            "       [0.00023416, 0.00063934],\n",
            "       [0.00031085, 0.0004783 ],\n",
            "       [0.00015099, 0.00053294]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.525365, 0.518912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.747047, 0.717983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.534399, 0.520239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.874218, 0.85136 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.268937, 0.2714  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.354413, 0.356337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), 'Z': array([[0.00026456, 0.00056541],\n",
            "       [0.0001984 , 0.00062972],\n",
            "       [0.00032084, 0.0005174 ],\n",
            "       ...,\n",
            "       [0.00023416, 0.00063934],\n",
            "       [0.00031085, 0.0004783 ],\n",
            "       [0.00015099, 0.00053294]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.009963333410894915 epoch:  0\n",
            "reconstruction_loss:  0.009958949746130399 epoch:  0\n",
            "dcc_loss:  4.383664660016459e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.0758392572826894\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.00994367201329004 epoch:  1\n",
            "reconstruction_loss:  0.009939479313915835 epoch:  1\n",
            "dcc_loss:  4.192685707639735e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.009940008827162663 epoch:  2\n",
            "reconstruction_loss:  0.0099370481199118 epoch:  2\n",
            "dcc_loss:  2.9607240023274014e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.009946322345743682 epoch:  3\n",
            "reconstruction_loss:  0.009944091151355696 epoch:  3\n",
            "dcc_loss:  2.2311900515171665e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.009943766280156082 epoch:  4\n",
            "reconstruction_loss:  0.009941972887000664 epoch:  4\n",
            "dcc_loss:  1.7933876807701815e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.009938091417884569 epoch:  5\n",
            "reconstruction_loss:  0.009936654265088015 epoch:  5\n",
            "dcc_loss:  1.4371739879335344e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.00993955942584281 epoch:  6\n",
            "reconstruction_loss:  0.009938364799770864 epoch:  6\n",
            "dcc_loss:  1.1945931397222497e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.00995172427946659 epoch:  7\n",
            "reconstruction_loss:  0.009950715190206558 epoch:  7\n",
            "dcc_loss:  1.0091006575741254e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.009938989286873383 epoch:  8\n",
            "reconstruction_loss:  0.009938102908294968 epoch:  8\n",
            "dcc_loss:  8.863575359131178e-07 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.009934100519023551 epoch:  9\n",
            "reconstruction_loss:  0.009933324123101201 epoch:  9\n",
            "dcc_loss:  7.763853143619886e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.00994004912487848 epoch:  10\n",
            "reconstruction_loss:  0.009939373489116396 epoch:  10\n",
            "dcc_loss:  6.756658249769669e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.1057553197607781\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.00994510745566008 epoch:  11\n",
            "reconstruction_loss:  0.009944468547153843 epoch:  11\n",
            "dcc_loss:  6.389251164395113e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.059575187182028\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.009942569809275253 epoch:  12\n",
            "reconstruction_loss:  0.009941943153385677 epoch:  12\n",
            "dcc_loss:  6.266383211208009e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.0758480133185298\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}], 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)}, {10: 9}, {10: 1.0881644651544142}], 21: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 7}, {10: 1.0714495073472743}], 22: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.121244221724319}], 23: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.051417670560449}], 24: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 3}, {10: 1.0758480133185298}]}\n"
          ]
        }
      ],
      "source": [
        "k=3\n",
        "for start_number in range(0,25):\n",
        "  change_time(start_number)\n",
        "  print('starting model with k=', k)\n",
        "  #construct the mkNN graph\n",
        "  compressed_data(dp.oyster.name, N, k, preprocess='none', algo='knn', isPCA=None, format='mat')\n",
        "\n",
        "  #creating hyperparameter dict\n",
        "  args = edict()\n",
        "\n",
        "  #setting some pretraining hyperparameters\n",
        "  set_pretraining_hypers(args)\n",
        "  args.k = k\n",
        "\n",
        "  #initializing net for pretraining\n",
        "  index = len(dp.oyster.dim) - 1\n",
        "  net = None\n",
        "\n",
        "  #start pretraining\n",
        "  index, net = pretraining.main(args)\n",
        "  print(index)\n",
        "\n",
        "  # extracting pretrained features\n",
        "  args.feat = 'pretrained'\n",
        "  args.torchmodel = 'checkpoint_{}.pth.tar'.format(index)\n",
        "  print('Extracted features:')\n",
        "  print(extract_feature.main(args, net=net))\n",
        "\n",
        "  # merging the features and mkNN graph\n",
        "  args.g = 'pretrained.mat'\n",
        "  args.out = 'pretrained'\n",
        "  args.feat = 'pretrained.pkl'\n",
        "  print('Merged features:')\n",
        "  print(copyGraph.main(args))\n",
        "\n",
        "  #defining necessary dicts for model selection and overfitting detection\n",
        "  cluster_dict = dict()\n",
        "  cluster_count_dict = dict()\n",
        "  dbl_dict = dict()\n",
        "\n",
        "#PHASE 1: starting the overfitting detection phase\n",
        "  #setting some training hyperparameters\n",
        "  set_training_hypers(args)\n",
        "\n",
        "  #start training for 10 epochs, then detect overfitting\n",
        "  clusters, dbl = DCC.main(args, net=net,start_number)\n",
        "  if cluster_count(clusters) == 1 or cluster_count(clusters) == 2:\n",
        "    print(cluster_count(clusters))\n",
        "    print('Overfitting Detected!')\n",
        "    continue\n",
        "  else:\n",
        "    print('Phase 1 done!')\n",
        "    cluster_dict[10] = clusters\n",
        "    cluster_count_dict[10] = cluster_count(clusters)\n",
        "    dbl_dict[10] = dbl\n",
        "\n",
        "    #storing the three dicts in the super dict for further evaluation\n",
        "    super_dict[start_number] = [cluster_dict, cluster_count_dict, dbl_dict]\n",
        "    print('done algorithm on model with k= ', k)\n",
        "    print (super_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2lqb1qx5KxD",
        "outputId": "35bb4185-7f9d-4fe4-f913-d0ae622f554c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)},\n",
              "  {10: 7},\n",
              "  {10: 0.9994927461794473}],\n",
              " 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.0750217119714527}],\n",
              " 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.0636585666466636}],\n",
              " 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.0360127844193727}],\n",
              " 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.0337662922969948}],\n",
              " 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 5},\n",
              "  {10: 1.1006713356321056}],\n",
              " 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.1048031905668751}],\n",
              " 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.120594644769965}],\n",
              " 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)},\n",
              "  {10: 9},\n",
              "  {10: 1.2089862779012048}],\n",
              " 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)},\n",
              "  {10: 11},\n",
              "  {10: 1.2488489052534233}],\n",
              " 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)},\n",
              "  {10: 11},\n",
              "  {10: 1.0769178501514183}],\n",
              " 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)},\n",
              "  {10: 7},\n",
              "  {10: 1.1540558331327075}],\n",
              " 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)},\n",
              "  {10: 7},\n",
              "  {10: 1.1823069403580344}],\n",
              " 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)},\n",
              "  {10: 6},\n",
              "  {10: 1.0995683257591817}],\n",
              " 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)},\n",
              "  {10: 9},\n",
              "  {10: 1.0441639365109077}],\n",
              " 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)},\n",
              "  {10: 7},\n",
              "  {10: 1.0932239105455974}],\n",
              " 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)},\n",
              "  {10: 5},\n",
              "  {10: 1.04455911368151}],\n",
              " 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)},\n",
              "  {10: 13},\n",
              "  {10: 1.1172606511122094}],\n",
              " 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)},\n",
              "  {10: 14},\n",
              "  {10: 1.0865713649029336}],\n",
              " 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)},\n",
              "  {10: 14},\n",
              "  {10: 1.1551956639775147}],\n",
              " 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)},\n",
              "  {10: 9},\n",
              "  {10: 1.0881644651544142}],\n",
              " 21: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 7},\n",
              "  {10: 1.0714495073472743}],\n",
              " 22: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.121244221724319}],\n",
              " 23: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.051417670560449}],\n",
              " 24: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)},\n",
              "  {10: 3},\n",
              "  {10: 1.0758480133185298}]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "super_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_values = {}\n",
        "list=[]\n",
        "for key in super_dict:\n",
        "    parsed_values[key] = super_dict[key][2][10]\n",
        "\n",
        "# Print the parsed values\n",
        "for key, value in parsed_values.items():\n",
        "    list.append(value)\n",
        "    print(f'Key {key}: {value}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MALGGp9QCdrw",
        "outputId": "533b0f9b-e861-46cf-e521-25dfccbf4cb1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key 0: 0.9994927461794473\n",
            "Key 1: 1.0750217119714527\n",
            "Key 2: 1.0636585666466636\n",
            "Key 3: 1.0360127844193727\n",
            "Key 4: 1.0337662922969948\n",
            "Key 5: 1.1006713356321056\n",
            "Key 6: 1.1048031905668751\n",
            "Key 7: 1.120594644769965\n",
            "Key 8: 1.2089862779012048\n",
            "Key 9: 1.2488489052534233\n",
            "Key 10: 1.0769178501514183\n",
            "Key 11: 1.1540558331327075\n",
            "Key 12: 1.1823069403580344\n",
            "Key 13: 1.0995683257591817\n",
            "Key 14: 1.0441639365109077\n",
            "Key 15: 1.0932239105455974\n",
            "Key 16: 1.04455911368151\n",
            "Key 17: 1.1172606511122094\n",
            "Key 18: 1.0865713649029336\n",
            "Key 19: 1.1551956639775147\n",
            "Key 20: 1.0881644651544142\n",
            "Key 21: 1.0714495073472743\n",
            "Key 22: 1.121244221724319\n",
            "Key 23: 1.051417670560449\n",
            "Key 24: 1.0758480133185298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data for plotting\n",
        "k_values = range(0,25)\n",
        "DBL_values = list\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot Davies-Bouldin Score\n",
        "plt.plot(k_values, DBL_values, 'bx-')\n",
        "plt.xlabel('k values')\n",
        "plt.ylabel('Davies-Bouldin Score')\n",
        "plt.xticks(k_values)\n",
        "plt.show()\n",
        "plt.savefig('Time.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "pbKDbAy6DYiG",
        "outputId": "d2e27920-697e-4964-95ad-553aaf7453ba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}