{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rNAxRKvX1nn",
        "outputId": "51d5f5a4-0d8e-4969-f909-2db438128828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vt_WUACMYdK",
        "outputId": "d6ef31d3-33fb-47ed-9ee4-e8057bb60d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboard_logger\n",
            "  Downloading tensorboard_logger-0.1.0-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (1.11.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard_logger) (9.4.0)\n",
            "Installing collected packages: tensorboard_logger\n",
            "Successfully installed tensorboard_logger-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard_logger\n",
        "\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/config.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/edgeConstruction.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/data_params.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/make_data.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/pretraining.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/extract_feature.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/copyGraph.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/DCC.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/extractSDAE.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/extractconvSDAE.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/SDAE.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/__init__.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/DCCComputation.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/DCCLoss.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/custom_data.py\" .\n",
        "!cp \"/content/drive/MyDrive/DCC Codes/convSDAE.py\" ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qg9L4N5gZIlA"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "85riHxlGsQfT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "from sklearn import preprocessing\n",
        "from scipy.io import savemat\n",
        "import scipy.io\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from config import cfg, get_data_dir\n",
        "from easydict import EasyDict as edict\n",
        "from edgeConstruction import compressed_data\n",
        "from custom_data import DCCPT_data\n",
        "import data_params as dp\n",
        "import make_data\n",
        "import pretraining\n",
        "import extract_feature\n",
        "import copyGraph\n",
        "import DCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE1IAfhL-Wr3"
      },
      "outputs": [],
      "source": [
        "#some necessary initial functions\n",
        "\n",
        "def set_pretraining_hypers(args):\n",
        "  args.db = dp.oyster.name\n",
        "  args.niter = 500\n",
        "  args.step = 300\n",
        "  args.lr = 0.001\n",
        "\n",
        "# if we need to resume for faster debugging/results\n",
        "  args.resume = False\n",
        "  args.level = None\n",
        "\n",
        "  args.batchsize = 300\n",
        "  args.ngpu = 0\n",
        "  args.deviceID = 0\n",
        "  args.tensorboard = True\n",
        "  args.h5 = False\n",
        "  args.id = 2\n",
        "  args.dim = 10\n",
        "  args.manualSeed = cfg.RNG_SEED\n",
        "  args.clean_log = True\n",
        "\n",
        "def cluster_count(clusters):\n",
        "  cluster_df = pd.DataFrame(clusters.T, columns= ['labels'])\n",
        "  cluster_df_count = pd.DataFrame(np.array(cluster_df['labels'].value_counts()), columns= ['labels'])\n",
        "  df_filtered = cluster_df_count[cluster_df_count['labels'] >= 100]\n",
        "  return len(df_filtered)\n",
        "\n",
        "def set_training_hypers(args):\n",
        "  args.batchsize = cfg.PAIRS_PER_BATCH\n",
        "  args.nepoch = 150\n",
        "  args.M = 10\n",
        "  args.lr = 0.001\n",
        "\n",
        "#function for finding the most frequent element in a list\n",
        "def most_frequent(List):\n",
        "  return max(set(List), key = List.count)\n",
        "\n",
        "#function for deleting certain key/value pairs in a dict and returning the poped keys\n",
        "def delete_dict_items(dict, val):\n",
        "  poped_keys = []\n",
        "  for key, value in dict.copy().items():\n",
        "    if value != val:\n",
        "      poped_keys.append(key)\n",
        "      dict.pop(key)\n",
        "\n",
        "  return dict, poped_keys\n",
        "\n",
        "#function for finding the minimum value and its corresponding key in a dict\n",
        "def min_value(dict):\n",
        "  min_value = min(dict.values())\n",
        "  for key in dict:\n",
        "    if dict[key] == min_value:\n",
        "      min_value_key = key\n",
        "  return min_value, min_value_key\n",
        "\n",
        "\n",
        "def cluster_metrics(labels, features):\n",
        "\n",
        "    numeval = len(labels)\n",
        "    dbl = metrics.davies_bouldin_score(features, labels[:numeval])\n",
        "    sil = metrics.silhouette_score(features, labels[:numeval], metric='euclidean')\n",
        "    ch = metrics.calinski_harabasz_score(features, labels[:numeval])\n",
        "\n",
        "    return dbl,sil,ch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def change_time(start_number):\n",
        "  mat_train = scipy.io.loadmat('/content/drive/MyDrive/DCC-master/data/oyster/Old_traindata.mat')\n",
        "  mat_test = scipy.io.loadmat('/content/drive/MyDrive/DCC-master/data/oyster/Old_testdata.mat')\n",
        "  D=start_number\n",
        "  v=np.array([0,D*60,(24-D)*60,24*60]).reshape(-1, 1)\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  v_scaled = min_max_scaler.fit_transform(v)\n",
        "  #changing EntTime\n",
        "  for i in range(mat_train['X'].shape[0]):\n",
        "      New_time=mat_train['X'][i][0]-v_scaled[1]\n",
        "      if New_time<0:\n",
        "          New_time=mat_train['X'][i][0]+v_scaled[2]\n",
        "      mat_train['X'][i][0]=New_time\n",
        "  #changing EXTime\n",
        "  for i in range(mat_train['X'].shape[0]):\n",
        "      New_time=mat_train['X'][i][1]-v_scaled[1]\n",
        "      if New_time<0:\n",
        "          New_time=mat_train['X'][i][1]+v_scaled[2]\n",
        "      mat_train['X'][i][1]=New_time\n",
        "    #changing EntTime\n",
        "  for i in range(mat_test['X'].shape[0]):\n",
        "      New_time=mat_test['X'][i][0]-v_scaled[1]\n",
        "      if New_time<0:\n",
        "          New_time=mat_test['X'][i][0]+v_scaled[2]\n",
        "      mat_test['X'][i][0]=New_time\n",
        "  #changing EXTime\n",
        "  for i in range(mat_test['X'].shape[0]):\n",
        "      New_time=mat_test['X'][i][1]-v_scaled[1]\n",
        "      if New_time<0:\n",
        "          New_time=mat_test['X'][i][1]+v_scaled[2]\n",
        "      mat_test['X'][i][1]=New_time\n",
        "  savemat(\"/content/drive/MyDrive/DCC-master/data/oyster/traindata.mat\", mat_train)\n",
        "  savemat(\"/content/drive/MyDrive/DCC-master/data/oyster/testdata.mat\", mat_test)"
      ],
      "metadata": {
        "id": "D9HHwm3oyQeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuU6tLOYs4dW"
      },
      "outputs": [],
      "source": [
        "#oyster dataset root directory\n",
        "datadir = get_data_dir(dp.oyster.name)\n",
        "\n",
        "#shuffling the oyster dataset and making train/test sets\n",
        "#X, Y = make_data.make_misc_data(datadir,'OysterMetro_Complete_Small_Sample_Feature_Extracted.csv',dim= 10)\n",
        "\n",
        "#oyster dataset shape (whole data)\n",
        "#N = X.shape[0]\n",
        "N = 12617\n",
        "#defining a super dict for storing clusters, cluster counts, and dbl for each eligible k\n",
        "super_dict = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnFJ2NIukcQY",
        "outputId": "454a18df-d28d-44f4-ddcb-17c1fa26712b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " Epoch: 2\n",
            "total_loss:  0.013394840724775172 epoch:  2\n",
            "reconstruction_loss:  0.013392176685049668 epoch:  2\n",
            "dcc_loss:  2.664013650035757e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.013399236206683157 epoch:  3\n",
            "reconstruction_loss:  0.013397390630176349 epoch:  3\n",
            "dcc_loss:  1.845577011678025e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.013402979850220828 epoch:  4\n",
            "reconstruction_loss:  0.013401344097148668 epoch:  4\n",
            "dcc_loss:  1.635748970778345e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.01119306474878607 epoch:  5\n",
            "reconstruction_loss:  0.011151912773386811 epoch:  5\n",
            "dcc_loss:  4.1151963756960074e-05 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.00833785122171764 epoch:  6\n",
            "reconstruction_loss:  0.008285128415358148 epoch:  6\n",
            "dcc_loss:  5.2722816259624665e-05 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.008269634499325117 epoch:  7\n",
            "reconstruction_loss:  0.00826067763994438 epoch:  7\n",
            "dcc_loss:  8.956863491910692e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.008243792273497063 epoch:  8\n",
            "reconstruction_loss:  0.008240982886269704 epoch:  8\n",
            "dcc_loss:  2.809410980797635e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.00822510694287519 epoch:  9\n",
            "reconstruction_loss:  0.008224262588671497 epoch:  9\n",
            "dcc_loss:  8.443761660997475e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.008198592922897742 epoch:  10\n",
            "reconstruction_loss:  0.008198240093540953 epoch:  10\n",
            "dcc_loss:  3.52842646418521e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.1315396705472434\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.008147941455467654 epoch:  11\n",
            "reconstruction_loss:  0.008147661163215573 epoch:  11\n",
            "dcc_loss:  2.802847437057442e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.055707979293442\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.007852128530067415 epoch:  12\n",
            "reconstruction_loss:  0.00784955841892179 epoch:  12\n",
            "dcc_loss:  2.570082170084167e-06 epoch:  12\n",
            "epoch:  12 DBL:  1.0441639365109077\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 6.929011344909668\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.2395004005016615 0\n",
            "val_loss_0 0.21215330106803998 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.1952040120125124 1\n",
            "val_loss_0 0.17147878589324075 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.15908185616512074 2\n",
            "val_loss_0 0.14092890842686365 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.1320342918645517 3\n",
            "val_loss_0 0.11815248480547816 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.11183150024045793 4\n",
            "val_loss_0 0.10118157418943623 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09672107942307136 5\n",
            "val_loss_0 0.08857968167391518 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.08547586188817254 6\n",
            "val_loss_0 0.07927071796685882 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07719449432363575 7\n",
            "val_loss_0 0.07241764047113727 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.07341611006947796 8\n",
            "val_loss_0 0.07186126906170898 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.07285165212355295 9\n",
            "val_loss_0 0.07132419669873741 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.072317736894911 10\n",
            "val_loss_0 0.07080481887430472 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.07177702497396007 11\n",
            "val_loss_0 0.07030097068111794 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.07125752870269163 12\n",
            "val_loss_0 0.06981306395562817 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.07075911661781453 13\n",
            "val_loss_0 0.0693409409309057 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.07028275345029791 14\n",
            "val_loss_0 0.06888096337981534 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.00039213122480147435 0\n",
            "val_loss_1 0.00038733941407117063 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0003914378005270517 1\n",
            "val_loss_1 0.00038649834611854576 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0003906167927734455 2\n",
            "val_loss_1 0.0003856052093376832 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0003897580535890964 3\n",
            "val_loss_1 0.0003846722867442428 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0003888272970959501 4\n",
            "val_loss_1 0.00038370251662589363 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.00038786641081690735 5\n",
            "val_loss_1 0.0003827016793908005 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.00038688654160842355 6\n",
            "val_loss_1 0.0003816704074361416 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0003858349172015141 7\n",
            "val_loss_1 0.00038060988442530997 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.00038523897110473654 8\n",
            "val_loss_1 0.00038050169626283335 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0003851223258185304 9\n",
            "val_loss_1 0.00038039304705140387 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.00038500849796362373 10\n",
            "val_loss_1 0.0003802838924664874 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.0003849139917677588 11\n",
            "val_loss_1 0.000380174341866534 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0003848141727891863 12\n",
            "val_loss_1 0.0003800643173492146 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0003846948271300378 13\n",
            "val_loss_1 0.00037995380872127006 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0003845891076605444 14\n",
            "val_loss_1 0.000379842750625921 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 9.879326115128581e-06 0\n",
            "val_loss_2 9.824614054598128e-06 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 9.862271485396706e-06 1\n",
            "val_loss_2 9.804517878405363e-06 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 9.844280406740854e-06 2\n",
            "val_loss_2 9.783410305187652e-06 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 9.823984630303461e-06 3\n",
            "val_loss_2 9.761415065141013e-06 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 9.8041887036116e-06 4\n",
            "val_loss_2 9.738598118972316e-06 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 9.782304524008832e-06 5\n",
            "val_loss_2 9.71501736589261e-06 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 9.761075554824128e-06 6\n",
            "val_loss_2 9.690723008279804e-06 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 9.738160796163928e-06 7\n",
            "val_loss_2 9.665757055852725e-06 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 9.72515157138155e-06 8\n",
            "val_loss_2 9.663219753010867e-06 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 9.72243436427162e-06 9\n",
            "val_loss_2 9.66067446218226e-06 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 9.720054939691224e-06 10\n",
            "val_loss_2 9.658121008963012e-06 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 9.717499394115613e-06 11\n",
            "val_loss_2 9.655562100216905e-06 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 9.715623131056167e-06 12\n",
            "val_loss_2 9.652995558057264e-06 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 9.712724369051335e-06 13\n",
            "val_loss_2 9.650424449686508e-06 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 9.710676173556339e-06 14\n",
            "val_loss_2 9.647849337232893e-06 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 2.4961488739686296e-07 0\n",
            "val_loss_3 2.4663629280530444e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 2.495030840148278e-07 1\n",
            "val_loss_3 2.464929590617685e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 2.4934873186664864e-07 2\n",
            "val_loss_3 2.4632659296847744e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 2.491805242322735e-07 3\n",
            "val_loss_3 2.4614533455255864e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 2.4899666484164416e-07 4\n",
            "val_loss_3 2.459535138006366e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 2.4880381145414684e-07 5\n",
            "val_loss_3 2.4575276651496397e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 2.4860362599777184e-07 6\n",
            "val_loss_3 2.455425248469024e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 2.4838674189303213e-07 7\n",
            "val_loss_3 2.453226223875684e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 2.4826289756495255e-07 8\n",
            "val_loss_3 2.4530008602141606e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 2.482395156894576e-07 9\n",
            "val_loss_3 2.4527744141854774e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 2.4821764902927476e-07 10\n",
            "val_loss_3 2.452547174285751e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 2.4819375291317066e-07 11\n",
            "val_loss_3 2.452319350412232e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 2.481753310582246e-07 12\n",
            "val_loss_3 2.4520906774908165e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 2.481497160468081e-07 13\n",
            "val_loss_3 2.4518613683465075e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 2.4812543564767845e-07 14\n",
            "val_loss_3 2.4516313824412094e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07726774707748078 0\n",
            "val_loss_4 0.07337407066402647 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.07249260269918185 1\n",
            "val_loss_4 0.06909389973347992 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.06860157700799377 2\n",
            "val_loss_4 0.06591023185339102 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.06570817063764929 3\n",
            "val_loss_4 0.06356464243138271 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.06356302149653624 4\n",
            "val_loss_4 0.06184140605211447 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.06197503161574454 5\n",
            "val_loss_4 0.06057512442538553 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.06079536347320481 6\n",
            "val_loss_4 0.0596555757990163 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.05992483225889429 7\n",
            "val_loss_4 0.0589797885264912 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.05952439876102567 8\n",
            "val_loss_4 0.05892465820332337 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.05946433003875577 9\n",
            "val_loss_4 0.058871972337719375 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.05940629922818361 10\n",
            "val_loss_4 0.058820528192253765 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.05934978974895436 11\n",
            "val_loss_4 0.058770710971389445 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.05929499635406272 12\n",
            "val_loss_4 0.05872231207067728 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.05924142169492351 13\n",
            "val_loss_4 0.05867569523431042 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.059189775974712956 14\n",
            "val_loss_4 0.05863000547569452 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.05913924939474974 15\n",
            "val_loss_4 0.05858592993082411 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.059090445054176245 16\n",
            "val_loss_4 0.058543154222982244 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.05906290695385691 17\n",
            "val_loss_4 0.058538976118039404 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.05905820559136319 18\n",
            "val_loss_4 0.058534793266450925 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.059053495117354936 19\n",
            "val_loss_4 0.05853065758612946 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.05904881225175986 20\n",
            "val_loss_4 0.05852651867643589 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.05904414547249464 21\n",
            "val_loss_4 0.05852238794348886 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.05903949722851408 22\n",
            "val_loss_4 0.058518226900493665 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.05903484197982142 23\n",
            "val_loss_4 0.058514115567394366 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.059030194167314594 24\n",
            "val_loss_4 0.05851002822138617 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.05902559248890913 25\n",
            "val_loss_4 0.05850592640519331 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.0590229151471689 26\n",
            "val_loss_4 0.05850551798703554 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.05902245260437735 27\n",
            "val_loss_4 0.058505107815452685 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.059021989707622326 28\n",
            "val_loss_4 0.05850469504029929 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.05902152473765264 29\n",
            "val_loss_4 0.058504284248818685 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.900365, 0.893912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.122047, 0.092983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.909399, 0.895239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.249218, 0.22636 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.643937, 0.6464  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.729413, 0.731337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), array([[2.4692301e-04, 5.5920903e-04],\n",
            "       [6.9453323e-05, 4.5285380e-04],\n",
            "       [3.2516086e-04, 5.0022255e-04],\n",
            "       ...,\n",
            "       [1.1822632e-04, 4.4576926e-04],\n",
            "       [3.0331887e-04, 4.5303922e-04],\n",
            "       [1.6064415e-04, 4.9796508e-04]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.900365, 0.893912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.122047, 0.092983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.909399, 0.895239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.249218, 0.22636 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.643937, 0.6464  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.729413, 0.731337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), 'Z': array([[2.4692301e-04, 5.5920903e-04],\n",
            "       [6.9453323e-05, 4.5285380e-04],\n",
            "       [3.2516086e-04, 5.0022255e-04],\n",
            "       ...,\n",
            "       [1.1822632e-04, 4.4576926e-04],\n",
            "       [3.0331887e-04, 4.5303922e-04],\n",
            "       [1.6064415e-04, 4.9796508e-04]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.013569713856535989 epoch:  0\n",
            "reconstruction_loss:  0.013565284172782762 epoch:  0\n",
            "dcc_loss:  4.429666053396726e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.085368282551483\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.013570653229740395 epoch:  1\n",
            "reconstruction_loss:  0.013566603842394179 epoch:  1\n",
            "dcc_loss:  4.049383239026811e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.013579415860225185 epoch:  2\n",
            "reconstruction_loss:  0.013576681352942453 epoch:  2\n",
            "dcc_loss:  2.734494783005623e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.013574190802001403 epoch:  3\n",
            "reconstruction_loss:  0.013572248596064842 epoch:  3\n",
            "dcc_loss:  1.9422500721406136e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.013567398124953606 epoch:  4\n",
            "reconstruction_loss:  0.013565861214721489 epoch:  4\n",
            "dcc_loss:  1.5369012770728136e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.013556424807687442 epoch:  5\n",
            "reconstruction_loss:  0.013555152967286455 epoch:  5\n",
            "dcc_loss:  1.271842458955775e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.013561464128860671 epoch:  6\n",
            "reconstruction_loss:  0.013560360675162057 epoch:  6\n",
            "dcc_loss:  1.1034573837125652e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.013566820399424482 epoch:  7\n",
            "reconstruction_loss:  0.013565835518042548 epoch:  7\n",
            "dcc_loss:  9.848732818635448e-07 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.013563472239712136 epoch:  8\n",
            "reconstruction_loss:  0.013562585290391949 epoch:  8\n",
            "dcc_loss:  8.869740069048533e-07 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.013561394306053161 epoch:  9\n",
            "reconstruction_loss:  0.013560564514616368 epoch:  9\n",
            "dcc_loss:  8.297708275810226e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.01356880193725464 epoch:  10\n",
            "reconstruction_loss:  0.013568015320858664 epoch:  10\n",
            "dcc_loss:  7.86617395882773e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.1242829283205225\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.01356603349669914 epoch:  11\n",
            "reconstruction_loss:  0.013565268297934316 epoch:  11\n",
            "dcc_loss:  7.651812682197622e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.0852693079832059\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.013555356928178474 epoch:  12\n",
            "reconstruction_loss:  0.013554576514984814 epoch:  12\n",
            "dcc_loss:  7.804356576099298e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.0932239105455974\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 8.141754388809204\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.243069279126555 0\n",
            "val_loss_0 0.21640476669449057 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.19796748383895832 1\n",
            "val_loss_0 0.17490016580478135 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16120513949724463 2\n",
            "val_loss_0 0.14369124462222901 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.13366570184630136 3\n",
            "val_loss_0 0.12041624268242372 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.11309711616879399 4\n",
            "val_loss_0 0.10305447066453671 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.0977037329214552 5\n",
            "val_loss_0 0.09016912188469514 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.08626347667431791 6\n",
            "val_loss_0 0.08063341795264635 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07783069696919977 7\n",
            "val_loss_0 0.07361185447963785 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.07398868757959377 8\n",
            "val_loss_0 0.07304166850121765 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.07341525610287197 9\n",
            "val_loss_0 0.07249057079221102 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.07286831865969845 10\n",
            "val_loss_0 0.07195881100020204 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.07231871566040292 11\n",
            "val_loss_0 0.0714421113879197 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.07179338261625356 12\n",
            "val_loss_0 0.07094225370090472 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.07128945928530923 13\n",
            "val_loss_0 0.07045797997769765 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.07079976791272158 14\n",
            "val_loss_0 0.06998650930385016 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0004064580212466889 0\n",
            "val_loss_1 0.00040288259026745007 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.000405738643276057 1\n",
            "val_loss_1 0.00040200743983830316 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.00040488691356169745 2\n",
            "val_loss_1 0.000401077491139754 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0004039922455472623 3\n",
            "val_loss_1 0.00040010500546336684 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0004030254950958109 4\n",
            "val_loss_1 0.0003990941117709523 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0004020276997818986 5\n",
            "val_loss_1 0.00039805102532156837 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0004010090927003225 6\n",
            "val_loss_1 0.00039697586552855425 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.00039991365776079495 7\n",
            "val_loss_1 0.0003958711361976727 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.00039929471638241066 8\n",
            "val_loss_1 0.00039575841632434025 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.00039917552281406107 9\n",
            "val_loss_1 0.0003956451954131152 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.00039905651823173585 10\n",
            "val_loss_1 0.0003955314149334732 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.000398956357605394 11\n",
            "val_loss_1 0.00039541725144557737 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.00039885078061289086 12\n",
            "val_loss_1 0.000395302513722041 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.00039872935354975096 13\n",
            "val_loss_1 0.0003951873676685348 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0003986182004444203 14\n",
            "val_loss_1 0.0003950715654643279 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.0216246156231784e-05 0\n",
            "val_loss_2 1.018898637273748e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.019857517348379e-05 1\n",
            "val_loss_2 1.0168214808085346e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.0180098390775293e-05 2\n",
            "val_loss_2 1.01463831574526e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.0159125627680849e-05 3\n",
            "val_loss_2 1.0123617458643842e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.0138649439231873e-05 4\n",
            "val_loss_2 1.0099997582878414e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.0116066483666904e-05 5\n",
            "val_loss_2 1.0075558527685401e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.0093988366957113e-05 6\n",
            "val_loss_2 1.005038031562637e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.0070318276146046e-05 7\n",
            "val_loss_2 1.0024524313811339e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.0056974934116705e-05 8\n",
            "val_loss_2 1.0021894869499115e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.0053948756436761e-05 9\n",
            "val_loss_2 1.0019258206883458e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.00516080654452e-05 10\n",
            "val_loss_2 1.0016612057272354e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.0049029633457732e-05 11\n",
            "val_loss_2 1.0013960576090884e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.0047094198184584e-05 12\n",
            "val_loss_2 1.001130137645596e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.0044073468551818e-05 13\n",
            "val_loss_2 1.0008635202106517e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.0042024965506499e-05 14\n",
            "val_loss_2 1.0005966330982768e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 2.5809113570492506e-07 0\n",
            "val_loss_3 2.5569630440731827e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 2.579741904180438e-07 1\n",
            "val_loss_3 2.5554731951809257e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 2.578149217506104e-07 2\n",
            "val_loss_3 2.553747183954043e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 2.576395523551516e-07 3\n",
            "val_loss_3 2.5518697489760037e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 2.5745004024893346e-07 4\n",
            "val_loss_3 2.549887250964501e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 2.5725093376433945e-07 5\n",
            "val_loss_3 2.5478164848526513e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 2.5704454861597227e-07 6\n",
            "val_loss_3 2.5456548102591434e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 2.568225678169824e-07 7\n",
            "val_loss_3 2.543397581518192e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 2.5669519879987996e-07 8\n",
            "val_loss_3 2.5431659252181326e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 2.566710843606676e-07 9\n",
            "val_loss_3 2.5429330987183727e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 2.56648471496149e-07 10\n",
            "val_loss_3 2.542699471366008e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 2.56624423846405e-07 11\n",
            "val_loss_3 2.5424649846060116e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 2.5660532442106e-07 12\n",
            "val_loss_3 2.5422297978882264e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 2.5657871028504085e-07 13\n",
            "val_loss_3 2.541993826983498e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 2.5655382935234027e-07 14\n",
            "val_loss_3 2.541756985410555e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07810173179028966 0\n",
            "val_loss_4 0.0747979867689772 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.07322285148349919 1\n",
            "val_loss_4 0.07039376031814022 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.06925328649830725 2\n",
            "val_loss_4 0.06710937748502249 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.066295556714475 3\n",
            "val_loss_4 0.06469221795110619 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.06410798464325862 4\n",
            "val_loss_4 0.06290737156964331 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.06248554227347536 5\n",
            "val_loss_4 0.0615987138034519 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.06128262273541329 6\n",
            "val_loss_4 0.0606437680590455 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.06039475917817638 7\n",
            "val_loss_4 0.05993949508563085 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.05998775409142409 8\n",
            "val_loss_4 0.05988181067155394 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.05992643818713966 9\n",
            "val_loss_4 0.05982699608297431 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.059867299529563725 10\n",
            "val_loss_4 0.059773435087050956 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.05980978308661327 11\n",
            "val_loss_4 0.05972126742054468 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.059753850827330014 12\n",
            "val_loss_4 0.05967062538734707 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.05969915998366771 13\n",
            "val_loss_4 0.05962203198633557 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.0596466782677874 14\n",
            "val_loss_4 0.05957414698085959 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.05959513778730461 15\n",
            "val_loss_4 0.05952805782507414 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.0595453420256607 16\n",
            "val_loss_4 0.0594833881583909 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.059517431376011204 17\n",
            "val_loss_4 0.059479012801983464 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.059512640121188516 18\n",
            "val_loss_4 0.05947464822589312 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.059507842951595495 19\n",
            "val_loss_4 0.059470317578873055 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.059503071138927416 20\n",
            "val_loss_4 0.05946597216647892 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.05949830183980605 21\n",
            "val_loss_4 0.059461654643579824 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.05949357047752336 22\n",
            "val_loss_4 0.059457294265082825 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.05948881364832498 23\n",
            "val_loss_4 0.05945300296681052 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.05948408407800547 24\n",
            "val_loss_4 0.05944873122188444 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.059479403650661435 25\n",
            "val_loss_4 0.05944442457376305 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.059476667800492836 26\n",
            "val_loss_4 0.05944399967813265 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.05947619762257248 27\n",
            "val_loss_4 0.05944356946318917 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.05947572499089904 28\n",
            "val_loss_4 0.05944313897076764 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.05947525061450675 29\n",
            "val_loss_4 0.05944270924583857 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.8586983 , 0.85224533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.08038034, 0.05131633, 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.86773235, 0.8535723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.20755133, 0.18469334, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.6022703 , 0.60473335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.68774635, 0.6896703 , 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[2.5719524e-04, 5.6475867e-04],\n",
            "       [8.3477396e-05, 4.5538388e-04],\n",
            "       [3.3335402e-04, 5.0656416e-04],\n",
            "       ...,\n",
            "       [1.3333763e-04, 4.5116301e-04],\n",
            "       [3.1340591e-04, 4.5777581e-04],\n",
            "       [1.7059209e-04, 5.0573354e-04]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.8586983 , 0.85224533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.08038034, 0.05131633, 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.86773235, 0.8535723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.20755133, 0.18469334, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.6022703 , 0.60473335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.68774635, 0.6896703 , 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[2.5719524e-04, 5.6475867e-04],\n",
            "       [8.3477396e-05, 4.5538388e-04],\n",
            "       [3.3335402e-04, 5.0656416e-04],\n",
            "       ...,\n",
            "       [1.3333763e-04, 4.5116301e-04],\n",
            "       [3.1340591e-04, 4.5777581e-04],\n",
            "       [1.7059209e-04, 5.0573354e-04]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.013663177598082927 epoch:  0\n",
            "reconstruction_loss:  0.013658320096729216 epoch:  0\n",
            "dcc_loss:  4.857508111965507e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.1292286136975538\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.013667463093186463 epoch:  1\n",
            "reconstruction_loss:  0.013663273036248938 epoch:  1\n",
            "dcc_loss:  4.1900647002153016e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.013659858116196584 epoch:  2\n",
            "reconstruction_loss:  0.01365710644465346 epoch:  2\n",
            "dcc_loss:  2.7516911376033917e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.013662507966255321 epoch:  3\n",
            "reconstruction_loss:  0.013660669120090474 epoch:  3\n",
            "dcc_loss:  1.8388576325825998e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.01365595144728162 epoch:  4\n",
            "reconstruction_loss:  0.013654537335909501 epoch:  4\n",
            "dcc_loss:  1.4141290872873164e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.01366173446238516 epoch:  5\n",
            "reconstruction_loss:  0.013660545553053082 epoch:  5\n",
            "dcc_loss:  1.1888747342885765e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.01364746358794788 epoch:  6\n",
            "reconstruction_loss:  0.013646361027645047 epoch:  6\n",
            "dcc_loss:  1.1025678748784636e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.013663326444138119 epoch:  7\n",
            "reconstruction_loss:  0.013662260736073102 epoch:  7\n",
            "dcc_loss:  1.0657196077636708e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.013659856722314347 epoch:  8\n",
            "reconstruction_loss:  0.013658845271244485 epoch:  8\n",
            "dcc_loss:  1.0114026289603498e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.013658674136141049 epoch:  9\n",
            "reconstruction_loss:  0.013657702350077515 epoch:  9\n",
            "dcc_loss:  9.718062334128007e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.013650643286138131 epoch:  10\n",
            "reconstruction_loss:  0.013649772104456991 epoch:  10\n",
            "dcc_loss:  8.711546028273854e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.1667840917903856\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.013646937044756124 epoch:  11\n",
            "reconstruction_loss:  0.013646115626871087 epoch:  11\n",
            "dcc_loss:  8.214341758687959e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.1004571576264224\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.013659185179568573 epoch:  12\n",
            "reconstruction_loss:  0.01365844587546724 epoch:  12\n",
            "dcc_loss:  7.393065080164183e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.04455911368151\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.659404039382935\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.25037315954399525 0\n",
            "val_loss_0 0.2244157700927813 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.20332133621612247 1\n",
            "val_loss_0 0.18089264874507432 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16496407926779466 2\n",
            "val_loss_0 0.1481459146681376 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.1362409707541557 3\n",
            "val_loss_0 0.12366951682063176 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.11477281381303653 4\n",
            "val_loss_0 0.10538878078128192 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09871449768410369 5\n",
            "val_loss_0 0.09180534267340523 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.0867887910790974 6\n",
            "val_loss_0 0.08174228900822142 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07799898417729116 7\n",
            "val_loss_0 0.07436223979891764 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.07401245018364144 8\n",
            "val_loss_0 0.07375976084340394 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.07341966565957121 9\n",
            "val_loss_0 0.0731783546330245 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.07284961951909281 10\n",
            "val_loss_0 0.07261735139482561 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.07228577671592681 11\n",
            "val_loss_0 0.07207115301522891 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.07173782271473941 12\n",
            "val_loss_0 0.0715449390056205 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.07121576067626056 13\n",
            "val_loss_0 0.07103304145783894 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.07070144926794195 14\n",
            "val_loss_0 0.07053614516407487 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.00044052259771027005 0\n",
            "val_loss_1 0.00043966964341252867 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0004397352617043907 1\n",
            "val_loss_1 0.00043870961994099504 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0004388054071733512 2\n",
            "val_loss_1 0.0004376884627896885 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0004378274332375646 3\n",
            "val_loss_1 0.0004366197539921466 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.00043677279394429837 4\n",
            "val_loss_1 0.00043550821727070894 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.00043567836386115903 5\n",
            "val_loss_1 0.00043436060074743374 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.00043456718983944556 6\n",
            "val_loss_1 0.00043317804606734714 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.00043336301999330674 7\n",
            "val_loss_1 0.000431962732881346 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.00043269004892954824 8\n",
            "val_loss_1 0.00043183863640832234 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0004325571143825528 9\n",
            "val_loss_1 0.00043171404175705347 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.00043242457924980214 10\n",
            "val_loss_1 0.0004315887753653935 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.00043231437767507583 11\n",
            "val_loss_1 0.00043146298538152537 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.000432202395576273 12\n",
            "val_loss_1 0.0004313365397081901 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0004320689305915663 13\n",
            "val_loss_1 0.00043120971720712924 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.00043194732212906384 14\n",
            "val_loss_1 0.0004310820530933991 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.1014119442200678e-05 0\n",
            "val_loss_2 1.1048038477571047e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.0994798278231165e-05 1\n",
            "val_loss_2 1.1025529498415749e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.0974850261641486e-05 2\n",
            "val_loss_2 1.100188280480813e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.095221848225988e-05 3\n",
            "val_loss_2 1.097716177454777e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.0930123153635444e-05 4\n",
            "val_loss_2 1.0951480885776016e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.0905684232160944e-05 5\n",
            "val_loss_2 1.0924921985809258e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.0881882787805106e-05 6\n",
            "val_loss_2 1.0897574245486195e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.0856121910780395e-05 7\n",
            "val_loss_2 1.0869470615612499e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.0841659562983145e-05 8\n",
            "val_loss_2 1.0866610054427145e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.0838373299645378e-05 9\n",
            "val_loss_2 1.086374046459708e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.083587851260515e-05 10\n",
            "val_loss_2 1.0860861386330216e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.0833187531436814e-05 11\n",
            "val_loss_2 1.0857973874698056e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.0830799804890018e-05 12\n",
            "val_loss_2 1.0855079149086528e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.0827504697475466e-05 13\n",
            "val_loss_2 1.0852174666946258e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.0825544112912135e-05 14\n",
            "val_loss_2 1.0849267157961495e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 2.7833989827577867e-07 0\n",
            "val_loss_3 2.772684505095039e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 2.7821264856058273e-07 1\n",
            "val_loss_3 2.771057713109889e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 2.7803983571661344e-07 2\n",
            "val_loss_3 2.769182809060301e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 2.7784940644214175e-07 3\n",
            "val_loss_3 2.767147860157836e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 2.7764530487520316e-07 4\n",
            "val_loss_3 2.765007155921316e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 2.7743077913311815e-07 5\n",
            "val_loss_3 2.762776493596274e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 2.7720905333540613e-07 6\n",
            "val_loss_3 2.7604622768505866e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 2.769722553331266e-07 7\n",
            "val_loss_3 2.7580566831830216e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 2.768373133708359e-07 8\n",
            "val_loss_3 2.757810310653319e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 2.7681073575529136e-07 9\n",
            "val_loss_3 2.757562840442065e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 2.7678683222752187e-07 10\n",
            "val_loss_3 2.7573144324495265e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 2.767616617358085e-07 11\n",
            "val_loss_3 2.757064977673269e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 2.7674123137303357e-07 12\n",
            "val_loss_3 2.756814869332821e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 2.767130300254827e-07 13\n",
            "val_loss_3 2.7565636232231494e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 2.7668666630069024e-07 14\n",
            "val_loss_3 2.756311418162299e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07871883680618968 0\n",
            "val_loss_4 0.0760333329887851 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.07358662397065059 1\n",
            "val_loss_4 0.07132293398472851 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.0694118202335001 2\n",
            "val_loss_4 0.06780679371627121 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.06630314633000728 3\n",
            "val_loss_4 0.06521508099419757 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.06400607964936539 4\n",
            "val_loss_4 0.0632914094794571 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.06230403517609036 5\n",
            "val_loss_4 0.061874461479827454 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.06104082098371161 6\n",
            "val_loss_4 0.06084054536680601 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.06011130244579186 7\n",
            "val_loss_4 0.06007040480355453 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.059687623942936543 8\n",
            "val_loss_4 0.06000700590633175 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.059623448478801154 9\n",
            "val_loss_4 0.059946942767467815 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.05956157930080912 10\n",
            "val_loss_4 0.05988820943326088 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.05950144451175409 11\n",
            "val_loss_4 0.05983108881323727 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.05944292690506377 12\n",
            "val_loss_4 0.05977566504228134 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.059385937395253975 13\n",
            "val_loss_4 0.05972204687407391 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.05933092617588781 14\n",
            "val_loss_4 0.05966946995697573 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.05927704726908248 15\n",
            "val_loss_4 0.059618857855983844 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.05922502815504914 16\n",
            "val_loss_4 0.05956967366289601 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.059195953322469116 17\n",
            "val_loss_4 0.05956487047667549 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.05919096327747496 18\n",
            "val_loss_4 0.059560053516917295 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.05918594843621333 19\n",
            "val_loss_4 0.05955530520640727 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.05918096490872399 20\n",
            "val_loss_4 0.05955052733563017 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.059175981332144814 21\n",
            "val_loss_4 0.05954578661739165 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.059171057201498956 22\n",
            "val_loss_4 0.05954093876901406 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.059166062192157774 23\n",
            "val_loss_4 0.05953624577109485 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.059161129642053174 24\n",
            "val_loss_4 0.05953155884226982 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.059156238160201394 25\n",
            "val_loss_4 0.05952681198999545 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.0591533995783153 26\n",
            "val_loss_4 0.05952634259751406 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.059152908774548076 27\n",
            "val_loss_4 0.05952586615000587 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.05915241453596944 28\n",
            "val_loss_4 0.05952539731838435 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.05915192031436924 29\n",
            "val_loss_4 0.059524923633848995 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.8170317 , 0.81057864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.03871367, 0.00964967, 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.82606566, 0.8119057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.16588467, 0.14302666, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.5606037 , 0.56306666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.64607966, 0.64800364, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[0.00027383, 0.00058249],\n",
            "       [0.00010086, 0.0004673 ],\n",
            "       [0.00034666, 0.00052745],\n",
            "       ...,\n",
            "       [0.00015351, 0.0004694 ],\n",
            "       [0.00032757, 0.00047421],\n",
            "       [0.00018553, 0.00052687]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.8170317 , 0.81057864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.03871367, 0.00964967, 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.82606566, 0.8119057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.16588467, 0.14302666, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.5606037 , 0.56306666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.64607966, 0.64800364, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[0.00027383, 0.00058249],\n",
            "       [0.00010086, 0.0004673 ],\n",
            "       [0.00034666, 0.00052745],\n",
            "       ...,\n",
            "       [0.00015351, 0.0004694 ],\n",
            "       [0.00032757, 0.00047421],\n",
            "       [0.00018553, 0.00052687]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.],\n",
            "       [12577., 12604.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.013609228966119352 epoch:  0\n",
            "reconstruction_loss:  0.013604810280607595 epoch:  0\n",
            "dcc_loss:  4.418694782996865e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.1607528775285867\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.013608905296083232 epoch:  1\n",
            "reconstruction_loss:  0.013604910804966779 epoch:  1\n",
            "dcc_loss:  3.994483530224199e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.013607923076747911 epoch:  2\n",
            "reconstruction_loss:  0.013605183534757608 epoch:  2\n",
            "dcc_loss:  2.7395734073504453e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.013598885473832697 epoch:  3\n",
            "reconstruction_loss:  0.01359691190292015 epoch:  3\n",
            "dcc_loss:  1.9735870094607413e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.013611753688101974 epoch:  4\n",
            "reconstruction_loss:  0.013610214372054223 epoch:  4\n",
            "dcc_loss:  1.5393053109640633e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.013606584142457393 epoch:  5\n",
            "reconstruction_loss:  0.013605277865641243 epoch:  5\n",
            "dcc_loss:  1.3063093271461495e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.013599852689662333 epoch:  6\n",
            "reconstruction_loss:  0.013598718158543079 epoch:  6\n",
            "dcc_loss:  1.1345386062443016e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.013603025729905342 epoch:  7\n",
            "reconstruction_loss:  0.013602005222232396 epoch:  7\n",
            "dcc_loss:  1.020527625098226e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.013604624887956019 epoch:  8\n",
            "reconstruction_loss:  0.013603722330830046 epoch:  8\n",
            "dcc_loss:  9.025720233487312e-07 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.0136059423050022 epoch:  9\n",
            "reconstruction_loss:  0.013605075897070286 epoch:  9\n",
            "dcc_loss:  8.66418356208214e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.013608877951256967 epoch:  10\n",
            "reconstruction_loss:  0.013607981352997475 epoch:  10\n",
            "dcc_loss:  8.966080393172104e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.057849481962388\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.0135934306044033 epoch:  11\n",
            "reconstruction_loss:  0.013592489227948394 epoch:  11\n",
            "dcc_loss:  9.413568700130169e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.1080856190796353\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.013595166845492284 epoch:  12\n",
            "reconstruction_loss:  0.013594254263312658 epoch:  12\n",
            "dcc_loss:  9.12584591254495e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.1172606511122094\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.830789089202881\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.261702438794467 0\n",
            "val_loss_0 0.23544688815643594 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.21101677294919835 1\n",
            "val_loss_0 0.188287125029579 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16970787683599098 2\n",
            "val_loss_0 0.15276694342659317 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.13875638657030545 3\n",
            "val_loss_0 0.12621833758337184 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.11564497518714233 4\n",
            "val_loss_0 0.10630643664137497 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09833236909309512 5\n",
            "val_loss_0 0.09152669713444264 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.0855071831774714 6\n",
            "val_loss_0 0.08058735525173544 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07608215659886323 7\n",
            "val_loss_0 0.07256577780290187 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.07180609705234252 8\n",
            "val_loss_0 0.0719124774689153 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.07118129573358625 9\n",
            "val_loss_0 0.07128259727344649 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.07057470253537788 10\n",
            "val_loss_0 0.07067319746062797 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.06997361145695834 11\n",
            "val_loss_0 0.07008227282063519 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.06938540312038663 12\n",
            "val_loss_0 0.06951077311947682 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.0688277602625368 13\n",
            "val_loss_0 0.06895731495790738 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.06827446747411411 14\n",
            "val_loss_0 0.06841989586546002 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005053703124591276 0\n",
            "val_loss_1 0.0005073252899395877 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.000504450445761189 1\n",
            "val_loss_1 0.0005061962655981598 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0005033581917322076 2\n",
            "val_loss_1 0.0005049946196355138 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0005022117480260826 3\n",
            "val_loss_1 0.0005037372200967517 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0005009800782868724 4\n",
            "val_loss_1 0.0005024297359560375 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0004996970242975332 5\n",
            "val_loss_1 0.0005010797232780576 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0004983958551879054 6\n",
            "val_loss_1 0.0004996894491635699 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0004969855256294312 7\n",
            "val_loss_1 0.000498258261197865 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0004961972712914415 8\n",
            "val_loss_1 0.0004981119513069611 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0004960422105407609 9\n",
            "val_loss_1 0.0004979650093878659 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0004958852965280544 10\n",
            "val_loss_1 0.0004978171456014817 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.0004957590070000402 11\n",
            "val_loss_1 0.0004976687925386578 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0004956215284405359 12\n",
            "val_loss_1 0.0004975195360578826 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0004954669471345875 13\n",
            "val_loss_1 0.0004973698353170522 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0004953271683545784 14\n",
            "val_loss_1 0.0004972191929681413 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.256402604713575e-05 0\n",
            "val_loss_2 1.2666773187179597e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.25417227925053e-05 1\n",
            "val_loss_2 1.2640788149542567e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.251868997615228e-05 2\n",
            "val_loss_2 1.261341368218265e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.2492742141962098e-05 3\n",
            "val_loss_2 1.2584740851319741e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.2467193660019964e-05 4\n",
            "val_loss_2 1.2554953444068612e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.2439013616749413e-05 5\n",
            "val_loss_2 1.2524160304315802e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.2411355860701086e-05 6\n",
            "val_loss_2 1.2492448485550221e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.2381587578177859e-05 7\n",
            "val_loss_2 1.2459767818545112e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.236489967918353e-05 8\n",
            "val_loss_2 1.2456437251829928e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.2361024098422472e-05 9\n",
            "val_loss_2 1.2453095596774426e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.2358102041440545e-05 10\n",
            "val_loss_2 1.2449741412024087e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.235514125005878e-05 11\n",
            "val_loss_2 1.2446376660703767e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.23521229610942e-05 12\n",
            "val_loss_2 1.2443006250625601e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.2348459413708425e-05 13\n",
            "val_loss_2 1.2439622982223768e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.234617339107099e-05 14\n",
            "val_loss_2 1.2436233859039878e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.1767834581020724e-07 0\n",
            "val_loss_3 3.180490730495827e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.175289314053409e-07 1\n",
            "val_loss_3 3.1785990180004255e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.17330835751388e-07 2\n",
            "val_loss_3 3.176441837280055e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.171120865370865e-07 3\n",
            "val_loss_3 3.174103098589759e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.1687727357142327e-07 4\n",
            "val_loss_3 3.171652649068702e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.1663137000024076e-07 5\n",
            "val_loss_3 3.1691083718726624e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.163805716407395e-07 6\n",
            "val_loss_3 3.166476810751157e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.161127903087719e-07 7\n",
            "val_loss_3 3.1637625640755276e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.159613520067334e-07 8\n",
            "val_loss_3 3.163486141105787e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.159315113801379e-07 9\n",
            "val_loss_3 3.1632087132416935e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.159049602151151e-07 10\n",
            "val_loss_3 3.162930251205731e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.158766235117426e-07 11\n",
            "val_loss_3 3.1626510423679594e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.15853546691611e-07 12\n",
            "val_loss_3 3.1623709597090096e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.158218098557451e-07 13\n",
            "val_loss_3 3.1620900081835386e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.1579160480724717e-07 14\n",
            "val_loss_3 3.161807978344718e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07763018610263549 0\n",
            "val_loss_4 0.07511225576540559 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.07202364229951022 1\n",
            "val_loss_4 0.06987217578759473 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.06746100546196243 2\n",
            "val_loss_4 0.06597052753656489 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.06407808995854863 3\n",
            "val_loss_4 0.06307582636312146 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.061572469914855966 4\n",
            "val_loss_4 0.06093294961634793 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.059725369342733724 5\n",
            "val_loss_4 0.05933916478428145 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.05835198339255338 6\n",
            "val_loss_4 0.058175068519856016 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.05734443937977419 7\n",
            "val_loss_4 0.057301877612593435 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.05688423538143342 8\n",
            "val_loss_4 0.05723021349466735 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.05681495973208278 9\n",
            "val_loss_4 0.05716194139820082 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.056748143730657986 10\n",
            "val_loss_4 0.05709466686481152 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.056682904605529216 11\n",
            "val_loss_4 0.05703030752162737 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.056619802296285666 12\n",
            "val_loss_4 0.056967396501646554 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.05655825604666842 13\n",
            "val_loss_4 0.056906473251700215 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.056498772860456244 14\n",
            "val_loss_4 0.05684692030049353 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.05644070910175279 15\n",
            "val_loss_4 0.05678914212835761 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.05638449043987776 16\n",
            "val_loss_4 0.056733114594702864 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.05635314481051462 17\n",
            "val_loss_4 0.05672763354733893 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.05634773811802488 18\n",
            "val_loss_4 0.05672216405368834 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.0563423238760367 19\n",
            "val_loss_4 0.05671671019326866 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.05633692758579173 20\n",
            "val_loss_4 0.05671127402059817 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.05633154928752148 21\n",
            "val_loss_4 0.05670584386979154 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.056326212128370565 22\n",
            "val_loss_4 0.05670035393722651 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.056320826183898616 23\n",
            "val_loss_4 0.056694998174150094 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.05631549520010012 24\n",
            "val_loss_4 0.05668961904978412 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.05631018709179125 25\n",
            "val_loss_4 0.05668424211572354 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.056307166226095266 26\n",
            "val_loss_4 0.05668370318252198 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.05630663271787074 27\n",
            "val_loss_4 0.056683157459964086 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.056306097332052656 28\n",
            "val_loss_4 0.05668261644862911 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.05630556045729956 29\n",
            "val_loss_4 0.05668208014851705 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.775365, 0.768912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.997047, 0.967983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.784399, 0.770239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.124218, 0.10136 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.518937, 0.5214  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.604413, 0.606337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), array([[0.00030946, 0.0006308 ],\n",
            "       [0.00026854, 0.00068431],\n",
            "       [0.00037691, 0.00057595],\n",
            "       ...,\n",
            "       [0.00019105, 0.00051889],\n",
            "       [0.00036153, 0.00051998],\n",
            "       [0.00022487, 0.00057717]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.775365, 0.768912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.997047, 0.967983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.784399, 0.770239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.124218, 0.10136 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.518937, 0.5214  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.604413, 0.606337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), 'Z': array([[0.00030946, 0.0006308 ],\n",
            "       [0.00026854, 0.00068431],\n",
            "       [0.00037691, 0.00057595],\n",
            "       ...,\n",
            "       [0.00019105, 0.00051889],\n",
            "       [0.00036153, 0.00051998],\n",
            "       [0.00022487, 0.00057717]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.012923372882869785 epoch:  0\n",
            "reconstruction_loss:  0.012918595341029874 epoch:  0\n",
            "dcc_loss:  4.777554448067551e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.111596748196622\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.012893059617629711 epoch:  1\n",
            "reconstruction_loss:  0.012889039090949406 epoch:  1\n",
            "dcc_loss:  4.020521975849473e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.012896249644752653 epoch:  2\n",
            "reconstruction_loss:  0.012893584447591182 epoch:  2\n",
            "dcc_loss:  2.6652184645049214e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.012899235379298326 epoch:  3\n",
            "reconstruction_loss:  0.01289731232483701 epoch:  3\n",
            "dcc_loss:  1.923051560526304e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.012890848049195906 epoch:  4\n",
            "reconstruction_loss:  0.012889278434173885 epoch:  4\n",
            "dcc_loss:  1.5696242769024426e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.012895239994811613 epoch:  5\n",
            "reconstruction_loss:  0.012893833969791983 epoch:  5\n",
            "dcc_loss:  1.4060314217889518e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.01288192027189371 epoch:  6\n",
            "reconstruction_loss:  0.012880667297872043 epoch:  6\n",
            "dcc_loss:  1.2529718222025152e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.012895307814732342 epoch:  7\n",
            "reconstruction_loss:  0.012894201297503691 epoch:  7\n",
            "dcc_loss:  1.1064844364162508e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.012887502189641726 epoch:  8\n",
            "reconstruction_loss:  0.012886475183290358 epoch:  8\n",
            "dcc_loss:  1.0270073008442425e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.012893731521347792 epoch:  9\n",
            "reconstruction_loss:  0.012892771161025527 epoch:  9\n",
            "dcc_loss:  9.60378592174919e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.012895217261497447 epoch:  10\n",
            "reconstruction_loss:  0.012894303392177404 epoch:  10\n",
            "dcc_loss:  9.138361381242795e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.0592048958148876\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.012880849775621678 epoch:  11\n",
            "reconstruction_loss:  0.012879972413263505 epoch:  11\n",
            "dcc_loss:  8.773823659116297e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.1309689809652657\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.012887754773020708 epoch:  12\n",
            "reconstruction_loss:  0.012886894288049794 epoch:  12\n",
            "dcc_loss:  8.604849746666912e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.0865713649029336\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 8.381032705307007\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.266963275527973 0\n",
            "val_loss_0 0.24034474754673554 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.21392159599485663 1\n",
            "val_loss_0 0.19080168344139486 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.17069664598987677 2\n",
            "val_loss_0 0.1535024451661601 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.138336718844243 3\n",
            "val_loss_0 0.12553766660286017 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.1141366775258438 4\n",
            "val_loss_0 0.1045948956899711 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09603813203193973 5\n",
            "val_loss_0 0.0890004579125418 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.08261670803443227 6\n",
            "val_loss_0 0.07748256088861762 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07279295665039884 7\n",
            "val_loss_0 0.06903279282258921 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.0683432550233289 8\n",
            "val_loss_0 0.06834625319921649 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.06768315778114178 9\n",
            "val_loss_0 0.06768568376707955 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.06706074313367191 10\n",
            "val_loss_0 0.06704592381835739 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.06643709151782402 11\n",
            "val_loss_0 0.06642553970076014 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.0658298480927643 12\n",
            "val_loss_0 0.06582561280525241 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.06524835647194825 13\n",
            "val_loss_0 0.06524435032259454 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.06468344359400015 14\n",
            "val_loss_0 0.06468054909028834 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005472895730509032 0\n",
            "val_loss_1 0.0005514835859479149 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0005462746821417902 1\n",
            "val_loss_1 0.0005502370200909545 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0005450758577693455 2\n",
            "val_loss_1 0.0005489096084975662 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0005438092074808747 3\n",
            "val_loss_1 0.0005475217672972608 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0005424550664989484 4\n",
            "val_loss_1 0.0005460803757868885 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0005410448149755342 5\n",
            "val_loss_1 0.000544593875460982 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0005396118980533338 6\n",
            "val_loss_1 0.0005430649168436635 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0005380658819562089 7\n",
            "val_loss_1 0.0005414898099751166 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0005371938713213631 8\n",
            "val_loss_1 0.000541328416920301 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0005370299188934328 9\n",
            "val_loss_1 0.0005411663125512646 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0005368572744117516 10\n",
            "val_loss_1 0.0005410032489089068 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.0005367124192524167 11\n",
            "val_loss_1 0.0005408394362234323 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0005365704849665401 12\n",
            "val_loss_1 0.0005406746930456034 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0005363950966508117 13\n",
            "val_loss_1 0.0005405093672376852 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0005362432024937726 14\n",
            "val_loss_1 0.0005403429525498472 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.3593858682726555e-05 0\n",
            "val_loss_2 1.3748659817769879e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.3569323828349252e-05 1\n",
            "val_loss_2 1.372015393035283e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.3544432629838713e-05 2\n",
            "val_loss_2 1.3690096939359158e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.3515917932257052e-05 3\n",
            "val_loss_2 1.3658648454622925e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.3488011636258587e-05 4\n",
            "val_loss_2 1.3625986352264025e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.345729170391361e-05 5\n",
            "val_loss_2 1.3592218918364795e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.3426790388845715e-05 6\n",
            "val_loss_2 1.3557405858153492e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.339419740366112e-05 7\n",
            "val_loss_2 1.3521506330849811e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.3375895753605633e-05 8\n",
            "val_loss_2 1.3517851393065443e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.3371640086330976e-05 9\n",
            "val_loss_2 1.3514183565247609e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.336840204874026e-05 10\n",
            "val_loss_2 1.351050229391617e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.3365261157132473e-05 11\n",
            "val_loss_2 1.3506811304972562e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.3362005028049408e-05 12\n",
            "val_loss_2 1.350311082038538e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.3357907594199793e-05 13\n",
            "val_loss_2 1.349939825868868e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.3355511303552437e-05 14\n",
            "val_loss_2 1.3495679523670572e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.437057780149737e-07 0\n",
            "val_loss_3 3.451960609746679e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.43540278950077e-07 1\n",
            "val_loss_3 3.4498876757475473e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.433253228977356e-07 2\n",
            "val_loss_3 3.4475389448670445e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.4308719763851507e-07 3\n",
            "val_loss_3 3.4449915340761946e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.4283130108947703e-07 4\n",
            "val_loss_3 3.4423256930399844e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.4256535959081403e-07 5\n",
            "val_loss_3 3.4395627132672056e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.422935312895849e-07 6\n",
            "val_loss_3 3.4367047896705375e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.4200288754356265e-07 7\n",
            "val_loss_3 3.4337615752714506e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.418403609760862e-07 8\n",
            "val_loss_3 3.433462021264213e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.418066253559783e-07 9\n",
            "val_loss_3 3.433161638027704e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.4177818877534534e-07 10\n",
            "val_loss_3 3.432860254851496e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.4174763467457645e-07 11\n",
            "val_loss_3 3.432558216759829e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.4172283651767576e-07 12\n",
            "val_loss_3 3.43225534673635e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.416890570104837e-07 13\n",
            "val_loss_3 3.4319515929823815e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.4165591868838306e-07 14\n",
            "val_loss_3 3.431647144675704e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07485233325676204 0\n",
            "val_loss_4 0.07210909786579538 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.06893045054673912 1\n",
            "val_loss_4 0.06650257863919067 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.06410582163481414 2\n",
            "val_loss_4 0.062328002620754074 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.060538437196435486 3\n",
            "val_loss_4 0.059214829194186795 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.057893638627803515 4\n",
            "val_loss_4 0.05692107578158001 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.05595066633828801 5\n",
            "val_loss_4 0.05520786667942245 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.05450977218729245 6\n",
            "val_loss_4 0.053949140401914265 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.05345095674982808 7\n",
            "val_loss_4 0.05301066403603403 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.05297263558737341 8\n",
            "val_loss_4 0.05293283718394591 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.0529000440203292 9\n",
            "val_loss_4 0.052858935069830026 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.052830053284287415 10\n",
            "val_loss_4 0.05278594700970854 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.05276164469782241 11\n",
            "val_loss_4 0.05271621263348357 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.05269578382421577 12\n",
            "val_loss_4 0.052647626142588735 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.05263120329483087 13\n",
            "val_loss_4 0.052581900615605234 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.05256903165929041 14\n",
            "val_loss_4 0.05251736499121185 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.05250835266744171 15\n",
            "val_loss_4 0.052454644232610516 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.05244947342855553 16\n",
            "val_loss_4 0.05239387443392477 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.05241682447687221 17\n",
            "val_loss_4 0.0523879093172622 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.05241116514562509 18\n",
            "val_loss_4 0.05238197263324317 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.052405502103113344 19\n",
            "val_loss_4 0.052376044710445706 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.052399870007492015 20\n",
            "val_loss_4 0.052370131033488955 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.05239424625529556 21\n",
            "val_loss_4 0.052364259964171 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.05238866817582992 22\n",
            "val_loss_4 0.052358305592402794 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.05238304016167695 23\n",
            "val_loss_4 0.052352501808557945 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.0523774709818645 24\n",
            "val_loss_4 0.05234666428456405 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.052371916336700484 25\n",
            "val_loss_4 0.05234083638964856 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.05236877398438996 26\n",
            "val_loss_4 0.052340245030808565 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.05236821266105824 27\n",
            "val_loss_4 0.05233965307568597 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.052367651929757196 28\n",
            "val_loss_4 0.05233906320460058 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.05236708499874337 29\n",
            "val_loss_4 0.052338473510628834 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.7336983 , 0.72724533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.9553803 , 0.9263163 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.74273235, 0.7285723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.08255133, 0.05969333, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.47727033, 0.47973335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.56274635, 0.5646703 , 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[0.0003105 , 0.00066549],\n",
            "       [0.0002685 , 0.00071739],\n",
            "       [0.00037631, 0.00061098],\n",
            "       ...,\n",
            "       [0.0001934 , 0.00055866],\n",
            "       [0.00036214, 0.00055875],\n",
            "       [0.00022596, 0.0006143 ]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.7336983 , 0.72724533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.9553803 , 0.9263163 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.74273235, 0.7285723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.08255133, 0.05969333, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.47727033, 0.47973335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.56274635, 0.5646703 , 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[0.0003105 , 0.00066549],\n",
            "       [0.0002685 , 0.00071739],\n",
            "       [0.00037631, 0.00061098],\n",
            "       ...,\n",
            "       [0.0001934 , 0.00055866],\n",
            "       [0.00036214, 0.00055875],\n",
            "       [0.00022596, 0.0006143 ]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.01195251492323002 epoch:  0\n",
            "reconstruction_loss:  0.011948129994186645 epoch:  0\n",
            "dcc_loss:  4.384917538933567e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.1425736143966065\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.011942933021475026 epoch:  1\n",
            "reconstruction_loss:  0.011938411339106664 epoch:  1\n",
            "dcc_loss:  4.521704199784057e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.010448180186014752 epoch:  2\n",
            "reconstruction_loss:  0.010406214012176429 epoch:  2\n",
            "dcc_loss:  4.1966204658684095e-05 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.00806721676314784 epoch:  3\n",
            "reconstruction_loss:  0.008042709501615319 epoch:  3\n",
            "dcc_loss:  2.4507250106300112e-05 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.006528375556112313 epoch:  4\n",
            "reconstruction_loss:  0.006500287562460405 epoch:  4\n",
            "dcc_loss:  2.808797142041514e-05 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.006118750875861827 epoch:  5\n",
            "reconstruction_loss:  0.006105751023186941 epoch:  5\n",
            "dcc_loss:  1.2999837165905043e-05 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.005940941220161266 epoch:  6\n",
            "reconstruction_loss:  0.005933800074884478 epoch:  6\n",
            "dcc_loss:  7.141138757525557e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.005723769375645385 epoch:  7\n",
            "reconstruction_loss:  0.00571877005801724 epoch:  7\n",
            "dcc_loss:  4.9993271371109785e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.005570782344421508 epoch:  8\n",
            "reconstruction_loss:  0.005567152031130372 epoch:  8\n",
            "dcc_loss:  3.630300181621799e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.0054464271443611725 epoch:  9\n",
            "reconstruction_loss:  0.005443548010153868 epoch:  9\n",
            "dcc_loss:  2.8791405762993587e-06 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.005274791847973171 epoch:  10\n",
            "reconstruction_loss:  0.00527291399101511 epoch:  10\n",
            "dcc_loss:  1.8778591802092553e-06 epoch:  10\n",
            "epoch:  10 DBL:  0.9917377978475873\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.00503805418553686 epoch:  11\n",
            "reconstruction_loss:  0.0050346654343699036 epoch:  11\n",
            "dcc_loss:  3.3887548912197513e-06 epoch:  11\n",
            "epoch:  11 DBL:  1.1267285860251366\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.004733375940606661 epoch:  12\n",
            "reconstruction_loss:  0.0047209382476511 epoch:  12\n",
            "dcc_loss:  1.243768925556918e-05 epoch:  12\n",
            "epoch:  12 DBL:  1.1551956639775147\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.734793663024902\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.2662049419864205 0\n",
            "val_loss_0 0.23804761811783876 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.21259048240668801 1\n",
            "val_loss_0 0.18822301189797427 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16891691275586887 2\n",
            "val_loss_0 0.15074245603356612 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.1362347122014933 3\n",
            "val_loss_0 0.12263026231347854 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.11178280503357955 4\n",
            "val_loss_0 0.10162475712231335 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09350990997065428 5\n",
            "val_loss_0 0.08599398245004769 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.07996200038295287 6\n",
            "val_loss_0 0.07447271464649358 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.07005310613862424 7\n",
            "val_loss_0 0.06604135208859119 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.06556949252922471 8\n",
            "val_loss_0 0.06535794924230282 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.06490571012342695 9\n",
            "val_loss_0 0.06470019565732846 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.0642777247795274 10\n",
            "val_loss_0 0.06406413473295146 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.06364927025279146 11\n",
            "val_loss_0 0.06344711323004518 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.06304390267398399 12\n",
            "val_loss_0 0.06285096325346294 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.06246121979627585 13\n",
            "val_loss_0 0.062272837087393944 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.061889918602063304 14\n",
            "val_loss_0 0.06171284702779185 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005535283774681638 0\n",
            "val_loss_1 0.000555319999929122 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0005524932732754859 1\n",
            "val_loss_1 0.0005540539729726249 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0005512711399730897 2\n",
            "val_loss_1 0.0005527052858795693 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0005499834676588653 3\n",
            "val_loss_1 0.0005512964024714736 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0005486027402917721 4\n",
            "val_loss_1 0.0005498335988440871 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0005471694514703127 5\n",
            "val_loss_1 0.0005483261429300317 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0005457099483532608 6\n",
            "val_loss_1 0.0005467784594340035 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0005441417374899047 7\n",
            "val_loss_1 0.0005451820466243817 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0005432538357253275 8\n",
            "val_loss_1 0.0005450184172330787 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0005430868961480606 9\n",
            "val_loss_1 0.0005448539909226274 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0005429109840787265 10\n",
            "val_loss_1 0.0005446885571860827 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.0005427623994421555 11\n",
            "val_loss_1 0.0005445223444262475 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0005426208333415051 12\n",
            "val_loss_1 0.0005443551410292165 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0005424421290288195 13\n",
            "val_loss_1 0.0005441873861814771 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0005422838880625321 14\n",
            "val_loss_1 0.0005440185383027168 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.3756798737353115e-05 0\n",
            "val_loss_2 1.3860238807111416e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.37317987410007e-05 1\n",
            "val_loss_2 1.3831303733324528e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.3706449902382024e-05 2\n",
            "val_loss_2 1.3800765231909252e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.3677549268973369e-05 3\n",
            "val_loss_2 1.3768798952092743e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.364901147051464e-05 4\n",
            "val_loss_2 1.37356001210206e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.361770718913039e-05 5\n",
            "val_loss_2 1.3701285806948244e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.3586878342429449e-05 6\n",
            "val_loss_2 1.3665926620392232e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.3553754737418346e-05 7\n",
            "val_loss_2 1.3629481498603658e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.3534946673179595e-05 8\n",
            "val_loss_2 1.3625769337603514e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.3530621898184762e-05 9\n",
            "val_loss_2 1.3622046702280075e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.3527364380214169e-05 10\n",
            "val_loss_2 1.3618311156744209e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.3524025974886434e-05 11\n",
            "val_loss_2 1.3614565364619062e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.3520755586389857e-05 12\n",
            "val_loss_2 1.3610807963824616e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.351665999576962e-05 13\n",
            "val_loss_2 1.3607040887217281e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.3514188389625414e-05 14\n",
            "val_loss_2 1.3603267249564172e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.478036009798414e-07 0\n",
            "val_loss_3 3.480471896207504e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.4763544171841035e-07 1\n",
            "val_loss_3 3.478369061759289e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.4741708638400216e-07 2\n",
            "val_loss_3 3.475989205278362e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.471737372748503e-07 3\n",
            "val_loss_3 3.473408160479801e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.469148485615716e-07 4\n",
            "val_loss_3 3.4707060720799664e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.4664458861887277e-07 5\n",
            "val_loss_3 3.4679056292511104e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.46368924913133e-07 6\n",
            "val_loss_3 3.465007763468592e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.4607344872745624e-07 7\n",
            "val_loss_3 3.4620234429898807e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.459077639879501e-07 8\n",
            "val_loss_3 3.461719937419143e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.4587404050191e-07 9\n",
            "val_loss_3 3.461415485509079e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.458446163703617e-07 10\n",
            "val_loss_3 3.461110028254238e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.45813845137645e-07 11\n",
            "val_loss_3 3.460803715645574e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.457888085796822e-07 12\n",
            "val_loss_3 3.460496896310712e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.4575476927277836e-07 13\n",
            "val_loss_3 3.4601890018154627e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.457208241407708e-07 14\n",
            "val_loss_3 3.459880335745123e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.07210048752725508 0\n",
            "val_loss_4 0.06907932503996107 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.0661104937306123 1\n",
            "val_loss_4 0.06347120247438857 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.061228726812155104 2\n",
            "val_loss_4 0.05930467431688838 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.05761721423586922 3\n",
            "val_loss_4 0.05620749519101791 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.05494328393770389 4\n",
            "val_loss_4 0.053926056200747634 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.052975447287966285 5\n",
            "val_loss_4 0.05223841175271667 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.05151994080739228 6\n",
            "val_loss_4 0.050993913793478826 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.05044869499599025 7\n",
            "val_loss_4 0.050073637873548335 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.04996398150002556 8\n",
            "val_loss_4 0.04999784020656829 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.04989075358083401 9\n",
            "val_loss_4 0.049925451103462096 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.04981999559333312 10\n",
            "val_loss_4 0.049854297633688725 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.04975085617009398 11\n",
            "val_loss_4 0.04978603916316516 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.0496842105915652 12\n",
            "val_loss_4 0.04971935574656054 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.04961914671822108 13\n",
            "val_loss_4 0.049654993758479316 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.04955623444159794 14\n",
            "val_loss_4 0.049592170209023165 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.04949496812615749 15\n",
            "val_loss_4 0.049531018081019305 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.04943540663613704 16\n",
            "val_loss_4 0.0494719206779109 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.04940244820890573 17\n",
            "val_loss_4 0.04946611054749383 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.04939672790698794 18\n",
            "val_loss_4 0.04946031525329629 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.049390999370528724 19\n",
            "val_loss_4 0.049454539364850274 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.04938531029675284 20\n",
            "val_loss_4 0.04944877044287423 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.04937961822406844 21\n",
            "val_loss_4 0.04944306998122513 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.04937397390323506 22\n",
            "val_loss_4 0.04943731374058595 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.04936830755177379 23\n",
            "val_loss_4 0.04943162784358212 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.04936268025256061 24\n",
            "val_loss_4 0.04942594252514953 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.04935707521835156 25\n",
            "val_loss_4 0.04942026605649531 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.049353878368214504 26\n",
            "val_loss_4 0.04941969419786745 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.0493533117970697 27\n",
            "val_loss_4 0.04941912070389028 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.049352746557255724 28\n",
            "val_loss_4 0.0494185457280623 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.04935217506384089 29\n",
            "val_loss_4 0.04941797934224602 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.6920317 , 0.68557864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.9137137 , 0.8846497 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.70106566, 0.6869057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.04088467, 0.01802667, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.43560368, 0.43806666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.52107966, 0.52300364, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[0.00031846, 0.00065792],\n",
            "       [0.00026969, 0.00071282],\n",
            "       [0.00037999, 0.00060447],\n",
            "       ...,\n",
            "       [0.00020257, 0.00056042],\n",
            "       [0.00036676, 0.00055607],\n",
            "       [0.00022761, 0.00061246]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.6920317 , 0.68557864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.9137137 , 0.8846497 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.70106566, 0.6869057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.04088467, 0.01802667, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.43560368, 0.43806666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.52107966, 0.52300364, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[0.00031846, 0.00065792],\n",
            "       [0.00026969, 0.00071282],\n",
            "       [0.00037999, 0.00060447],\n",
            "       ...,\n",
            "       [0.00020257, 0.00056042],\n",
            "       [0.00036676, 0.00055607],\n",
            "       [0.00022761, 0.00061246]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.011231457084028444 epoch:  0\n",
            "reconstruction_loss:  0.01122671618855397 epoch:  0\n",
            "dcc_loss:  4.740871441766797e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.1148094974252278\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.011223914008521352 epoch:  1\n",
            "reconstruction_loss:  0.011219788467414815 epoch:  1\n",
            "dcc_loss:  4.125511275725919e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.011223405082650083 epoch:  2\n",
            "reconstruction_loss:  0.01122062200610812 epoch:  2\n",
            "dcc_loss:  2.783083067326276e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.011223034559351348 epoch:  3\n",
            "reconstruction_loss:  0.011221096334960415 epoch:  3\n",
            "dcc_loss:  1.9382020906185324e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.011223811865279884 epoch:  4\n",
            "reconstruction_loss:  0.011222129431877063 epoch:  4\n",
            "dcc_loss:  1.6824509575218996e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.01122825477597324 epoch:  5\n",
            "reconstruction_loss:  0.011226763357639028 epoch:  5\n",
            "dcc_loss:  1.491424173007647e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.011230808305430842 epoch:  6\n",
            "reconstruction_loss:  0.011229524609868886 epoch:  6\n",
            "dcc_loss:  1.2837002268112222e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.011232167646674767 epoch:  7\n",
            "reconstruction_loss:  0.011230967846720952 epoch:  7\n",
            "dcc_loss:  1.1997684020884553e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.011222736026411917 epoch:  8\n",
            "reconstruction_loss:  0.011221625952777366 epoch:  8\n",
            "dcc_loss:  1.1100842127439402e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.011222168955303655 epoch:  9\n",
            "reconstruction_loss:  0.011221124846004248 epoch:  9\n",
            "dcc_loss:  1.0441301134730172e-06 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.011222831661100774 epoch:  10\n",
            "reconstruction_loss:  0.011221825393334132 epoch:  10\n",
            "dcc_loss:  1.0062576657290458e-06 epoch:  10\n",
            "epoch:  10 DBL:  1.1357768252348046\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.011226332053799833 epoch:  11\n",
            "reconstruction_loss:  0.011225356931127325 epoch:  11\n",
            "dcc_loss:  9.751582445530477e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.123221864450469\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.011227981737640525 epoch:  12\n",
            "reconstruction_loss:  0.011227073911512182 epoch:  12\n",
            "dcc_loss:  9.078518280871274e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.0881644651544142\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}], 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)}, {10: 9}, {10: 1.0881644651544142}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 8.025096654891968\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.26260612006497125 0\n",
            "val_loss_0 0.23487808263717477 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.20937975695882505 1\n",
            "val_loss_0 0.18537644491709546 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16604375394693655 2\n",
            "val_loss_0 0.1481202961242331 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.1336118330139832 3\n",
            "val_loss_0 0.12019083960640072 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.10935587855328137 4\n",
            "val_loss_0 0.0993307414339197 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.09124273674821494 5\n",
            "val_loss_0 0.08380088971694562 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.07781243432646498 6\n",
            "val_loss_0 0.07235258249728313 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.06799083822273493 7\n",
            "val_loss_0 0.06397626242598906 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.06354608644125932 8\n",
            "val_loss_0 0.06329604117834436 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.06288824388627241 9\n",
            "val_loss_0 0.06264231517533114 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.06226379622390718 10\n",
            "val_loss_0 0.06200950062596099 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.061641780604745566 11\n",
            "val_loss_0 0.06139625502652865 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.06103625536122939 12\n",
            "val_loss_0 0.0608026649029527 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.06046534858973981 13\n",
            "val_loss_0 0.0602274361684942 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.05989519317235687 14\n",
            "val_loss_0 0.05967029972849104 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005407828422999428 0\n",
            "val_loss_1 0.0005431893032941548 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0005397707849672146 1\n",
            "val_loss_1 0.000541950520843846 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0005385759231333348 2\n",
            "val_loss_1 0.0005406312611826331 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0005373178193823461 3\n",
            "val_loss_1 0.0005392521683815955 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0005359665820359213 4\n",
            "val_loss_1 0.0005378208428299536 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0005345657073727851 5\n",
            "val_loss_1 0.0005363467476543234 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0005331372625613784 6\n",
            "val_loss_1 0.000534834840468525 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0005316076268601425 7\n",
            "val_loss_1 0.0005332761544331306 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0005307417683938478 8\n",
            "val_loss_1 0.0005331164506919351 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.000530577048470255 9\n",
            "val_loss_1 0.0005329559057531805 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0005304072355060927 10\n",
            "val_loss_1 0.0005327945086395106 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.0005302625945957028 11\n",
            "val_loss_1 0.0005326321947782429 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0005301205589294169 12\n",
            "val_loss_1 0.0005324689720103462 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0005299455716254286 13\n",
            "val_loss_1 0.0005323051516683964 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0005297931739458822 14\n",
            "val_loss_1 0.0005321403463162989 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.3451577768171095e-05 0\n",
            "val_loss_2 1.356756154408543e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.3427212310763927e-05 1\n",
            "val_loss_2 1.3539202046398794e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.3402190657840503e-05 2\n",
            "val_loss_2 1.3509254487364301e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.33740319538586e-05 3\n",
            "val_loss_2 1.347787407059525e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.3345894181861666e-05 4\n",
            "val_loss_2 1.3445290046410669e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.3315266992474733e-05 5\n",
            "val_loss_2 1.341161029443092e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.3285047910652871e-05 6\n",
            "val_loss_2 1.3376935362105926e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.3252682908837682e-05 7\n",
            "val_loss_2 1.334119336476174e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.3234223464109724e-05 8\n",
            "val_loss_2 1.3337553726955596e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.3229961577614235e-05 9\n",
            "val_loss_2 1.3333903557171979e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.3226699287882776e-05 10\n",
            "val_loss_2 1.3330241752774679e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.3223533333041224e-05 11\n",
            "val_loss_2 1.3326570449851099e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.3220151853276664e-05 12\n",
            "val_loss_2 1.3322886759926777e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.321625378691941e-05 13\n",
            "val_loss_2 1.3319193949111056e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.3213841097973547e-05 14\n",
            "val_loss_2 1.3315494954884446e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.401759470759882e-07 0\n",
            "val_loss_3 3.4078386660498815e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.4001126268040506e-07 1\n",
            "val_loss_3 3.405779118630849e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.3979750913135647e-07 2\n",
            "val_loss_3 3.4034435428128763e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.395588916018208e-07 3\n",
            "val_loss_3 3.400909491126305e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.3930518232198447e-07 4\n",
            "val_loss_3 3.3982576690644896e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.3903925037231744e-07 5\n",
            "val_loss_3 3.395505720608441e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.387687709069018e-07 6\n",
            "val_loss_3 3.3926560519193596e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.384778693605152e-07 7\n",
            "val_loss_3 3.389721791034378e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.3831505976710496e-07 8\n",
            "val_loss_3 3.3894233608332427e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.3828249232064964e-07 9\n",
            "val_loss_3 3.38912379241246e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.382537717791768e-07 10\n",
            "val_loss_3 3.3888234470115064e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.3822348100091347e-07 11\n",
            "val_loss_3 3.3885222016648247e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.3819895240673517e-07 12\n",
            "val_loss_3 3.388220264017551e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.381656606473906e-07 13\n",
            "val_loss_3 3.387917434532168e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.3813214943009685e-07 14\n",
            "val_loss_3 3.387613965445717e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.06975792796320289 0\n",
            "val_loss_4 0.06674163650408411 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.06382697596658828 1\n",
            "val_loss_4 0.061178652389586065 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.058997257404198415 2\n",
            "val_loss_4 0.05703093034175421 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.055419479003202855 3\n",
            "val_loss_4 0.05394948201870956 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.05277137056858242 4\n",
            "val_loss_4 0.051677792467377644 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.05081992998887523 5\n",
            "val_loss_4 0.04999605066212346 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.04937821675716124 6\n",
            "val_loss_4 0.04875266205419073 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.04831584945895397 7\n",
            "val_loss_4 0.0478324089996316 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.04783310551212208 8\n",
            "val_loss_4 0.04775665889356858 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.04776044457857063 9\n",
            "val_loss_4 0.04768411386551456 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.04769021288604268 10\n",
            "val_loss_4 0.04761297915207603 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.0476216041939465 11\n",
            "val_loss_4 0.04754448493148559 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.04755538854853941 12\n",
            "val_loss_4 0.04747772845550309 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.04749083891953879 13\n",
            "val_loss_4 0.047413075463802044 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.04742829164274736 14\n",
            "val_loss_4 0.04735013809521489 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.047367484880048244 15\n",
            "val_loss_4 0.047288882361633464 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.04730834103629833 16\n",
            "val_loss_4 0.047229700097474736 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.04727564159849762 17\n",
            "val_loss_4 0.04722386489957335 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.0472699617683728 18\n",
            "val_loss_4 0.04721804627950898 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.04726426650263516 19\n",
            "val_loss_4 0.04721228350337636 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.047258630851507895 20\n",
            "val_loss_4 0.047206485948027975 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.04725297133275979 21\n",
            "val_loss_4 0.047200774824137544 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.047247360057868254 22\n",
            "val_loss_4 0.04719500628000394 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.0472417302620868 23\n",
            "val_loss_4 0.04718928228585542 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.04723614430093593 24\n",
            "val_loss_4 0.047183570300011926 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.04723057466049113 25\n",
            "val_loss_4 0.04717786694550665 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.04722738256837619 26\n",
            "val_loss_4 0.04717729490386136 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.047226819181795415 27\n",
            "val_loss_4 0.04717671879450606 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.04722625831909604 28\n",
            "val_loss_4 0.04717613991037035 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.04722568990468365 29\n",
            "val_loss_4 0.04717557214897144 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.650365, 0.643912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.872047, 0.842983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.659399, 0.645239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.999218, 0.97636 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.393937, 0.3964  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.479413, 0.481337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), array([[0.00030553, 0.00064224],\n",
            "       [0.00025101, 0.00070038],\n",
            "       [0.00036466, 0.00058834],\n",
            "       ...,\n",
            "       [0.00028564, 0.00072072],\n",
            "       [0.00035191, 0.00054342],\n",
            "       [0.0002082 , 0.0005977 ]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.650365, 0.643912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.872047, 0.842983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.659399, 0.645239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.999218, 0.97636 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.393937, 0.3964  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.479413, 0.481337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), 'Z': array([[0.00030553, 0.00064224],\n",
            "       [0.00025101, 0.00070038],\n",
            "       [0.00036466, 0.00058834],\n",
            "       ...,\n",
            "       [0.00028564, 0.00072072],\n",
            "       [0.00035191, 0.00054342],\n",
            "       [0.0002082 , 0.0005977 ]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.0107355454953307 epoch:  0\n",
            "reconstruction_loss:  0.010731144805109005 epoch:  0\n",
            "dcc_loss:  4.400687341545072e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.127709431072422\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.010737493223050934 epoch:  1\n",
            "reconstruction_loss:  0.010733219249749256 epoch:  1\n",
            "dcc_loss:  4.273977919513257e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.010724103954510927 epoch:  2\n",
            "reconstruction_loss:  0.010721257131526074 epoch:  2\n",
            "dcc_loss:  2.8468224021769157e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.01072365235927861 epoch:  3\n",
            "reconstruction_loss:  0.0107215850532881 epoch:  3\n",
            "dcc_loss:  2.0673226036960743e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.010714386909785205 epoch:  4\n",
            "reconstruction_loss:  0.01071273850066539 epoch:  4\n",
            "dcc_loss:  1.6483941823464225e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.010721976654952503 epoch:  5\n",
            "reconstruction_loss:  0.010720567543473369 epoch:  5\n",
            "dcc_loss:  1.4091041437342464e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.010723414657370867 epoch:  6\n",
            "reconstruction_loss:  0.010722127842642922 epoch:  6\n",
            "dcc_loss:  1.2868228408666483e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.01072629838483766 epoch:  7\n",
            "reconstruction_loss:  0.010725096442578165 epoch:  7\n",
            "dcc_loss:  1.2019771207675024e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.010719453368550078 epoch:  8\n",
            "reconstruction_loss:  0.010718309474656488 epoch:  8\n",
            "dcc_loss:  1.1439085625480196e-06 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.01071878473085674 epoch:  9\n",
            "reconstruction_loss:  0.010717687141529314 epoch:  9\n",
            "dcc_loss:  1.097606940319975e-06 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.010724170614534331 epoch:  10\n",
            "reconstruction_loss:  0.010723136116963192 epoch:  10\n",
            "dcc_loss:  1.0344694680126036e-06 epoch:  10\n",
            "epoch:  10 DBL:  1.1246394368514294\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.010723607724244863 epoch:  11\n",
            "reconstruction_loss:  0.010722680472022867 epoch:  11\n",
            "dcc_loss:  9.272208484711632e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.1495188175968498\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.01072047125830303 epoch:  12\n",
            "reconstruction_loss:  0.010719614666380318 epoch:  12\n",
            "dcc_loss:  8.565828338685489e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.0714495073472743\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}], 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)}, {10: 9}, {10: 1.0881644651544142}], 21: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 7}, {10: 1.0714495073472743}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.803095579147339\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.258234411356979 0\n",
            "val_loss_0 0.2311142521029229 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.20571410146622265 1\n",
            "val_loss_0 0.18219958368553035 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.16295727098099153 2\n",
            "val_loss_0 0.1453797245507384 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.1309592228770008 3\n",
            "val_loss_0 0.11779896142743469 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.10703597766644629 4\n",
            "val_loss_0 0.09718658302788122 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.08917598355327856 5\n",
            "val_loss_0 0.08184181969677778 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.07593776208433373 6\n",
            "val_loss_0 0.07051875650079806 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.06625074741094611 7\n",
            "val_loss_0 0.06223203414215717 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.06186271264239663 8\n",
            "val_loss_0 0.061558229237700414 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.061212675187714234 9\n",
            "val_loss_0 0.06091120007275212 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.06059644896753725 10\n",
            "val_loss_0 0.0602841348329739 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.059980572991454775 11\n",
            "val_loss_0 0.05967636409373556 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.059382596908656334 12\n",
            "val_loss_0 0.059087880681934525 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.05881985631011693 13\n",
            "val_loss_0 0.05851716750113976 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.0582559076227808 14\n",
            "val_loss_0 0.05796539823033164 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005232275278954062 0\n",
            "val_loss_1 0.0005261502082005037 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0005222494794079073 1\n",
            "val_loss_1 0.0005249517637514546 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0005210926191545237 2\n",
            "val_loss_1 0.0005236764636965212 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0005198759254569461 3\n",
            "val_loss_1 0.0005223420252115798 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0005185677479615496 4\n",
            "val_loss_1 0.0005209584946990627 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0005172156321540801 5\n",
            "val_loss_1 0.000519533071551832 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0005158322039007699 6\n",
            "val_loss_1 0.0005180722868354864 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0005143573882760838 7\n",
            "val_loss_1 0.0005165669435551081 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0005135198902108243 8\n",
            "val_loss_1 0.000516412793987072 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.000513361937768372 9\n",
            "val_loss_1 0.0005162578244382153 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0005131965076686392 10\n",
            "val_loss_1 0.0005161019446912758 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.000513057072292491 11\n",
            "val_loss_1 0.0005159452423806083 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0005129219353903334 12\n",
            "val_loss_1 0.0005157876907546728 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0005127525861554691 13\n",
            "val_loss_1 0.0005156294307664108 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0005126059756427044 14\n",
            "val_loss_1 0.0005154702493259698 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.3029803368143394e-05 0\n",
            "val_loss_2 1.3153613110806918e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.300634202398317e-05 1\n",
            "val_loss_2 1.312608309823268e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.2981971983778811e-05 2\n",
            "val_loss_2 1.309698803553549e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.2954726797163199e-05 3\n",
            "val_loss_2 1.3066502121939854e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.2927310824329479e-05 4\n",
            "val_loss_2 1.3034835674131835e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.2897564325073935e-05 5\n",
            "val_loss_2 1.3002120978187876e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.28683662222879e-05 6\n",
            "val_loss_2 1.2968445142368357e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.2836919178981019e-05 7\n",
            "val_loss_2 1.2933760508286091e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.2819018303041893e-05 8\n",
            "val_loss_2 1.293022993921779e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.2814917177777389e-05 9\n",
            "val_loss_2 1.2926687638965054e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.2811769029135387e-05 10\n",
            "val_loss_2 1.2923137529453532e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.2808647816873524e-05 11\n",
            "val_loss_2 1.2919575870368245e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.2805375634889078e-05 12\n",
            "val_loss_2 1.2916002784224328e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.2801648530737564e-05 13\n",
            "val_loss_2 1.291242129786627e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.2799299861730209e-05 14\n",
            "val_loss_2 1.2908833625214616e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.2961751362024203e-07 0\n",
            "val_loss_3 3.3045925464404876e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.2945794312456095e-07 1\n",
            "val_loss_3 3.3025978306973167e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.2925029850276285e-07 2\n",
            "val_loss_3 3.300330151235252e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.29019195095361e-07 3\n",
            "val_loss_3 3.297869351887899e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.2877316806588217e-07 4\n",
            "val_loss_3 3.2952924176522583e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.2851418941294457e-07 5\n",
            "val_loss_3 3.292616144812713e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.282518556381747e-07 6\n",
            "val_loss_3 3.289844194860167e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.2796844531435385e-07 7\n",
            "val_loss_3 3.2869894926908503e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.2780994626896503e-07 8\n",
            "val_loss_3 3.286699049395445e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.277785809963326e-07 9\n",
            "val_loss_3 3.286407345815683e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.277510862519582e-07 10\n",
            "val_loss_3 3.2861148913803e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.277213762587819e-07 11\n",
            "val_loss_3 3.2858215437555384e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.2769734770421394e-07 12\n",
            "val_loss_3 3.2855275542775926e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.276647611822945e-07 13\n",
            "val_loss_3 3.285232641431908e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.2763275024715865e-07 14\n",
            "val_loss_3 3.2849369079149934e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.06769527941366424 0\n",
            "val_loss_4 0.06469062971993218 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.06186419573497999 1\n",
            "val_loss_4 0.05920182837382927 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.05711426258745626 2\n",
            "val_loss_4 0.05510495931925562 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.05359171880439572 3\n",
            "val_loss_4 0.05205987230911497 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.0509860021747146 4\n",
            "val_loss_4 0.049805098120270744 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.049063508485476276 5\n",
            "val_loss_4 0.04813628169676959 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.04764286133375901 6\n",
            "val_loss_4 0.04690472596869605 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.046597655095956515 7\n",
            "val_loss_4 0.04598690941929817 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.04611877619668016 8\n",
            "val_loss_4 0.045911684490610415 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.04604718897830893 9\n",
            "val_loss_4 0.045839439221493986 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.04597785385792642 10\n",
            "val_loss_4 0.045768826308463906 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.04591033936955347 11\n",
            "val_loss_4 0.04570042502530592 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.045845011092134076 12\n",
            "val_loss_4 0.045633892485910287 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.04578132975990846 13\n",
            "val_loss_4 0.045569552070270816 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.04571967865700742 14\n",
            "val_loss_4 0.0455068624364404 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.04565966728494128 15\n",
            "val_loss_4 0.04544582053216059 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.045601318601877265 16\n",
            "val_loss_4 0.04538679605854672 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.04556911063996838 17\n",
            "val_loss_4 0.04538098299985076 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.04556350939859428 18\n",
            "val_loss_4 0.04537515031105932 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.04555788559546251 19\n",
            "val_loss_4 0.04536940987486091 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.045552325450392554 20\n",
            "val_loss_4 0.04536363596418391 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.045546753414142806 21\n",
            "val_loss_4 0.045357938466236446 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.04554121281309807 22\n",
            "val_loss_4 0.045352221054811674 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.04553567832798123 23\n",
            "val_loss_4 0.04534649327633498 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.04553016323482188 24\n",
            "val_loss_4 0.04534079946235129 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.04552467102305771 25\n",
            "val_loss_4 0.045335120189397726 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.04552152704155553 26\n",
            "val_loss_4 0.045334547728583474 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.0455209691128037 27\n",
            "val_loss_4 0.045333969676881886 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.04552041240723745 28\n",
            "val_loss_4 0.045333391660603024 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.04551985218639677 29\n",
            "val_loss_4 0.04533282118936537 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.6086983 , 0.60224533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.8303803 , 0.8013163 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.61773235, 0.6035723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.95755136, 0.93469334, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.35227033, 0.35473335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.43774635, 0.43967032, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[0.00029849, 0.00062246],\n",
            "       [0.00023946, 0.00068365],\n",
            "       [0.00035556, 0.00056871],\n",
            "       ...,\n",
            "       [0.00027568, 0.00070015],\n",
            "       [0.00034333, 0.00052481],\n",
            "       [0.00019509, 0.00058009]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.6086983 , 0.60224533, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.8303803 , 0.8013163 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.61773235, 0.6035723 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.95755136, 0.93469334, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.35227033, 0.35473335, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.43774635, 0.43967032, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[0.00029849, 0.00062246],\n",
            "       [0.00023946, 0.00068365],\n",
            "       [0.00035556, 0.00056871],\n",
            "       ...,\n",
            "       [0.00027568, 0.00070015],\n",
            "       [0.00034333, 0.00052481],\n",
            "       [0.00019509, 0.00058009]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.010348421584149857 epoch:  0\n",
            "reconstruction_loss:  0.010343911296404149 epoch:  0\n",
            "dcc_loss:  4.510273719607047e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.0884112845012384\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.01032742750429075 epoch:  1\n",
            "reconstruction_loss:  0.010323004282133598 epoch:  1\n",
            "dcc_loss:  4.423217663206035e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.010322166926568515 epoch:  2\n",
            "reconstruction_loss:  0.010319097127941814 epoch:  2\n",
            "dcc_loss:  3.0697839070726597e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.010322852820999229 epoch:  3\n",
            "reconstruction_loss:  0.010320783566075003 epoch:  3\n",
            "dcc_loss:  2.0692659588742238e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.010324280671975888 epoch:  4\n",
            "reconstruction_loss:  0.010322729687675742 epoch:  4\n",
            "dcc_loss:  1.5509950125665604e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.01032968292964214 epoch:  5\n",
            "reconstruction_loss:  0.010328388151761004 epoch:  5\n",
            "dcc_loss:  1.2947628519184165e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.010324355670439475 epoch:  6\n",
            "reconstruction_loss:  0.010323254778782289 epoch:  6\n",
            "dcc_loss:  1.100897966985196e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.010319011730170507 epoch:  7\n",
            "reconstruction_loss:  0.010318037005679789 epoch:  7\n",
            "dcc_loss:  9.747352656983754e-07 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.01031867117177002 epoch:  8\n",
            "reconstruction_loss:  0.01031780073372752 epoch:  8\n",
            "dcc_loss:  8.704574355997368e-07 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.010325552707066319 epoch:  9\n",
            "reconstruction_loss:  0.010324769087897876 epoch:  9\n",
            "dcc_loss:  7.83638311558134e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.010319028525034505 epoch:  10\n",
            "reconstruction_loss:  0.010318302357136008 epoch:  10\n",
            "dcc_loss:  7.261407383132265e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.0535045810609172\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.010322747244262078 epoch:  11\n",
            "reconstruction_loss:  0.010322049573246901 epoch:  11\n",
            "dcc_loss:  6.976868652049139e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.0885802530598143\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.01032386851370717 epoch:  12\n",
            "reconstruction_loss:  0.010323207347217112 epoch:  12\n",
            "dcc_loss:  6.611412207718694e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.121244221724319\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}], 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)}, {10: 9}, {10: 1.0881644651544142}], 21: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 7}, {10: 1.0714495073472743}], 22: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.121244221724319}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.8145670890808105\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.253048214532511 0\n",
            "val_loss_0 0.22576279387987927 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.20154668042914406 1\n",
            "val_loss_0 0.1779126914811399 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.15961884991933437 2\n",
            "val_loss_0 0.14191889411012645 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.128251594955534 3\n",
            "val_loss_0 0.11496411425706513 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.10480224822972582 4\n",
            "val_loss_0 0.0948385991829133 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.087301519391476 5\n",
            "val_loss_0 0.07987675943342518 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.07432998068845509 6\n",
            "val_loss_0 0.06882703680952452 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.06482904740500992 7\n",
            "val_loss_0 0.0607497088386782 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.06052365246818782 8\n",
            "val_loss_0 0.0600919587015161 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.05988403284448138 9\n",
            "val_loss_0 0.05946112571729911 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.05927987176145043 10\n",
            "val_loss_0 0.0588499130991106 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.058675290205977566 11\n",
            "val_loss_0 0.05825667140530142 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.058087215328401394 12\n",
            "val_loss_0 0.05768238316200048 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.057531063207122056 13\n",
            "val_loss_0 0.05712605129024117 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.05698322391150953 14\n",
            "val_loss_0 0.05658672118881236 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.0005012086119617572 0\n",
            "val_loss_1 0.0005027281756275931 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0005002729709588754 1\n",
            "val_loss_1 0.0005015841030376253 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.0004991671055588839 2\n",
            "val_loss_1 0.0005003674976138779 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0004980049738212233 3\n",
            "val_loss_1 0.0004990942229272535 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0004967541937537478 4\n",
            "val_loss_1 0.0004977746801148279 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0004954605784794441 5\n",
            "val_loss_1 0.0004964155807228472 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0004941382450083671 6\n",
            "val_loss_1 0.0004950231516795755 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.000492730416381001 7\n",
            "val_loss_1 0.0004935898685609412 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.000491932643449703 8\n",
            "val_loss_1 0.0004934431535687015 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.0004917803837545512 9\n",
            "val_loss_1 0.0004932956886570015 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0004916228366145852 10\n",
            "val_loss_1 0.000493147353720652 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.000491492917934359 11\n",
            "val_loss_1 0.0004929982561347992 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0004913597580799434 12\n",
            "val_loss_1 0.0004928484584426982 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.0004911995213342699 13\n",
            "val_loss_1 0.0004926978602338281 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.0004910599729108049 14\n",
            "val_loss_1 0.0004925464009021139 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.2504927647224238e-05 0\n",
            "val_loss_2 1.2594922374972794e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.248245636919224e-05 1\n",
            "val_loss_2 1.2568472762829765e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.245899369543781e-05 2\n",
            "val_loss_2 1.2540500535482187e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.2432720429210992e-05 3\n",
            "val_loss_2 1.2511213173030429e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.2406495953099136e-05 4\n",
            "val_loss_2 1.2480770193061864e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.237780373374002e-05 5\n",
            "val_loss_2 1.2449330917139657e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.234990018136438e-05 6\n",
            "val_loss_2 1.2416996242962877e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.231975111468483e-05 7\n",
            "val_loss_2 1.2383749142369232e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.2302414351730614e-05 8\n",
            "val_loss_2 1.238036578172071e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.2298499528521456e-05 9\n",
            "val_loss_2 1.2376971100673792e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.2295567519722887e-05 10\n",
            "val_loss_2 1.237357065853286e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.2292487229045582e-05 11\n",
            "val_loss_2 1.2370157940375486e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.2289355655590833e-05 12\n",
            "val_loss_2 1.2366735390738935e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.2285853791726566e-05 13\n",
            "val_loss_2 1.2363307061270762e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.2283461292968348e-05 14\n",
            "val_loss_2 1.2359871955994992e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 3.1649502697974476e-07 0\n",
            "val_loss_3 3.1659420916464523e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 3.163419037009007e-07 1\n",
            "val_loss_3 3.1640337810529125e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 3.161427085494718e-07 2\n",
            "val_loss_3 3.1618561506648743e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 3.159205884494691e-07 3\n",
            "val_loss_3 3.159493775562152e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 3.1568418519158945e-07 4\n",
            "val_loss_3 3.1570172514874154e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 3.154349388353932e-07 5\n",
            "val_loss_3 3.154443567956639e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 3.1518220893730756e-07 6\n",
            "val_loss_3 3.1517762707019406e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 3.1490941445565285e-07 7\n",
            "val_loss_3 3.14902724594345e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 3.1475640618434225e-07 8\n",
            "val_loss_3 3.148747263728893e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 3.1472635575298484e-07 9\n",
            "val_loss_3 3.148466222569188e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 3.1469987338804183e-07 10\n",
            "val_loss_3 3.148184326956507e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 3.146711694073609e-07 11\n",
            "val_loss_3 3.147901380055873e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 3.146478158240402e-07 12\n",
            "val_loss_3 3.147617899854067e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 3.146160700981181e-07 13\n",
            "val_loss_3 3.1473334341261084e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 3.1458568457613003e-07 14\n",
            "val_loss_3 3.147048089171893e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.06593173194333747 0\n",
            "val_loss_4 0.06283462733006705 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.06024356544081276 1\n",
            "val_loss_4 0.05751424119844679 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.05560967147816849 2\n",
            "val_loss_4 0.05355028857708733 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.05217218512102089 3\n",
            "val_loss_4 0.050606219768051866 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.049627291348488846 4\n",
            "val_loss_4 0.048424809627978814 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.047746850793483085 5\n",
            "val_loss_4 0.04681099978424668 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.04635525498504883 6\n",
            "val_loss_4 0.04562367579166183 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.045331766593964734 7\n",
            "val_loss_4 0.04473718404533746 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.04485983777830755 8\n",
            "val_loss_4 0.04466491258111317 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.04478963266515193 9\n",
            "val_loss_4 0.04459541964030304 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.04472158668788103 10\n",
            "val_loss_4 0.04452744677733695 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.044655346958193495 11\n",
            "val_loss_4 0.04446149001668448 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.044591191609444615 12\n",
            "val_loss_4 0.04439748135851416 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.044528670400826954 13\n",
            "val_loss_4 0.0443355938605669 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.04446815496188236 14\n",
            "val_loss_4 0.044275084566531585 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.0444091730215446 15\n",
            "val_loss_4 0.04421640828062917 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.044351842399824246 16\n",
            "val_loss_4 0.044159866485703386 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.04432032335936037 17\n",
            "val_loss_4 0.04415426759872875 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.04431481632689802 18\n",
            "val_loss_4 0.04414865107123523 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.044309292605002264 19\n",
            "val_loss_4 0.04414312284079671 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.04430382136566651 20\n",
            "val_loss_4 0.04413759853637728 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.04429835743185646 21\n",
            "val_loss_4 0.044132106295431094 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.04429291723353072 22\n",
            "val_loss_4 0.0441266029848822 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.044287484715363555 23\n",
            "val_loss_4 0.0441210810006515 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.04428206278140571 24\n",
            "val_loss_4 0.04411560827172502 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.04427666970857435 25\n",
            "val_loss_4 0.04411014784038917 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.044273569914558934 26\n",
            "val_loss_4 0.044109598262657645 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.04427302064038027 27\n",
            "val_loss_4 0.04410904382020471 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.04427247417539456 28\n",
            "val_loss_4 0.04410848662658652 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.04427192482001468 29\n",
            "val_loss_4 0.04410793653522543 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.5670317 , 0.56057864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.7887137 , 0.7596497 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.57606566, 0.5619057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.9158847 , 0.89302665, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.31060368, 0.31306666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.39607966, 0.39800367, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), array([[0.00028969, 0.00059192],\n",
            "       [0.00022661, 0.00065451],\n",
            "       [0.00034502, 0.00053823],\n",
            "       ...,\n",
            "       [0.0002623 , 0.00066711],\n",
            "       [0.00033335, 0.00049736],\n",
            "       [0.00018098, 0.00055366]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.5670317 , 0.56057864, 0.624645  , ..., 0.182024  , 0.900969  ,\n",
            "        0.801938  ],\n",
            "       [0.7887137 , 0.7596497 , 0.608954  , ..., 0.039828  , 0.722521  ,\n",
            "        0.        ],\n",
            "       [0.57606566, 0.5619057 , 0.318055  , ..., 0.189481  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       ...,\n",
            "       [0.9158847 , 0.89302665, 0.566812  , ..., 0.182532  , 1.        ,\n",
            "        0.356896  ],\n",
            "       [0.31060368, 0.31306666, 0.576957  , ..., 0.359267  , 0.099031  ,\n",
            "        0.801938  ],\n",
            "       [0.39607966, 0.39800367, 0.817702  , ..., 0.338625  , 0.277479  ,\n",
            "        0.        ]], dtype=float32), 'Z': array([[0.00028969, 0.00059192],\n",
            "       [0.00022661, 0.00065451],\n",
            "       [0.00034502, 0.00053823],\n",
            "       ...,\n",
            "       [0.0002623 , 0.00066711],\n",
            "       [0.00033335, 0.00049736],\n",
            "       [0.00018098, 0.00055366]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.010054586873423855 epoch:  0\n",
            "reconstruction_loss:  0.010049864230282975 epoch:  0\n",
            "dcc_loss:  4.7226446677155165e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.1021781719877373\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.010040866769965514 epoch:  1\n",
            "reconstruction_loss:  0.010036371179899792 epoch:  1\n",
            "dcc_loss:  4.495611023794652e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.010036063563172229 epoch:  2\n",
            "reconstruction_loss:  0.0100332086625253 epoch:  2\n",
            "dcc_loss:  2.8549112818779244e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.010042261180479625 epoch:  3\n",
            "reconstruction_loss:  0.010040241669740001 epoch:  3\n",
            "dcc_loss:  2.0194714146239204e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.0100338195675412 epoch:  4\n",
            "reconstruction_loss:  0.010032283890975503 epoch:  4\n",
            "dcc_loss:  1.5356956575229988e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.010040296558749847 epoch:  5\n",
            "reconstruction_loss:  0.010039058431478164 epoch:  5\n",
            "dcc_loss:  1.238154261882098e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.010032674563381026 epoch:  6\n",
            "reconstruction_loss:  0.010031605246670523 epoch:  6\n",
            "dcc_loss:  1.0693436516667677e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.010037209741799027 epoch:  7\n",
            "reconstruction_loss:  0.010036243923261905 epoch:  7\n",
            "dcc_loss:  9.65823091200128e-07 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.010030943248412225 epoch:  8\n",
            "reconstruction_loss:  0.010030026157085412 epoch:  8\n",
            "dcc_loss:  9.171030382732581e-07 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.010034670801695312 epoch:  9\n",
            "reconstruction_loss:  0.010033829158673908 epoch:  9\n",
            "dcc_loss:  8.416346231247524e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.010028735813163714 epoch:  10\n",
            "reconstruction_loss:  0.010027898015913798 epoch:  10\n",
            "dcc_loss:  8.377867444780584e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.0614525431984505\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.010033994590217329 epoch:  11\n",
            "reconstruction_loss:  0.010033236773731517 epoch:  11\n",
            "dcc_loss:  7.578175309163958e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.0394309668087685\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.010032796389833783 epoch:  12\n",
            "reconstruction_loss:  0.010032066019667966 epoch:  12\n",
            "dcc_loss:  7.303708185035043e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.051417670560449\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}], 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)}, {10: 9}, {10: 1.0881644651544142}], 21: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 7}, {10: 1.0714495073472743}], 22: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.121244221724319}], 23: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.051417670560449}]}\n",
            "starting model with k= 3\n",
            "No preprocessing is applied\n",
            "The time taken for edge set computation is 7.326040029525757\n",
            "\n",
            "Index: 0 \t Maxepoch: 15\n",
            "\n",
            "Index: 0 \t Epoch: 0\n",
            "train_loss_0 0.24571837007225794 0\n",
            "val_loss_0 0.21883706271459485 0\n",
            "\n",
            "Index: 0 \t Epoch: 1\n",
            "train_loss_0 0.19597155513015077 1\n",
            "val_loss_0 0.17270272700986847 1\n",
            "\n",
            "Index: 0 \t Epoch: 2\n",
            "train_loss_0 0.15547540181221486 2\n",
            "val_loss_0 0.13800748660806242 2\n",
            "\n",
            "Index: 0 \t Epoch: 3\n",
            "train_loss_0 0.12517992422437924 3\n",
            "val_loss_0 0.11205754255530573 3\n",
            "\n",
            "Index: 0 \t Epoch: 4\n",
            "train_loss_0 0.1025404123735005 4\n",
            "val_loss_0 0.09269118790770106 4\n",
            "\n",
            "Index: 0 \t Epoch: 5\n",
            "train_loss_0 0.08564539407477274 5\n",
            "val_loss_0 0.07829924968031432 5\n",
            "\n",
            "Index: 0 \t Epoch: 6\n",
            "train_loss_0 0.073118962996379 6\n",
            "val_loss_0 0.0676623728191229 6\n",
            "\n",
            "Index: 0 \t Epoch: 7\n",
            "train_loss_0 0.06393150383429202 7\n",
            "val_loss_0 0.059884563176973875 7\n",
            "\n",
            "Index: 0 \t Epoch: 8\n",
            "train_loss_0 0.059760758907406016 8\n",
            "val_loss_0 0.059250719426125996 8\n",
            "\n",
            "Index: 0 \t Epoch: 9\n",
            "train_loss_0 0.05913847149928769 9\n",
            "val_loss_0 0.05864254403534669 9\n",
            "\n",
            "Index: 0 \t Epoch: 10\n",
            "train_loss_0 0.0585556832781357 10\n",
            "val_loss_0 0.05805289476118072 10\n",
            "\n",
            "Index: 0 \t Epoch: 11\n",
            "train_loss_0 0.057969543340257994 11\n",
            "val_loss_0 0.05748090447884543 11\n",
            "\n",
            "Index: 0 \t Epoch: 12\n",
            "train_loss_0 0.057397904563065885 12\n",
            "val_loss_0 0.05692718319134365 12\n",
            "\n",
            "Index: 0 \t Epoch: 13\n",
            "train_loss_0 0.05685698965420976 13\n",
            "val_loss_0 0.05639062685487577 13\n",
            "\n",
            "Index: 0 \t Epoch: 14\n",
            "train_loss_0 0.056326514724699246 14\n",
            "val_loss_0 0.05587007619570439 14\n",
            "\n",
            "Index: 1 \t Maxepoch: 15\n",
            "\n",
            "Index: 1 \t Epoch: 0\n",
            "train_loss_1 0.00046751975937195866 0\n",
            "val_loss_1 0.0004679921493004334 0\n",
            "\n",
            "Index: 1 \t Epoch: 1\n",
            "train_loss_1 0.0004666494704076097 1\n",
            "val_loss_1 0.0004669311294489335 1\n",
            "\n",
            "Index: 1 \t Epoch: 2\n",
            "train_loss_1 0.00046562130899669324 2\n",
            "val_loss_1 0.0004658039884159132 2\n",
            "\n",
            "Index: 1 \t Epoch: 3\n",
            "train_loss_1 0.0004645447790611223 3\n",
            "val_loss_1 0.0004646249909987806 3\n",
            "\n",
            "Index: 1 \t Epoch: 4\n",
            "train_loss_1 0.0004633830421816723 4\n",
            "val_loss_1 0.0004634043232614555 4\n",
            "\n",
            "Index: 1 \t Epoch: 5\n",
            "train_loss_1 0.0004621838769919251 5\n",
            "val_loss_1 0.0004621474516504959 5\n",
            "\n",
            "Index: 1 \t Epoch: 6\n",
            "train_loss_1 0.0004609580019975077 6\n",
            "val_loss_1 0.0004608595448401469 6\n",
            "\n",
            "Index: 1 \t Epoch: 7\n",
            "train_loss_1 0.0004596537063613864 7\n",
            "val_loss_1 0.00045953589732679 7\n",
            "\n",
            "Index: 1 \t Epoch: 8\n",
            "train_loss_1 0.0004589151076305666 8\n",
            "val_loss_1 0.0004594004496680381 8\n",
            "\n",
            "Index: 1 \t Epoch: 9\n",
            "train_loss_1 0.00045877542157738555 9\n",
            "val_loss_1 0.00045926433534246297 9\n",
            "\n",
            "Index: 1 \t Epoch: 10\n",
            "train_loss_1 0.0004586302822368657 10\n",
            "val_loss_1 0.00045912749134557597 10\n",
            "\n",
            "Index: 1 \t Epoch: 11\n",
            "train_loss_1 0.00045850857223535783 11\n",
            "val_loss_1 0.000458989933866671 11\n",
            "\n",
            "Index: 1 \t Epoch: 12\n",
            "train_loss_1 0.0004583838724604683 12\n",
            "val_loss_1 0.00045885168504495346 12\n",
            "\n",
            "Index: 1 \t Epoch: 13\n",
            "train_loss_1 0.000458237726432496 13\n",
            "val_loss_1 0.0004587127755063242 13\n",
            "\n",
            "Index: 1 \t Epoch: 14\n",
            "train_loss_1 0.00045810705375746275 14\n",
            "val_loss_1 0.0004585730722771807 14\n",
            "\n",
            "Index: 2 \t Maxepoch: 15\n",
            "\n",
            "Index: 2 \t Epoch: 0\n",
            "train_loss_2 1.1704528671580217e-05 0\n",
            "val_loss_2 1.176773409415243e-05 0\n",
            "\n",
            "Index: 2 \t Epoch: 1\n",
            "train_loss_2 1.168347820098091e-05 1\n",
            "val_loss_2 1.1742969873703893e-05 1\n",
            "\n",
            "Index: 2 \t Epoch: 2\n",
            "train_loss_2 1.166152553831043e-05 2\n",
            "val_loss_2 1.1716773803027607e-05 2\n",
            "\n",
            "Index: 2 \t Epoch: 3\n",
            "train_loss_2 1.163687069515228e-05 3\n",
            "val_loss_2 1.1689386826151817e-05 3\n",
            "\n",
            "Index: 2 \t Epoch: 4\n",
            "train_loss_2 1.1612366410808573e-05 4\n",
            "val_loss_2 1.1660880682173663e-05 4\n",
            "\n",
            "Index: 2 \t Epoch: 5\n",
            "train_loss_2 1.158560765553499e-05 5\n",
            "val_loss_2 1.1631431435430407e-05 5\n",
            "\n",
            "Index: 2 \t Epoch: 6\n",
            "train_loss_2 1.1559420360211741e-05 6\n",
            "val_loss_2 1.1601184815512091e-05 6\n",
            "\n",
            "Index: 2 \t Epoch: 7\n",
            "train_loss_2 1.1531307476091719e-05 7\n",
            "val_loss_2 1.157011791208863e-05 7\n",
            "\n",
            "Index: 2 \t Epoch: 8\n",
            "train_loss_2 1.15150769498449e-05 8\n",
            "val_loss_2 1.1566959986493723e-05 8\n",
            "\n",
            "Index: 2 \t Epoch: 9\n",
            "train_loss_2 1.151141714082207e-05 9\n",
            "val_loss_2 1.1563791443881426e-05 9\n",
            "\n",
            "Index: 2 \t Epoch: 10\n",
            "train_loss_2 1.1508703212686994e-05 10\n",
            "val_loss_2 1.1560617983367507e-05 10\n",
            "\n",
            "Index: 2 \t Epoch: 11\n",
            "train_loss_2 1.1505740575192283e-05 11\n",
            "val_loss_2 1.1557433159214558e-05 11\n",
            "\n",
            "Index: 2 \t Epoch: 12\n",
            "train_loss_2 1.1502878296153116e-05 12\n",
            "val_loss_2 1.1554240494093024e-05 12\n",
            "\n",
            "Index: 2 \t Epoch: 13\n",
            "train_loss_2 1.1499642046733695e-05 13\n",
            "val_loss_2 1.1551041283780615e-05 13\n",
            "\n",
            "Index: 2 \t Epoch: 14\n",
            "train_loss_2 1.1497376989873016e-05 14\n",
            "val_loss_2 1.1547836633796252e-05 14\n",
            "\n",
            "Index: 3 \t Maxepoch: 15\n",
            "\n",
            "Index: 3 \t Epoch: 0\n",
            "train_loss_3 2.963264269181054e-07 0\n",
            "val_loss_3 2.9591350880764367e-07 0\n",
            "\n",
            "Index: 3 \t Epoch: 1\n",
            "train_loss_3 2.961840924869325e-07 1\n",
            "val_loss_3 2.957361835103909e-07 1\n",
            "\n",
            "Index: 3 \t Epoch: 2\n",
            "train_loss_3 2.9599807963860103e-07 2\n",
            "val_loss_3 2.955328144233686e-07 2\n",
            "\n",
            "Index: 3 \t Epoch: 3\n",
            "train_loss_3 2.9579044176389655e-07 3\n",
            "val_loss_3 2.9531215340617643e-07 3\n",
            "\n",
            "Index: 3 \t Epoch: 4\n",
            "train_loss_3 2.9556957843116576e-07 4\n",
            "val_loss_3 2.950804209395394e-07 4\n",
            "\n",
            "Index: 3 \t Epoch: 5\n",
            "train_loss_3 2.953361851570628e-07 5\n",
            "val_loss_3 2.948392702570913e-07 5\n",
            "\n",
            "Index: 3 \t Epoch: 6\n",
            "train_loss_3 2.9509916882604304e-07 6\n",
            "val_loss_3 2.9458913241391813e-07 6\n",
            "\n",
            "Index: 3 \t Epoch: 7\n",
            "train_loss_3 2.948427109756996e-07 7\n",
            "val_loss_3 2.943307592565711e-07 7\n",
            "\n",
            "Index: 3 \t Epoch: 8\n",
            "train_loss_3 2.9469891878933365e-07 8\n",
            "val_loss_3 2.943044053954105e-07 8\n",
            "\n",
            "Index: 3 \t Epoch: 9\n",
            "train_loss_3 2.9467050279071537e-07 9\n",
            "val_loss_3 2.942779428471107e-07 9\n",
            "\n",
            "Index: 3 \t Epoch: 10\n",
            "train_loss_3 2.946455636122367e-07 10\n",
            "val_loss_3 2.942513846739469e-07 10\n",
            "\n",
            "Index: 3 \t Epoch: 11\n",
            "train_loss_3 2.9461847292753167e-07 11\n",
            "val_loss_3 2.9422472389435835e-07 11\n",
            "\n",
            "Index: 3 \t Epoch: 12\n",
            "train_loss_3 2.9459662713659364e-07 12\n",
            "val_loss_3 2.941979949657264e-07 12\n",
            "\n",
            "Index: 3 \t Epoch: 13\n",
            "train_loss_3 2.9456684384621387e-07 13\n",
            "val_loss_3 2.9417115059360586e-07 13\n",
            "\n",
            "Index: 3 \t Epoch: 14\n",
            "train_loss_3 2.9453790622348087e-07 14\n",
            "val_loss_3 2.941442091102246e-07 14\n",
            "\n",
            "Index: 4 \t Maxepoch: 30\n",
            "\n",
            "Index: 4 \t Epoch: 0\n",
            "train_loss_4 0.06457165510044052 0\n",
            "val_loss_4 0.061491537360967435 0\n",
            "\n",
            "Index: 4 \t Epoch: 1\n",
            "train_loss_4 0.05912025441375653 1\n",
            "val_loss_4 0.05641130486683309 1\n",
            "\n",
            "Index: 4 \t Epoch: 2\n",
            "train_loss_4 0.05467180161634758 2\n",
            "val_loss_4 0.052636212549761245 2\n",
            "\n",
            "Index: 4 \t Epoch: 3\n",
            "train_loss_4 0.05137272134429161 3\n",
            "val_loss_4 0.04983099573623926 3\n",
            "\n",
            "Index: 4 \t Epoch: 4\n",
            "train_loss_4 0.048927756871064404 4\n",
            "val_loss_4 0.047753739045274614 4\n",
            "\n",
            "Index: 4 \t Epoch: 5\n",
            "train_loss_4 0.047120133057394646 5\n",
            "val_loss_4 0.04621669201389923 5\n",
            "\n",
            "Index: 4 \t Epoch: 6\n",
            "train_loss_4 0.04578053092671305 6\n",
            "val_loss_4 0.045087112423782306 6\n",
            "\n",
            "Index: 4 \t Epoch: 7\n",
            "train_loss_4 0.04479504255327859 7\n",
            "val_loss_4 0.04424501212222263 7\n",
            "\n",
            "Index: 4 \t Epoch: 8\n",
            "train_loss_4 0.04433887759492098 8\n",
            "val_loss_4 0.04417656434082947 8\n",
            "\n",
            "Index: 4 \t Epoch: 9\n",
            "train_loss_4 0.04427122851073547 9\n",
            "val_loss_4 0.04411062415211023 9\n",
            "\n",
            "Index: 4 \t Epoch: 10\n",
            "train_loss_4 0.04420561235286218 10\n",
            "val_loss_4 0.04404616256121977 10\n",
            "\n",
            "Index: 4 \t Epoch: 11\n",
            "train_loss_4 0.0441417774170959 11\n",
            "val_loss_4 0.043983505693413374 11\n",
            "\n",
            "Index: 4 \t Epoch: 12\n",
            "train_loss_4 0.04407986623076243 12\n",
            "val_loss_4 0.043922778968765695 12\n",
            "\n",
            "Index: 4 \t Epoch: 13\n",
            "train_loss_4 0.04401954495905649 13\n",
            "val_loss_4 0.04386411838693966 13\n",
            "\n",
            "Index: 4 \t Epoch: 14\n",
            "train_loss_4 0.04396116724175976 14\n",
            "val_loss_4 0.043806818905751416 14\n",
            "\n",
            "Index: 4 \t Epoch: 15\n",
            "train_loss_4 0.04390433181199413 15\n",
            "val_loss_4 0.043751049166908355 15\n",
            "\n",
            "Index: 4 \t Epoch: 16\n",
            "train_loss_4 0.04384895711716452 16\n",
            "val_loss_4 0.043697582171099314 16\n",
            "\n",
            "Index: 4 \t Epoch: 17\n",
            "train_loss_4 0.04381863416986329 17\n",
            "val_loss_4 0.043692263141392525 17\n",
            "\n",
            "Index: 4 \t Epoch: 18\n",
            "train_loss_4 0.043813316804615886 18\n",
            "val_loss_4 0.043686937216061233 18\n",
            "\n",
            "Index: 4 \t Epoch: 19\n",
            "train_loss_4 0.04380798743930525 19\n",
            "val_loss_4 0.04368170693800113 19\n",
            "\n",
            "Index: 4 \t Epoch: 20\n",
            "train_loss_4 0.0438027119343916 20\n",
            "val_loss_4 0.0436764780650426 20\n",
            "\n",
            "Index: 4 \t Epoch: 21\n",
            "train_loss_4 0.043797441027312214 21\n",
            "val_loss_4 0.0436712696369023 21\n",
            "\n",
            "Index: 4 \t Epoch: 22\n",
            "train_loss_4 0.043792189759854036 22\n",
            "val_loss_4 0.043666041661319564 22\n",
            "\n",
            "Index: 4 \t Epoch: 23\n",
            "train_loss_4 0.043786944098970666 23\n",
            "val_loss_4 0.043660809015840435 23\n",
            "\n",
            "Index: 4 \t Epoch: 24\n",
            "train_loss_4 0.04378171113574279 24\n",
            "val_loss_4 0.043655624285505616 24\n",
            "\n",
            "Index: 4 \t Epoch: 25\n",
            "train_loss_4 0.043776506089123625 25\n",
            "val_loss_4 0.04365044937907422 25\n",
            "\n",
            "Index: 4 \t Epoch: 26\n",
            "train_loss_4 0.04377351443667212 26\n",
            "val_loss_4 0.043649927094555126 26\n",
            "\n",
            "Index: 4 \t Epoch: 27\n",
            "train_loss_4 0.04377298851042652 27\n",
            "val_loss_4 0.04364940697672628 27\n",
            "\n",
            "Index: 4 \t Epoch: 28\n",
            "train_loss_4 0.043772463979734534 28\n",
            "val_loss_4 0.043648884060501866 28\n",
            "\n",
            "Index: 4 \t Epoch: 29\n",
            "train_loss_4 0.04377193981260253 29\n",
            "val_loss_4 0.04364836655805115 29\n",
            "4\n",
            "Extracted features:\n",
            "Extracting features ...\n",
            "Done.\n",
            "\n",
            "(array([[0.525365, 0.518912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.747047, 0.717983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.534399, 0.520239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.874218, 0.85136 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.268937, 0.2714  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.354413, 0.356337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), array([[0.00026456, 0.00056541],\n",
            "       [0.0001984 , 0.00062972],\n",
            "       [0.00032084, 0.0005174 ],\n",
            "       ...,\n",
            "       [0.00023416, 0.00063934],\n",
            "       [0.00031085, 0.0004783 ],\n",
            "       [0.00015099, 0.00053294]], dtype=float32), array([1., 1., 1., ..., 1., 1., 1.]))\n",
            "Merged features:\n",
            "{'gtlabels': array([1., 1., 1., ..., 1., 1., 1.]), 'X': array([[0.525365, 0.518912, 0.624645, ..., 0.182024, 0.900969, 0.801938],\n",
            "       [0.747047, 0.717983, 0.608954, ..., 0.039828, 0.722521, 0.      ],\n",
            "       [0.534399, 0.520239, 0.318055, ..., 0.189481, 0.099031, 0.801938],\n",
            "       ...,\n",
            "       [0.874218, 0.85136 , 0.566812, ..., 0.182532, 1.      , 0.356896],\n",
            "       [0.268937, 0.2714  , 0.576957, ..., 0.359267, 0.099031, 0.801938],\n",
            "       [0.354413, 0.356337, 0.817702, ..., 0.338625, 0.277479, 0.      ]],\n",
            "      dtype=float32), 'Z': array([[0.00026456, 0.00056541],\n",
            "       [0.0001984 , 0.00062972],\n",
            "       [0.00032084, 0.0005174 ],\n",
            "       ...,\n",
            "       [0.00023416, 0.00063934],\n",
            "       [0.00031085, 0.0004783 ],\n",
            "       [0.00015099, 0.00053294]], dtype=float32), 'w': array([[    0.,  1233.],\n",
            "       [    0.,  2008.],\n",
            "       [    0.,  6001.],\n",
            "       ...,\n",
            "       [12512., 12593.],\n",
            "       [12517., 12553.],\n",
            "       [12526., 12592.]], dtype=float32)}\n",
            "\n",
            " Loaded `oyster` dataset for finetuning\n",
            "The endpoints are Delta1: 0.000, Delta2: 0.005\n",
            "\n",
            " Epoch: 0\n",
            "total_loss:  0.009963333410894915 epoch:  0\n",
            "reconstruction_loss:  0.009958949746130399 epoch:  0\n",
            "dcc_loss:  4.383664660016459e-06 epoch:  0\n",
            "epoch:  0 DBL:  1.0758392572826894\n",
            "\n",
            " Epoch: 1\n",
            "total_loss:  0.00994367201329004 epoch:  1\n",
            "reconstruction_loss:  0.009939479313915835 epoch:  1\n",
            "dcc_loss:  4.192685707639735e-06 epoch:  1\n",
            "\n",
            " Epoch: 2\n",
            "total_loss:  0.009940008827162663 epoch:  2\n",
            "reconstruction_loss:  0.0099370481199118 epoch:  2\n",
            "dcc_loss:  2.9607240023274014e-06 epoch:  2\n",
            "\n",
            " Epoch: 3\n",
            "total_loss:  0.009946322345743682 epoch:  3\n",
            "reconstruction_loss:  0.009944091151355696 epoch:  3\n",
            "dcc_loss:  2.2311900515171665e-06 epoch:  3\n",
            "\n",
            " Epoch: 4\n",
            "total_loss:  0.009943766280156082 epoch:  4\n",
            "reconstruction_loss:  0.009941972887000664 epoch:  4\n",
            "dcc_loss:  1.7933876807701815e-06 epoch:  4\n",
            "\n",
            " Epoch: 5\n",
            "total_loss:  0.009938091417884569 epoch:  5\n",
            "reconstruction_loss:  0.009936654265088015 epoch:  5\n",
            "dcc_loss:  1.4371739879335344e-06 epoch:  5\n",
            "\n",
            " Epoch: 6\n",
            "total_loss:  0.00993955942584281 epoch:  6\n",
            "reconstruction_loss:  0.009938364799770864 epoch:  6\n",
            "dcc_loss:  1.1945931397222497e-06 epoch:  6\n",
            "\n",
            " Epoch: 7\n",
            "total_loss:  0.00995172427946659 epoch:  7\n",
            "reconstruction_loss:  0.009950715190206558 epoch:  7\n",
            "dcc_loss:  1.0091006575741254e-06 epoch:  7\n",
            "\n",
            " Epoch: 8\n",
            "total_loss:  0.009938989286873383 epoch:  8\n",
            "reconstruction_loss:  0.009938102908294968 epoch:  8\n",
            "dcc_loss:  8.863575359131178e-07 epoch:  8\n",
            "\n",
            " Epoch: 9\n",
            "total_loss:  0.009934100519023551 epoch:  9\n",
            "reconstruction_loss:  0.009933324123101201 epoch:  9\n",
            "dcc_loss:  7.763853143619886e-07 epoch:  9\n",
            "\n",
            " Epoch: 10\n",
            "total_loss:  0.00994004912487848 epoch:  10\n",
            "reconstruction_loss:  0.009939373489116396 epoch:  10\n",
            "dcc_loss:  6.756658249769669e-07 epoch:  10\n",
            "epoch:  10 DBL:  1.1057553197607781\n",
            "\n",
            " Epoch: 11\n",
            "total_loss:  0.00994510745566008 epoch:  11\n",
            "reconstruction_loss:  0.009944468547153843 epoch:  11\n",
            "dcc_loss:  6.389251164395113e-07 epoch:  11\n",
            "epoch:  11 DBL:  1.059575187182028\n",
            "\n",
            " Epoch: 12\n",
            "total_loss:  0.009942569809275253 epoch:  12\n",
            "reconstruction_loss:  0.009941943153385677 epoch:  12\n",
            "dcc_loss:  6.266383211208009e-07 epoch:  12\n",
            "epoch:  12 DBL:  1.0758480133185298\n",
            "Phase 1 done!\n",
            "done algorithm on model with k=  3\n",
            "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 7}, {10: 0.9994927461794473}], 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0750217119714527}], 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0636585666466636}], 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0360127844193727}], 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.0337662922969948}], 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 5}, {10: 1.1006713356321056}], 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.1048031905668751}], 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.120594644769965}], 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)}, {10: 9}, {10: 1.2089862779012048}], 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)}, {10: 11}, {10: 1.2488489052534233}], 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)}, {10: 11}, {10: 1.0769178501514183}], 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1540558331327075}], 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)}, {10: 7}, {10: 1.1823069403580344}], 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)}, {10: 6}, {10: 1.0995683257591817}], 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)}, {10: 9}, {10: 1.0441639365109077}], 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 7}, {10: 1.0932239105455974}], 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 5}, {10: 1.04455911368151}], 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)}, {10: 13}, {10: 1.1172606511122094}], 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)}, {10: 14}, {10: 1.0865713649029336}], 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)}, {10: 14}, {10: 1.1551956639775147}], 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)}, {10: 9}, {10: 1.0881644651544142}], 21: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 7}, {10: 1.0714495073472743}], 22: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.121244221724319}], 23: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)}, {10: 4}, {10: 1.051417670560449}], 24: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)}, {10: 3}, {10: 1.0758480133185298}]}\n"
          ]
        }
      ],
      "source": [
        "k=3\n",
        "for start_number in range(0,25):\n",
        "  change_time(start_number)\n",
        "  print('starting model with k=', k)\n",
        "  #construct the mkNN graph\n",
        "  compressed_data(dp.oyster.name, N, k, preprocess='none', algo='knn', isPCA=None, format='mat')\n",
        "\n",
        "  #creating hyperparameter dict\n",
        "  args = edict()\n",
        "\n",
        "  #setting some pretraining hyperparameters\n",
        "  set_pretraining_hypers(args)\n",
        "  args.k = k\n",
        "\n",
        "  #initializing net for pretraining\n",
        "  index = len(dp.oyster.dim) - 1\n",
        "  net = None\n",
        "\n",
        "  #start pretraining\n",
        "  index, net = pretraining.main(args)\n",
        "  print(index)\n",
        "\n",
        "  # extracting pretrained features\n",
        "  args.feat = 'pretrained'\n",
        "  args.torchmodel = 'checkpoint_{}.pth.tar'.format(index)\n",
        "  print('Extracted features:')\n",
        "  print(extract_feature.main(args, net=net))\n",
        "\n",
        "  # merging the features and mkNN graph\n",
        "  args.g = 'pretrained.mat'\n",
        "  args.out = 'pretrained'\n",
        "  args.feat = 'pretrained.pkl'\n",
        "  print('Merged features:')\n",
        "  print(copyGraph.main(args))\n",
        "\n",
        "  #defining necessary dicts for model selection and overfitting detection\n",
        "  cluster_dict = dict()\n",
        "  cluster_count_dict = dict()\n",
        "  dbl_dict = dict()\n",
        "\n",
        "#PHASE 1: starting the overfitting detection phase\n",
        "  #setting some training hyperparameters\n",
        "  set_training_hypers(args)\n",
        "\n",
        "  #start training for 10 epochs, then detect overfitting\n",
        "  clusters, dbl = DCC.main(args, net=net,start_number)\n",
        "  if cluster_count(clusters) == 1 or cluster_count(clusters) == 2:\n",
        "    print(cluster_count(clusters))\n",
        "    print('Overfitting Detected!')\n",
        "    continue\n",
        "  else:\n",
        "    print('Phase 1 done!')\n",
        "    cluster_dict[10] = clusters\n",
        "    cluster_count_dict[10] = cluster_count(clusters)\n",
        "    dbl_dict[10] = dbl\n",
        "\n",
        "    #storing the three dicts in the super dict for further evaluation\n",
        "    super_dict[start_number] = [cluster_dict, cluster_count_dict, dbl_dict]\n",
        "    print('done algorithm on model with k= ', k)\n",
        "    print (super_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2lqb1qx5KxD",
        "outputId": "35bb4185-7f9d-4fe4-f913-d0ae622f554c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)},\n",
              "  {10: 7},\n",
              "  {10: 0.9994927461794473}],\n",
              " 1: [{10: array([0, 1, 2, ..., 4, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.0750217119714527}],\n",
              " 2: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.0636585666466636}],\n",
              " 3: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.0360127844193727}],\n",
              " 4: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.0337662922969948}],\n",
              " 5: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 5},\n",
              "  {10: 1.1006713356321056}],\n",
              " 6: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.1048031905668751}],\n",
              " 7: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.120594644769965}],\n",
              " 8: [{10: array([0, 1, 2, ..., 1, 9, 1], dtype=int32)},\n",
              "  {10: 9},\n",
              "  {10: 1.2089862779012048}],\n",
              " 9: [{10: array([0, 1, 2, ..., 4, 8, 5], dtype=int32)},\n",
              "  {10: 11},\n",
              "  {10: 1.2488489052534233}],\n",
              " 10: [{10: array([0, 1, 2, ..., 4, 9, 7], dtype=int32)},\n",
              "  {10: 11},\n",
              "  {10: 1.0769178501514183}],\n",
              " 11: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)},\n",
              "  {10: 7},\n",
              "  {10: 1.1540558331327075}],\n",
              " 12: [{10: array([0, 1, 2, ..., 4, 1, 5], dtype=int32)},\n",
              "  {10: 7},\n",
              "  {10: 1.1823069403580344}],\n",
              " 13: [{10: array([0, 1, 2, ..., 4, 2, 5], dtype=int32)},\n",
              "  {10: 6},\n",
              "  {10: 1.0995683257591817}],\n",
              " 14: [{10: array([0, 1, 0, ..., 3, 0, 4], dtype=int32)},\n",
              "  {10: 9},\n",
              "  {10: 1.0441639365109077}],\n",
              " 15: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)},\n",
              "  {10: 7},\n",
              "  {10: 1.0932239105455974}],\n",
              " 16: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)},\n",
              "  {10: 5},\n",
              "  {10: 1.04455911368151}],\n",
              " 17: [{10: array([0, 1, 2, ..., 4, 2, 2], dtype=int32)},\n",
              "  {10: 13},\n",
              "  {10: 1.1172606511122094}],\n",
              " 18: [{10: array([0, 1, 1, ..., 9, 1, 1], dtype=int32)},\n",
              "  {10: 14},\n",
              "  {10: 1.0865713649029336}],\n",
              " 19: [{10: array([ 0,  1,  2, ..., 13,  2, 14], dtype=int32)},\n",
              "  {10: 14},\n",
              "  {10: 1.1551956639775147}],\n",
              " 20: [{10: array([0, 1, 2, ..., 9, 2, 4], dtype=int32)},\n",
              "  {10: 9},\n",
              "  {10: 1.0881644651544142}],\n",
              " 21: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 7},\n",
              "  {10: 1.0714495073472743}],\n",
              " 22: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.121244221724319}],\n",
              " 23: [{10: array([0, 1, 2, ..., 1, 2, 3], dtype=int32)},\n",
              "  {10: 4},\n",
              "  {10: 1.051417670560449}],\n",
              " 24: [{10: array([0, 1, 2, ..., 1, 2, 1], dtype=int32)},\n",
              "  {10: 3},\n",
              "  {10: 1.0758480133185298}]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "super_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_values = {}\n",
        "list=[]\n",
        "for key in super_dict:\n",
        "    parsed_values[key] = super_dict[key][2][10]\n",
        "\n",
        "# Print the parsed values\n",
        "for key, value in parsed_values.items():\n",
        "    list.append(value)\n",
        "    print(f'Key {key}: {value}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MALGGp9QCdrw",
        "outputId": "533b0f9b-e861-46cf-e521-25dfccbf4cb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key 0: 0.9994927461794473\n",
            "Key 1: 1.0750217119714527\n",
            "Key 2: 1.0636585666466636\n",
            "Key 3: 1.0360127844193727\n",
            "Key 4: 1.0337662922969948\n",
            "Key 5: 1.1006713356321056\n",
            "Key 6: 1.1048031905668751\n",
            "Key 7: 1.120594644769965\n",
            "Key 8: 1.2089862779012048\n",
            "Key 9: 1.2488489052534233\n",
            "Key 10: 1.0769178501514183\n",
            "Key 11: 1.1540558331327075\n",
            "Key 12: 1.1823069403580344\n",
            "Key 13: 1.0995683257591817\n",
            "Key 14: 1.0441639365109077\n",
            "Key 15: 1.0932239105455974\n",
            "Key 16: 1.04455911368151\n",
            "Key 17: 1.1172606511122094\n",
            "Key 18: 1.0865713649029336\n",
            "Key 19: 1.1551956639775147\n",
            "Key 20: 1.0881644651544142\n",
            "Key 21: 1.0714495073472743\n",
            "Key 22: 1.121244221724319\n",
            "Key 23: 1.051417670560449\n",
            "Key 24: 1.0758480133185298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DBL_values = [\n",
        "    0.9994927461794473,\n",
        "    1.0750217119714527,\n",
        "    1.0636585666466636,\n",
        "    1.0360127844193727,\n",
        "    1.0337662922969948,\n",
        "    1.1006713356321056,\n",
        "    1.1048031905668751,\n",
        "    1.120594644769965,\n",
        "    1.2089862779012048,\n",
        "    1.2488489052534233,\n",
        "    1.0769178501514183,\n",
        "    1.1540558331327075,\n",
        "    1.1823069403580344,\n",
        "    1.0995683257591817,\n",
        "    1.0441639365109077,\n",
        "    1.0932239105455974,\n",
        "    1.04455911368151,\n",
        "    1.1172606511122094,\n",
        "    1.0865713649029336,\n",
        "    1.1551956639775147,\n",
        "    1.0881644651544142,\n",
        "    1.0714495073472743,\n",
        "    1.121244221724319,\n",
        "    1.051417670560449\n",
        "]"
      ],
      "metadata": {
        "id": "N_SwVPIC3GvG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data for plotting\n",
        "k_values = range(0,24)\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot Davies-Bouldin Score\n",
        "plt.plot(k_values, DBL_values, 'bx-')\n",
        "plt.xlabel('k values')\n",
        "plt.ylabel('Davies-Bouldin Score')\n",
        "plt.xticks(k_values)\n",
        "plt.show()\n",
        "plt.savefig('Time.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "pbKDbAy6DYiG",
        "outputId": "79593c16-28a1-41b6-8f84-71b4a92281e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}